 
 
 
g  
 
 
Evaluatievermogen bij 
beleidsdepartementen 
 
Praktijken rond uitvoering en gebruik van ex 
post beleids- en wetsevaluaties  
 
 
C.M. Klein Haarhuis  
 
m.m.v. A. Parapuf 

 
Inhoud 
Afkortingen  
Samenvatting 
 
Hoofdstuk 1 Inleiding en opzet  
 
1.1 Achtergrond en doel van dit onderzoek  
1.2 Onderzoeksvragen  
1.3 Conceptueel kader: evaluatievermogen in context  
1.4 Aanpak  
1.5 Opbouw van dit rapport 
 
Hoofdstuk 2 Internationale ervaringen met evaluatievermogen  
 
2.1 Structuur  
2.1.1 Evaluatie-instituties (organisatieonderdelen)  
2.2 Programmering en evaluatieverplichtingen  
2.3 Middelen  
2.4 Evaluatiekader  
2.4.1 Standaarden en programma’s  
2.4.2 Evaluatiedesigns  
2.5 Borging van evaluatiekennis en –ervaring  
2.6 Gebruik  
2.7 Werkt evaluatievermogen?  
2.8 Samenvatting  
 
Hoofdstuk 3 Praktijkervaringen met evaluatievermogen bij beleidsdepartementen   
 
3.1 Evaluatiebeleid en wetgevingskwaliteitsbeleid  
3.1.1 Verantwoording binnen het rijksbrede evaluatiestelsel  
3.1.2 Wetgevingskwaliteitsbeleid en wetsevaluaties  
3.2 Evaluatiestructuur  
3.2.1 Doelen van evaluatieonderzoek  
3.2.2 Organisatieonderdelen met evaluatietaken  
3.2.3 Samenvattend over structuur  
3.3 Evaluatieprogrammering  
3.3.1 Beleidsdoorlichtingen  
3.3.2 (Evaluatie)onderzoek gericht op kennisvergroting  
3.3.3 Evaluatiebepalingen in wetten  
3.3.4 Politieke context en evaluatieprogrammering  
3.3.5 Samenvattend over programmering  
3.4 Middelen  
3.5 Evaluatiekader 
3.5.1 Evaluatieprogramma’s specifiek gericht op wetgeving  
3.5.2 Evaluatieprogramma’s algemeen  
3.5.3 Handreikingen, richtlijnen, standaarden  
3.5.4 Samenvattend over evaluatiekader  
3.6 Borging van evaluatiekennis en –ervaring  
3.6.1 Communicatie van het evaluatiekader en cursussen 
3.6.2 Selectie van onderzoekers  
3.6.3 Begeleidingscommissies  
3.6.4 Delen en verankeren van ervaringen met evaluatie (aanvragers) 
  
 Pagina 3 van 146 
 

 
Afkortingen 
 
AEA American Evaluation Association 
AEP Algemene Economische Politiek (Directie bij Economische Zaken)  
Anw Algemene nabestaandenwet  
AR Algemene Rekenkamer 
Awb Algemene wet bestuursrecht  
BAT Beleidsanalistenteam (opvolger van de Regiegroep M&E bij 
Economische Zaken) 
BC Begeleidingscommissie  
BEC Beleidskwaliteit- en Evaluatiecommissie (Economische Zaken)  
Bopz Wet bijzondere opnemingen in psychiatrische ziekenhuizen 
BVO  Bureau Verkenning en Onderzoek (vallend onder DG Bestuur en 
Koninkrijksrelaties bij het ministerie van BZK 
BZ Ministerie van Buitenlandse Zaken  
BZK  Ministerie van Binnenlandse Zaken en Koninkrijksrelaties 
CCV  Centrum voor Criminaliteitspreventie en Veiligheid  
CER Commissie Evaluatie Regelgeving  
CES Canadian Evaluation Society  
CIE Counterfactual Impact Evaluations. Om onwtikkelingen op de 
doelvariabele te kunnen attribueren aan beleid, wordt een 
counterfactual (bijv. controlegroep) betrokken: wat zou de situatie zijn 
geweest zonder het beleid?  
CPB  Centraal Planbureau 
CROW Kenniscentrum voor verkeer, vervoer en infrastructuur  
CSO Chief Science Officer (SZW)  
Cw Comptabiliteitswet  
DFEZ  Directie Financieel-Economische Zaken  
DG Directoraat-Generaal  
DG B&I Directoraat-Generaal Bedrijfsleven en Innovatie (Economische Zaken)  
DGIS Directoraat-Generaal Internationale Samenwerking  
DWJZ Directie Wetgeving en/of Juridische Zaken (bij de meeste 
departementen zo geheten) 
EC Europese Commissie  
ECB  Evaluation Capacity Building    
EES European Evaluation Society  
EFRO Europees Fonds voor Regionale Ontwikkeling  
EPTF  Evaluation Policy Task Force van de American Evaluation Association  
ESF Europees Sociaal Fonds  
EWB Extern-wetenschappelijke Betrekkingen (afdeling van WODC bij VenJ)  
EZ  Ministerie van Economische Zaken  
FEZ  (Directie) Financieel-Economische Zaken  
FMC Financiën, Management en Control (‘FEZ’ IenM) 
GAO  General Audit Office  
HBJZ Hoofddirectie Bestuurlijke en Juridische Zaken (IenM)  
IAK Integraal afwegingskader voor beleid en wetgevin g 
IBP Interdepartementale Begeleidingsgroep Prestatiegegevens en 
Beleidsevaluatie  
IEG Independent Evaluation Group (Wereldbank)  
IenM Ministerie van Infrastructuur en Milieu  
IOB Inspectie Ontwikkelingssamenwerking en Beleidsevaluatie (IOB) 
IOFEZ Interdepartementaal overlegorgaan Financieel-economische Zaken  
IOWJZ Interdepartementaal overlegorgaan Wetgeving en Juridische Zaken  
IUC Inkoopuitvoeringscentra (per 2014) 
  
 Pagina 5 van 146 
 

 
 
Samenvatting  
 
Inleiding en vraagstelling  
 
In de borging van wetgevingskwaliteit spelen evaluaties een belangrijke rol. Willen 
deze daadwerkelijk een bijdrage leveren aan de kwaliteit van wetgeving, dan is het 
zaak om: (1) te zorgen dat ze worden verricht; (2) ze zó op te zetten dat ze 
bruikbare, valide en betrouwbare inzichten genereren; en (3) dat ze worden 
gebruikt door beleidsmakers en wetgevingsjuristen in de terugkoppeling naar hun 
producten.  
Dit is in de praktijk echter niet vanzelfsprekend. Wetten en wetswijzigingen zijn 
doorgaans complexe evaluatieobjecten: een mix van regels, normenkaders en 
beleidsinterventies. Behalve empirische en beleidsevaluatie-vaardigheden vereisen 
wetsevaluaties dus ook een juridische blik. Veel individuele aanvragers zeggen niet 
goed te weten wat te doen als zij een wetsevaluatie (proces- of ex post) moeten 
gaan opleveren. Deze factoren kunnen eraan hebben bijgedragen dat wetsvaluaties 
in de praktijk heel verschillend worden aangepakt: zowel qua inrichting van het 
aansturingsproces als methodologisch-inhoudelijk. Een centraal wetsevaluatiebeleid 
is er niet. Dit laatste hoeft niet problematisch te zijn, ware het niet dat eerder in 
meta-onderzoek ook aanwijzingen zijn gevonden voor verschillen in de validiteit en 
betrouwbaarheid van wetsevaluaties.  
De gevarieerde praktijk van wetsevaluatie houdt mogelijk verband met verschillen 
in evaluatievermogen: het vermogen om wetsevaluaties te verrichten en ze 
bovendien te gebruiken. In hoeverre zijn binnen beleidsdepartementen de 
voorwaarden aanwezig om het doen en gebruiken van evaluaties mogelijk te 
maken? Die voorwaarden kunnen zowel het evaluatieproces als de inhoud van 
evaluaties betreffen.  
Doel van deze studie is om een beeld te krijgen van bestaande departementale 
praktijken rond het verrichten en gebruiken van evaluaties – met name 
wetsevaluaties. Aan de basis ligt een streven bij de aanvrager van dit onderzoek, de 
directie Wetgevingskwaliteitsbeleid (WKB) van het Kenniscentrum Wetgeving en 
Juridische Zaken (KCWJ), naar meer vergelijkbaarheid tussen wetsevaluaties. Dit 
om te zorgen dat ervaringen met en kennis uit wetsevaluaties worden 
geaccumuleerd om bredere lessen te kunnen trekken.  
 
Onderzoeksvragen en aanpak  
1. Welke inzichten bieden de internationale (a) literatuur en (b) normen en 
handreikingen met betrekking tot evaluatievermogen? (hoofdstuk 1 en 2)  
2. Welke praktijkervaringen met evaluatievermogen zijn opgedaan door 
beleidsmedewerkers en wetgevingsjuristen, betrokken bij (wets)evaluaties? 
(hoofdstuk 3)  
3. Welke inhoudelijke en methodologische eigenschappen van ex post-
evaluaties en -wetsevaluaties acht men in de praktijk van belang? 
(hoofdstuk 3)  
4. Welke lessen bieden de antwoorden op voorgaande onderzoeksvragen voor 
verdere gedachtevorming over de ontwikkeling van evaluatievermogen – 
vooral met betrekking tot wetgeving? (hoofdstuk 4) 
Ruggengraat van dit onderzoek vormden de volgende aspecten van 
evaluatievermogen en hun kenmerken – gebaseerd op literatuur over 
evaluatievermogen:   
• Structuur: doelstellingen en organisatieonderdelen voor evaluatie 
• Programmering en evaluatieverplichtingen 
• Middelen voor evaluatie: financiële en andere  
  
 Pagina 7 van 146 
 

 
stroming staat het kunnen toeschrijven van resultaten aan beleid centraal: heeft het 
beleid gewerkt en in welke mate? Met de designs in deze stroming wordt gepoogd 
de effecten van het beleid of de wet in kwestie zo goed mogelijk te isoleren van 
andere, om tot een gerichte, netto-uitspraak over doeltreffendheid te komen. In de 
praktijk komt het dan vaak neer op experimentele designs. De aanpakken in de 
tweede stroming zijn gericht op het begrijpen van hoe en waarom – onder welke 
condities – veranderingen zijn opgetreden en resultaten zijn geboekt. De 
stromingen sluiten elkaar niet uit, ze kunnen wederzijds aanvullend worden 
gebruikt. Terugkerend uitgangspunt bij de keuze voor een benadering is fit for 
purpose: keuzes in het evaluatiedesign worden afhankelijk gesteld van de situatie, 
o.a. soort beleid, beleidsfase, evaluatieonderwerp, hoofdvragen en soort doelgroep. 
In dit licht ontwikkelde onder meer de Amerikaanse federale Rekenkamer 
handgrepen voor de evaluatie van complexe federale programma’s. Net als 
Nederlandse wetten zijn ook deze meestal geen ‘cleane’ interventies waarvan de 
werking eenvoudig gemeten en bovendien geïsoleerd kan worden van de context, 
zoals externe invloeden en parallel beleid. Met wat voor soort evaluatiedesigns valt 
dan toch het maximale te zeggen?  
 
Naast het evalueren als zodanig beschouwen grote instituties en landen ook het 
evaluatiegebruik steeds meer als onderdeel van het beleidsproces en niet slechts als 
resultante van een eindrapport. Zo voorziet het Smart Regulation-programma van 
de EC in terugkoppeling van ex post evaluatiebevindingen naar de 
regelgevingscyclus, via de verplichte ex ante evaluaties waarin die bevindingen een 
plek dienen te krijgen. Ook bij de VN wordt evaluatiegebruik voorgeschreven en 
gemonitord; in dat verband stimuleert men ook het aanleggen van metastudies en 
kennisbanken om het overzicht over evaluatiekennis te behouden. Uit recent 
internationaal onderzoek blijkt dat op landenniveau het systematisch gebruiken van 
evaluaties lastig te bewerkstelligen. Wel spelen evaluatiegemeenschappen en 
parlementen zeer waarschijnlijk een stimulerende rol in de totstandkoming en het 
gebruik van evaluaties.  
Behalve de aanvrager speelt de evaluatieonderzoeker nog steeds een belangrijke rol 
in de bevordering van gebruik. Dit blijkt bijvoorbeeld uit leidraden om bevindingen 
toegankelijk en bruikbaar te presenteren. Ook blijkt het uit 
competentiebeschrijvingen van onderzoekers: niet alleen methodologisch-
inhoudelijke vaardigheden worden belangrijk gevonden, maar ook interpersoonlijke 
en communicatieve vaardigheden, flexibiliteit en een gevoelige antenne voor de 
(politieke en beleids-)context.  
Hoewel internationaal veel is verschenen over het bevorderen van uitvoering en 
gebruik van evaluaties, is nog niet aangetoond dat dit een positieve uitwerking heeft 
op de doeltreffendheid van beleid of wetgeving. Ook is gewaarschuwd voor 
ongewenste effecten van evaluatievermogen. Zo blijkt dat het ontwikkelen en 
handhaven van standaarden opportunistisch gedrag of ritualisering in de hand kan 
werken.  
 
Praktijkervaringen met evaluatievermogen bij beleidsdepartementen 
(onderzoeksvragen 2 en 3)  
 
Beleidsdepartementen hebben heel verschillend vorm gegeven aan 
evaluatievermogen, zowel in termen van de betrokken organisatieonderdelen als in 
termen van procesmatige en inhoudelijke evaluatiekaders.  
Het verantwoorden van begrotingsuitgaven blijkt een belangrijk doel van de 
evaluaties bij (rijks)beleidsdepartementen. Veruit het belangrijkste 
verantwoordingsinstrument is de beleidsdoorlichting op begrotingsartikelniveau, die 
volgens de Regeling Prestatiegegevens en evaluatieonderzoek (RPE) elke vier tot 
maximaal zeven jaar dient plaats te vinden en aan de Kamer moet worden 
  
 Pagina 9 van 146 
 

 
feitelijke belang van programmering: zij stellen dat evaluaties vooral samenhangen 
met veranderingen in kabinetssteun voor een bepaald beleid of met 
beleidsaanpassingen en -ontwikkelingen.  
De meeste departementen stellen eerst een integraal budget voor onderzoek vast. 
Uitgevoerde evaluaties moeten binnen dat financiële kader passen. In de praktijk 
gaat men vaak uit van de bestedingsplannen. Onderzoeksinstituten beheren vaak 
hun eigen budget.  
Wat de niet-financiële middelen betreft, melden respondenten vaak dat 
beleidsdirecties weinig tijd hebben voor evaluaties en onderzoek, onder meer 
blijkend uit beperkte capaciteit voor onderzoekscoördinatie. Dit zet ook de 
programmering onder druk. Bovendien werkt tijdgebrek in de hand dat men te laat 
begint aan geplande evaluaties, en belangrijke meetkansen ‘mist’ – zoals het 
aanleggen van een monitoringsysteem of het vormen van een controlegroep. Het 
laatste is een veelvoorkomend euvel.  
In de context van verantwoording zijn de afgelopen paar jaar door FEZ en sommige 
andere organisatieonderdelen initiatieven genomen om de evaluatieprogrammering 
proactiever te maken, vooral met het oog op aankomende beleidsdoorlichtingen en 
het meer doordacht onderzoeken van doeltreffendheid/doeltreffendheid en 
doelmatigheid. We hebben niet overkoepelend kunnen vaststellen wat dit heeft 
opgeleverd.  
 
Evaluatiekader 
Met evaluatiekader doelen we voornamelijk op evaluatie(bevorderende) 
programma’s – die in de praktijk vaak zowel het proces als de inhoud van evaluaties 
betreffen (onderzoeksvragen 2 en 3).  
In het derde hoofdstuk zijn praktijkervaringen met drie speciaal op wetgeving 
gerichte evaluatieprogramma’s de revue gepasseerd, te weten het Programma 
Evaluatie Regelgeving (PER), de Awb-evaluaties en het programma Structurele 
Evaluatie Milieuwetgeving (STEM). Deze bevatten lessen over het evaluatieproces 
(programmering, afbakening hoofdvragen door de aanvrager), het evaluatiedesign 
en evaluatiecriteria. Een overkoepelende les die uit de praktijk van elk van deze 
programma’s is getrokken, betreft de meerwaarde van een multidisciplinaire aanpak 
van wetsevaluaties. Het voldoen aan de juridische randvoorwaarden voor 
doeltreffendheid betekent nog niet dat de wet in de werkelijkheid ook doel zal 
treffen – en omgekeerd. In het PER is een gecombineerd juridisch-
sociaalwetenschappelijke aanpak de eerste van zeven basisvereisten voor een 
wetsevaluatie.  
Daarnaast zijn een aantal algemene beleidsevaluatieprogramma’s besproken, die 
van toepassing zijn op zowel beleid als wetgeving. Bij OCW is beschreven, hoe de 
directie Kennis combinaties van black box- en clear-box (experimentele en 
verklarende) evaluatiebenaderingen poogt te bevorderen. Bij EZ worden evaluatie-
handvatten – in lijn met wat internationaal wordt voorgestaan – geïntegreerd 
aangeboden in een handreiking voor beleidskwaliteit. Het overkoepelende doel van 
die handreiking is effectiever beleid bewerkstelligen en één van de middelen is 
evalueren. De handreiking omvat alle fasen van de beleidscyclus, van 
probleemanalyse, instrumentkeuze en ex ante gevolgenbeoordeling, tot en met 
politieke beslissing, uitvoering en – uiteindelijk – ex post evaluatie. Ze bevat ook 
aanwijzingen en tips bij het opstellen van (a) vroege beleidsexperimenten en (b) het 
opzetten van een monitoringsysteem. 
In de Nederlandse praktijk verschilt de inhoudelijke evaluatiefocus enigszins tussen 
departementen, vaak passend bij de rol van de meest actieve 
organisatieonderdelen. Is FEZ relatief actief, dan weerspiegelt zich dat in een meer 
summatieve, op verantwoording gerichte aanpak, zoals bij EZ en SZW. Bij EZ staat 
men aanvragers bijvoorbeeld bij met uitleg en checklists voor de opzet van 
doeltreffendheidsevaluaties (procesmatig en inhoudelijk) en voor de beoordeling van 
  
 Pagina 11 van 146 
 

 
Contact en overleg tussen aanvrager en onderzoekers gedurende het 
evaluatietraject – al dan niet in een BC – is volgens de literatuur en ook volgens dit 
onderzoek een cruciale manier om de kans op gebruik te vergroten. Ook timing en 
tijdigheid in relatie tot de beleids- en besluitvorming zijn dat. 
Over het algemeen hanteert elk onderzocht departement een vergelijkbare formele 
wijze van communicatie en (korte-termijn) kennisname en gebruik van 
evaluatieonderzoek. In de regel worden rapporten openbaar gemaakt – en vaak ook 
naar de Tweede Kamer gestuurd – met een beleidsreactie. Door onderzoekers vindt 
daarnaast vaak nog verspreiding plaats in de vorm van academische en 
vakpublicaties en via toelichtingen op de departementale werkvloer. Bij BZ is voor 
‘leren’ een adviespanel ingesteld om de bruikbaarheid en terugkoppeling van 
evaluaties naar beleid te vergroten.  
Vervolgens is het aan beleidsmedewerkers en politiek wat er met bevindingen 
gebeurt. Er zijn bij departementen geen ‘verplichte’ constructies gevonden voor 
terugkoppeling naar de beleidscyclus, los van de horizonbepaling in financiële 
regelingen die stelt dat de regeling na vijf jaar vervalt. Eventuele voorzetting dient 
te worden gemotiveerd, bijvoorbeeld met onderzoek naar doeltreffendheid en 
doelmatigheid. Niettemin percipiëren enkele respondenten dat de Tweede Kamer 
toenemende belangstelling laat zien in ex post evaluaties en in wat de minister 
daarmee wil gaan doen. Het betreft wel met name de verantwoordingsfunctie en 
financiële lasten. 
Om de leerfunctie van evaluatie een belangrijker plaats te geven, zouden ex post-
evaluaties volgens sommige gebruikers meer actie- en toekomstgericht moeten zijn. 
Ook worden evaluaties leerzaam gevonden als ze de impliciete aannames waarop 
wetgeving of beleid rust, boven water halen en toetsen aan de empirie. Een 
aanvullende weg is om – in lijn met wat bijvoorbeeld in de VN en bij BZ wordt 
gestimuleerd – meer werk te maken van de systematische stapeling van kennis uit 
evaluaties, bijvoorbeeld in metastudies en kennisbanken. Zo verzet de aanvrager 
van dit onderzoek (WKB) op dit punt al geruime tijd werk.  
 
Slotbeschouwing (Onderzoeksvraag 4)  
 
Veel van de beschreven constructies, procedures en aanpakken zijn nog vers, 
waardoor niet kan worden nagegaan in hoeverre ze structurele verbeteringen 
hebben opgeleverd in termen van leren en verantwoorden of zelfs tot effectiever 
beleid hebben geleid. Uit ervaringen elders is opgemaakt dat evaluatievermogen ook 
een kwestie van doorzettingsvermogen is. Dit maakt het interessant om de verdere 
ontwikkelingen te volgen.   
Een randvoorwaarde voor ‘lerend evaluatievermogen’ lijkt dat initiatieven ter 
vermeerdering van beleidsrelevante kennis niet worden afgestraft, maar 
ondersteund door de politiek en door de instrumenten in het centrale 
evaluatiestelsel. Sommige respondenten betreuren de nadruk die met het centrale 
evaluatiestelsel en de beleidsdoorlichtingen op (eind)doeltreffendheid en 
verantwoording is komen te liggen. Van plan- en procesevaluatie – en uiteraard vele 
andere soorten beleidsonderzoek – kan bijvoorbeeld veel geleerd worden, ondanks 
dat de ‘hamvraag’ van doeltreffendheid en doelmatigheid niet of niet volledig wordt 
beantwoord.  
 
Wat betekenen bovenstaande bevindingen nu voor wetgevingsdirecties en de 
aanvrager, WKB? Directies Wetgeving en/of juridische zaken lijken op grond van dit 
onderzoek nauwelijks aangehaakt bij ex post beleidsevaluaties, hoewel zij soms wel 
een wetsevaluatie begeleiden, zeker als er meerdere directies (bijv. 
Spoorwetgeving) of departementen (bijv. Awb) bij betrokken zijn. Over het 
algemeen zijn beleidsdirecties leidend, terwijl de wetgevingsjurist bijna los staat van 
het centrale evaluatiestelsel met de beleidsdoorlichtingen. Aan de voorkant zijn 
  
 Pagina 13 van 146 
 

 
1. Inleiding en opzet 
1.1 Achtergrond en doel van dit onderzoek  
 
Meer dan twintig jaar geleden constateerde de Algemene Rekenkamer op basis van 
een rijksbreed onderzoek naar de inrichting en uitvoering van de wetgevingsfunctie 
2
dat ‘structurele voorzieningen voor de evaluatie van wetgeving veelal ontbreken’. 
Volgens de Rekenkamer betwijfelen de wetgevingsafdelingen het nut van evaluaties. 
Voor hen zijn de contacten met uitvoerende organisaties en maatschappelijke 
groeperingen meestal voldoende, wetsevaluatie is in hun visie een taak voor de 
beleidsafdelingen (vgl. Olsthoorn-Heim, 2003: 14).  
In de tussentijd is heel wat water door de Rijn gegaan. Allereerst is de 
theorievorming op het vlak van wetsevaluatieonderzoek verder voortgeschreden. 
Winter (1996) deed daartoe een belangrijke aanzet, door met zijn dissertatie inzicht 
te verschaffen in de mogelijkheden en beperkingen van wetsevaluatie. Analoog aan 
evaluatie in het algemeen zijn verschillende benaderingen van wetsevaluatie 
onderscheiden (doelbereiking, doeltreffendheid, effecten). Ook is erop gewezen dat 
wetsevaluatie in feite draait om het toetsen van veronderstellingen van de wetgever 
aan de werkelijkheid. Goed evaluatieonderzoek zou moeten voldoen aan 
theoretische, methodische en onderzoekstechnische eisen. Het ontwikkelen van 
kwaliteitsnormen werd wenselijk geacht, evenals een goed doordacht 
evaluatiebeleid en een goede evaluatie-infrastructuur bij de opdrachtgevers 
(Olsthoorn-Heim, 2003). Over hoe wetsevaluaties methodologisch-inhoudelijk aan 
te pakken zijn in latere jaren verschillende publicaties verschenen (o.a. Nelen, 
2000). Daaronder bevinden zich meta-studies ter inventarisatie en systematisering 
van gevolgde aanpakken en uitkomsten van wetsevaluaties (Klein Haarhuis en 
Niemeijer 2008; Veerman, 2013).  
 
Waarom zijn wetsevaluaties belangrijk?  
Elk departement is verantwoordelijk voor de kwaliteit van wetgeving op zijn eigen 
terrein. Het ministerie van VenJ draagt zorg voor het overkoepelende kwaliteits- en 
evaluatiebeleid. Het Kenniscentrum Wetgeving en Juridische zaken (KCWJ) van dat 
ministerie ondersteunt de directies Wetgeving en/of Juridische Zaken van de 
afzonderlijke departementen (KCWJ, 2015). 
Wetsevaluaties – ex post – vervullen een belangrijke functie in de waarborging van 
wetgevingskwaliteit: door terug te blikken op de in- en uitvoering en de werking van 
wetten, kan geleerd worden voor toekomstige wetten of wetswijzigingen. Als 
wetgevingsjuristen en beleidsmakers die lessen vervolgens ook inzetten, dan dragen 
wetsevaluaties bij aan de kwaliteit van wet- en regelgeving en/of aan de kwaliteit 
van de uitvoering ervan. Op deze manier wordt de regelgevingscyclus, de regulatory 
cycle, gesloten (EC, 2013; OESO, 2015; Smismans, 2015; Mastenbroek et al., 
2015). Dit draagt bij aan de professionalisering van het beleidsproces. 
 
Wetsevaluaties zijn dus een essentieel onderdeel van het 
wetgevingskwaliteitsbeleid. Willen ze daadwerkelijk een bijdrage kunnen leveren 
aan de kwaliteit van wetgeving, dan is het vanuit het perspectief van de aanvrager 
van belang om: (1) ze tot stand te brengen; (2) ze zó op te zetten dat ze bruikbare 
en liefst ook valide en betrouwbare kennis en inzichten genereren; (3) dat 
beleidsmakers/ wetgevingsjuristen kennis nemen van wetsevaluaties en deze 
gebruiken in de terugkoppeling naar hun producten.  
 
                                                
2
 TK 1993-1994, 23710, nrs. 1-2.  
  
 Pagina 15 van 146 
 

 
steeds belangrijker wordende instrumentele functie van wetgeving is het verschil tussen wets- 
en beleidsevaluatie op voorhand niet altijd duidelijk. Een wet bestaat uit recht, maar is een 
‘voertuig van beleid’; zij maakt deel uit van een breed beleid. Wet en beleid zijn hierdoor in 
veel gevallen niet goed meer te scheiden (Veerman, 2007: 228). 
Het meerledige karakter van wetgeving betekent dat naast onderzoek naar de werking van het 
in de wet neergelegde beleidsinstrumentarium – een empirische activiteit vergelijkbaar met 
beleidsevaluatie – óók juridisch georiënteerde onderzoeksactiviteiten worden ondernomen. 
Denk aan wetstechnische analyse (hoe duidelijk is de wettekst, past deze in het 
rechtssysteem, hoe is de relatie met andere wet- en regelgeving?) en verder aan 
jurisprudentie-analyse naar de gelding van de wet en aan internationaal rechtsvergelijkend 
7
onderzoek. Een wetsevaluatie is [dus] idealiter een combinatie van juridisch en empirisch 
onderzoek, ‘beleidsevaluatie met een juridische plus’ (Winter & Klein Haarhuis, 2007; Van 
Aeken, 2007; ZonMw, 2013).  
In veel empirische wetsevaluaties – vooral door externe onderzoeksbureaus – bleek echter een 
juridische invalshoek te ontbreken (o.a. Klein Haarhuis & Niemeijer 2008). Elders ontbrak juist 
weer de empirische component en stonden juridische aandachtspunten, zoals gelding en de 
consistentie met binnenlandse en EU-wet- en regelgeving centraal.  
 
Evaluatievermogen 
De zojuist beschreven gevarieerde praktijk van wetsevaluatie houdt verband met 
verschillen in wat in de literatuur als ‘evaluation capacity’ is aangeduid, en wat wij 
verder zullen noemen: evaluatievermogen. Daarbij gaat het om het vermogen om 
(a) wetsevaluaties te verrichten en (b) ze te gebruiken (Bourgeois & Cousins, 
2013). Dit vermogen kan verschillen tussen departementen maar ook daarbinnen. 
Concreet kan daarbij gedacht worden aan: kenmerken van de beleidsorganisatie 
(bijv. evaluatie-units), het hebben van een evaluatieprogrammering en aan 
leidraden voor aanvragers ten aanzien van evaluatieproces en –inhoud. 
Het doel van dit onderzoek is om een beeld te krijgen van bestaande 
departementale praktijken rondom de uitvoering en het gebruik van evaluaties en 
met name, wetsevaluaties. Hoe worden evaluaties ‘gemanaged’, welke 
organisatieonderdelen en procedures zijn ervoor ingericht?  
Hier aan ten grondslag ligt een streven bij de aanvrager van voorliggend onderzoek, 
de directie wetgevingskwaliteitsbeleid (WKB) van het Kenniscentrum Wetgeving en 
Juridische Zaken (KCWJ), naar meer vergelijkbaarheid tussen wetsevaluaties, niet 
alleen in termen van de inhoud, maar ook procesmatig. Vergelijkbaarheid in deze 
termen bevordert dat ervaringen met wetsevaluaties en kennis uit wetsevaluaties 
kunnen worden geaccumuleerd en men beter in staat is om lessen te trekken voor 
de toekomst. 
In het verleden is via het zogenoemde Clearing House Wetsevaluaties al getracht 
8
om wetsevaluaties meer vergelijkbaar te maken. In vervolg op dit meerjarige 
                                                
7
 Vgl. Oosting (1987) die in ‘De veelzijdigheid van wetsevaluatie’ meerdere niveaus 
onderscheidt: analyse & beoordeling van de tekst van de wet (formele gelding, soorten regels, 
doelen en instrumenten en wetstechnische aspecten); de nadere verkenning van de materiële 
bepalingen (normrelevante situaties, normadressaten en de beoogde rechtsgevolgen van de 
wettelijke bepalingen), het onderzoek naar de werking van de wet (effecten in de praktijk, als 
zodanig en als gevolg van de uitvoering) en de beoordeling en advisering (op basis van 
voorgaande, en met betrekking tot de geldings- en werkingsaspecten van de wettelijke 
regeling).  Zie ook het stappenplan van Nelen (2000): de inrichting van de evaluatie, de 
analyse van de voorgeschiedenis van de wet, de analyse van de wijze van uitvoering, de 
juridische ijkpunten, het vaststellen en beoordelen van effecten en, ten slotte, het uitdragen 
van onderzoeksresultaten. 
8
 Dit gebeurde door aandacht te besteden aan een aantal vaste, door wetgevingsdirecties 
gedragen punten (Veerman, 2013):  
1. de aard van de norm (open of gesloten, doel of middel, algemene regels of 
vergunningstelsel); 
2. de aard van de handhaving en sanctionering (strafrechtelijk, bestuursrechtelijk, 
civielrechtelijk); 
  
 Pagina 17 van 146 
 

 
1. Welke inzichten bieden de internationale (a) literatuur en (b) normen en 
handreikingen met betrekking tot evaluatievermogen? (hoofdstuk 1 en 2)  
2. Welke praktijkervaringen met evaluatievermogen zijn opgedaan door 
beleidsmedewerkers en wetgevingsjuristen, betrokken bij (wets)evaluaties? 
(hoofdstuk 3)  
3. Welke inhoudelijke en methodologische eigenschappen van ex post-
evaluaties en -wetsevaluaties acht men daarbij van belang? (hoofdstuk 3)  
4. Welke lessen bieden de antwoorden op voorgaande onderzoeksvragen voor 
verdere gedachtevorming over de ontwikkeling van evaluatievermogen – 
vooral met betrekking tot wetgeving? (hoofdstuk 4) 
 
Ex ante evaluaties worden in dit onderzoek buiten beschouwing gelaten. We richten 
ons op ex post evaluaties: onderzoek naar doelbereiking, doeltreffendheid en 
neveneffecten. Procesevaluaties staan niet centraal, maar kunnen voor het begrip 
12
van de doeltreffendheid wel belangrijke inzichten bieden. Om die reden sluiten we 
procesevaluaties niet uit. 
Met onderzoeksvraag 3 doelen we vooral op wat beleidsdepartementen adviseren of 
voorschrijven rondom het design, het methodologische ontwerp van een 
evaluatieonderzoek. Deze onderzoeksvraag naar evaluatie-aanpakken komt voort 
uit behoefte bij de aanvrager (KCWJ-WKB) aan nadere kennis en informatie op dit 
punt. De afbakening en designkeuzes in een evaluatie bepalen in belangrijke mate 
de inhoud en ook de kwaliteit. Bijvoorbeeld, of op basis van de onderzoeksgegevens 
en de analyse een gefundeerd oordeel kan worden gegeven over de in- en 
uitvoering van een wet, over de mate van doeltreffendheid of doelbereiking en over 
neveneffecten, zoals in termen van milieu en administratieve lasten. Die kwaliteit is 
afhankelijk van het toepassen van geschikte methoden, het op tijd verzamelen van 
data en nauwkeurigheid in de interpretatie van data (o.a. Cooksy, 2012). 
Er bestaat niet één ideaal evaluatiedesign, dit is situatieafhankelijk.  
In dit rapport stellen we zelf geen inhoudelijke of methodologische normen of 
criteria op, maar geven we een beschrijving van bestaande leidraden hierover en zo 
mogelijk ook van wat hun toepassing heeft opgeleverd. Daarbij houden we oog 
verschillen tussen domeinen, typen wetgeving en evaluatievragen. Na het voorlopig 
beantwoorden van onderzoeksvraag 4 in het conclusiehoofdstuk van dit rapport, zal 
een expertmeeting worden georganiseerd ten behoeve van verdere discussie en 
13
vooruit kijken. 
De vraag, of de aanwezigheid en inzet van evaluatievermogen ook echt doorwerkt in 
de inhoud en kwaliteit van (wets)evaluatieonderzoek, blijft in deze studie buiten 
beschouwing. Dit is overigens wel een veel gemaakte aanname in de literatuur – 
eentje die goeddeels ongetoetst is gebleven (zie hoofdstuk 2).  
 
1.3 Conceptueel kader : evaluatievermogen in context 
 
Wat is evaluatievermogen?  
Sleutelwoord in de grote hoeveelheid internationale wetenschappelijke literatuur 
over dit onderwerp is evaluation capacity (building) of ECB. Dit is een 
veelomvattend begrip, dat door de tijd heen verschillend – doch steeds breder – is 
gedefinieerd. Omdat ‘capaciteit’ in ons taalgebied al gauw vernauwd wordt tot 
middelen en mankracht, gebruiken we vanaf hier het begrip ‘evaluatievermogen’, 
omdat dit meer omvat dan mensen en middelen alleen.  
                                                
12
 Zo laat de mate van programma-integriteit zien in hoeverre een wet of beleid is 
geïmplementeerd (Lösel, 1995). Blijkt de implementatie niet of maar deels volbracht, dan kan 
van effectiviteit geen sprake zijn.  
13
 De resultaten van die expertmeeting zijn niet meer in dit rapport verwerkt.   
  
 Pagina 19 van 146 
 

 
 
Evaluatievermogen: kunnen uitvoeren en kunnen gebruiken  
In wat volgt werken we met behulp van de literatuur over evaluatievermogen verder 
uit, wat evaluatievermogen inhoudt. Voor zover wij hebben kunnen nagaan, is de 
internationale academische belangstelling voor evaluatievermogen niet of nauwelijks 
toegespitst op de specifieke kenmerken van wet- en regelgeving en doen we het dus 
met bredere inzichten.  
Het vermogen om evaluaties te verrichten gaat volgens Bourgeois en Cousins 
(2013) over het hele evaluatietraject vanaf het aanvragen, opstellen van 
startnotitie, uitvoeren en begeleiden van evaluatieonderzoek. Dit vereist 
organisatorische en individuele capaciteit, een evaluatieprogrammering en -
activiteiten. Deze aspecten werken we verderop uit, onderscheid makend tussen de 
vraag- en aanbodzijde van evaluaties.  
De tweede dimensie van evaluatievermogen die we van Bourgeois & Cousins 
overnemen is het vermogen om evaluatiebevindingen te gebruiken. Deze dimensie 
geeft de mate aan waarin men in staat is kennis en inzichten uit (wets)evaluaties 
terug te koppelen naar het proces van beleid en wetgeving. Gebruik is volgens de 
auteurs een belangrijke dimensie, want als een departement een meerjarige 
evaluatieprogrammering hanteert zonder vervolgens de bevindingen in te zetten om 
beleid en wetgeving te verbeteren, is evalueren weinig zinvol.  
Het vermogen om evaluaties te gebruiken wordt volgens dezelfde auteurs bepaald 
door: de betrokkenheid van de aanvrager en andere stakeholders en de mate van 
19
integratie van evaluatie in het proces van besluitvorming (vgl. Smismans, 2015). 
Bourgeois & Cousins (2013) noemen in het kader van gebruik ook de 
leeropbrengsten van evalueren; verwant aan wat elders wel conceptueel of indirect 
gebruik is genoemd (o.a. Winter, 1996). Daarmee worden bedoeld de attitude- en 
gedragsveranderingen voortvloeiend uit evaluaties, de mate waarin leerervaringen 
formeel en informeel worden gedeeld en het ‘denken in evaluatietermen’ door 
organisatieleden. Dit laatste is bijvoorbeeld het geval als beleidsmedewerkers al bij 
de ontwikkeling van beleidsdoelen en instrumenten rekening houden met 
evalueerbaarheid.  
 
Evaluatievermogen: vraag- en aanbodzijde 
Het model van Nielsen et al. (2011) overlapt in veel opzichten met dat van 
Bourgeois & Cousins (2013). Een verschil is echter dat Nielsen et al. nadrukkelijk 
20
onderscheid maken tussen de vraag- en de aanbodzijde van evaluatievermogen. 
De vraagzijde heeft betrekking op de aanvragers van evaluaties, de aanbodzijde op 
de uitvoerders. Omdat juist in Nederland veel beleidsonderzoek extern wordt 
aanbesteed, nemen we dit onderscheid over in de verdere uitwerking van 
evaluatievermogen.  
Belangrijke aspecten van evaluatievermogen aan de vraagzijde (hier: de 
aanvragers, beleidsmedewerkers) zijn het hebben van duidelijke 
evaluatiedoelstellingen en structuren en processen voor evaluatie.  
 
 
 
                                                                                                                             
mindere mate het geval (zie ook Hoofdstuk 2) en hebben evaluatiegemeenschappen een 
minder prominente rol.  Wel is er de European Evaluation Society (EES).  
19
 Ook Johnson et al. (2009) lieten op basis van een internationale metastudie (met slagen om 
de arm) zien dat het betrekken van belangrijke stakeholders in het evaluatietraject het meest 
cruciaal is voor gebruik; waarschijnlijk meer nog dan de inhoudelijke kwaliteit. Verder bleken 
van belang – niet verrassend – de politieke of ambtelijke besluitvormingscontext en de timing 
en tijdigheid van oplevering van evaluaties.  
20
 Verder benoemen Nielsen et al. de evaluatieprogrammering niet expliciet als onderdeel van 
evaluatievermogen.  
  
 Pagina 21 van 146 
 

 
Tabel 1 Evaluatiedoelen, benaderingen en waarden  
Evaluatiedoel Benaderingen Waarden 
 
• Meerdere mogelijke vormen • Nut  
‘Leren’ 
van evaluatie 
• Bruikbaarheid voor beleid 
(enlightenment, 
• Toekomstgericht  
embetterment)  
• Interne en externe evaluatie 
• Formatieve evaluatie 
• Lange termijn- perspectief 
• Dynamisch / 
contextgebonden 
• Doelbereikings- of • Doeltreffendheid 
Verantwoorden 
doeltreffendheidsevaluatie  
• Objectiviteit 
• Outputgericht • Transparantie 
• Externe evaluatie 
• Summatieve evaluatie   
• Korte-termijn perspectief 
• Statisch 
Ordening op basis van de aangehaalde literatuur.  
 
Voor het ‘sluiten van de cyclus’ van wetgeving en beleid zou een lerend 
evaluatieperspectief de voorkeur genieten boven een verantwoordingsperspectief. 
Evaluaties die ‘opener’ zijn ingestoken (hoe pakt het uit? welke lessen kunnen we 
trekken?) bieden immers meer kans om nieuwe inzichten te genereren dan 
verantwoordingsexercities (is het beleid succesvol?). In de praktijk zal de scheidslijn 
tussen de evaluatievormen van Tabel 1 echter niet heel scherp zijn. Zo maakt 
verklarende evaluatie steeds vaker deel uit van handreikingen voor summatief 
onderzoek. Omgekeerd worden tussentijdse, op leren gerichte evaluaties soms 
ingezet voor verantwoordingsdoeleinden – al dan niet gerechtvaardigd.  
Op langere termijn werkt het motief mogelijk door in de manier waarop structureel 
met evaluaties wordt omgegaan: in het evaluatievermogen.  
 
Vraagzijde: structuren en processen  
Evaluatiedoelen, zoals bijvoorbeeld politieke of uitgavenverantwoording of juist 
‘leren’, kunnen doorwerken in de organisatiestructuren en -processen. Bij structuren 
en processen denken Nielsen et al. (2011) aan de aanwezigheid van een 
evaluatieafdeling of –unit. Daarbij valt te denken aan een aparte stafdirectie of een 
unit bij een beleidsdirectie, financiële of kennisdirectie.  
Bourgeois & Cousins (2013) benadrukken het belang van een (meerjarige) 
evaluatieprogrammering voor de oplevering van verschillende evaluaties van het 
ingezette beleidsinstrumentarium. De meerjarigheid kan de programmering 
‘weerbaarder’ maken tegen incidenten en politieke invloeden. Tegelijkertijd wordt 
aanpassingsvermogen van de programmering aan nieuwe ontwikkelingen van 
belang geacht.  
Voorts bepalen de tijd en de financiële middelen die de beleidsorganisatie vrijmaakt 
voor evaluaties het evaluatievermogen bij de aanvrager. 
Volgens Cooksy (2012, p. 82) weerspiegelen deze structuren en processen de 
waarde die aan evaluatie wordt gehecht en hoe bereidwillig een organisatie is om 
goed evaluatieonderzoek te verrichten. Zij voegt kwaliteitscontrole op de 
onderbouwing van evaluaties toe aan het lijstje. Daarbij kan bijvoorbeeld gebruik 
worden gemaakt van ‘checklists’ en procedures.  
 
Aanbodzijde: menselijk kapitaal (en technologie)  
Aan de aanbodzijde, bij onderzoekers die evaluaties uitvoeren, zijn menselijk 
kapitaal (Nielsen et al., 2011; Bourgeois & Cousins, 2013) en technologie (Nielsen 
  
 Pagina 23 van 146 
 

Vermogen om evaluaties te doen en vermogen om evaluaties te gebruiken 
 
 
 
Context 
 
 Aspecten Kenmerken 
 • Politieke Motivatie voor al dan niet verrichten van wetsevaluaties 
(verantwoording, leren, anders)  
commitment 
 
- Nationale / internationale evaluatiegemeenschappen  
• Andere 
 
contextfactoren  - Overige (bijv. conjunctuur ...)  
 
Aspecten Kenmerken Bronnen 
 
Organisatie- - Formele doel(en) van 
• Structuur 
 
evaluatieonderzoek  
 
inrichting/  
 
- Aanwezigheid evaluatie unit, aantal 
 
vraagzijde 
 
medewerkers 
 
- Verhouding intern-extern uitgevoerde 
evaluaties  
 
• Evaluatie-  - (Meerjarige) evaluatieprogrammering  
 
programmering  
 
  
(Online) 
- Omvang en toedeling financiële en 
• Middelen 
documenten  
andere middelen (bijv. tijd) 
 
 
• Evaluatiekader  - Aanwezigheid en inhoud van 
Organigram-
(ten aanzien evaluatiebevorderende programma’s 
men 
van zowel - Richtlijnen, handreikingen, procedures 
 
bij aanvraag/ uitvoering  
evaluatieproces 
als -design) Jaarverslagen 
 
 
Interviews  
 
• Borging van Kennis (aanvrager)  
evaluatiekennis - Kennis van en ervaring met 
en –ervaring  evaluatieproces en – inhoud 
- Communicatie van evaluatiekader in de 
 
organisatie & trainingen/cursussen 
 
voor beleidsmedewerkers 
Selectie  
- Competenties waarop aanvragers 
onderzoekers selecteren  
- Begeleidingscommissies en hun 
samenstelling  
Ervaring 
- Delen en verankeren van ervaringen 
met evaluaties   
• Gebruik Proces (bruikbaarheid)  
- Communicatie tussen onderzoekers en 
 
aanvragers – al dan niet via een 
 
evaluatie- of BC  
- (Wijze van) presentatie bevindingen; 
aanbevelingen  
- Rapportage (openbaarheid, 
toegankelijkheid) en verdere 
verspreiding   
Gebruik  
- Direct gebruik in beleids- en 
besluitvorming 
- Overige leeropbrengsten 
 
  
 Pagina 25 van 146 
 

 
29
toegespitst op de Nederlandse context. Elk afzonderlijk interview is voorbereid aan 
de hand van online beschikbaar materiaal over het departementale onderdeel in 
kwestie.  
Bijlage 2 bevat een overzicht van de 35 in 20 gesprekken geraadpleegde 
sleutelinformanten met het oog op onderzoeksvragen 2 en 3. Voor de selectie van 
de sleutelinformanten is aangesloten bij waar binnen de rijksoverheid ervaring met 
evaluatievermogen aanwezig is. Eerste ingangen waren WKB, het 
30
Interdepartementaal Overleg Directies Wetgeving en/of Juridische Zaken (IOWJZ) 
en ervaringsdeskundigen bij beleids- en tevens kennisdirecties bij verschillende 
ministeries. Ook is gebruik gemaakt van het bestaande professionele netwerk van 
onderzoeker en aanvrager(s). In de periode juni-oktober 2015 is gesproken met 
(senior)-beleidsmedewerkers en onderzoekscoördinatoren, directies Financieel-
economische Zaken (FEZ) en enkele directies Wetgeving e/o Juridische Zaken, en 
indien van toepassing, rijksonderzoeksinstituten of -organen. Het betreft uiteindelijk 
31 32
negen departementen: BZK, BZ , EZ, Financiën , IenM, OCenW, SZW, VenJ en 
VWS. Per departement is niet met alle typen respondenten gesproken; de 
samenstelling wisselt afhankelijk van waar relevante initiatieven zijn ontplooid.  
33
Andere respondenten zijn vertegenwoordigers van: VIDE , WRR, de Algemene 
Rekenkamer, de Raad van State en betrokkenen bij twee wetgevings-
evaluatieprogramma’s (Awb en ZonMw). 
Om de respondenten te stimuleren hun ervaringen met evaluatievermogen feitelijk 
te onderbouwen, is steeds doorgevraagd naar geschreven bronnen, zoals 
taakomschrijvingen van organisatieonderdelen verantwoordelijk voor evalueren, 
evaluatieprogramma’s, -leidraden of handreikingen. Acht (groepen) respondenten 
met een aanzienlijk aandeel in de eindtekst zijn enkele maanden na het interview in 
de gelegenheid gesteld om de betreffende tekstdelen op feitelijke onjuistheden te 
34
corrigeren en aan te vullen.   
 
1.5 Opbouw van dit rapport  
 
 
Het conceptuele kader vormt de ruggengraat voor de beschrijvingen in elk van de 
drie nu volgende hoofdstukken. In het tweede hoofdstuk wordt onderzoeksvraag 1 
verder beantwoord, op basis van aanpakken en ervaringen van internationale 
organisaties en enkele grotere landen. Het derde hoofdstuk vangt aan met een 
algemene beschrijving van het Nederlandse evaluatiestelsel en het 
wetsevaluatiebeleid. Daarna zijn de Nederlandse beleidsdepartementen aan de beurt 
en we volgen ook hier de aspecten van het conceptuele kader (onderzoeksvragen 2 
en 3). Binnen het aspect ‘evaluatiekader’ gaat speciale aandacht uit naar ervaringen 
met het design, het methodologische ontwerp, van evaluaties. Voor dit derde 
hoofdstuk is behalve van desk study ook gebruik gemaakt van interviews met 
sleutelinformanten bij de negen beleidsdepartementen en daarbuiten.  
                                                
29
 Een eerste versie van de topiclijst is getest in een interview met vier EWB-medewerkers 
(WODC-intern).      
30
 Tijdens een vergadering van dit platform is al in een vroeg stadium van het onderzoek 
aangegeven dat men primair navraag zou moeten doen bij beleidsdirecties die belast zijn met 
evaluatie en niet zozeer bij wetgevingsdirecties zelf.   
31
 Bij BZ worden geen wetsevaluaties verricht. Toch achten we de ervaringen van de IOB 
(Inspectie Ontwikkelingssamenwerking en Beleidsevaluatie), het instituut verantwoordelijk 
voor de meeste evaluaties bij BZ, relevant met het oog op wetsevaluaties, met name rond het 
gebruik en de leeropbrengsten van evaluaties (zie par. 3.7).  
32
 Bij Financiën is alleen het centrale evaluatiestelsel aan bod gekomen.  
33
 Beroepsvereniging voor toezichthouders, inspecteurs, handhavers en evaluatoren.  
34
 Het betrof de respondenten die zijn geïnterviewd rondom: EZ, OCW (Kennis), SZW (FEZ), 
IenM (FEZ/HBJZ), VenJ (WODC-EWB), Awb, ZonMw en BZK (FEZ).  
  
 Pagina 27 van 146 
 

 
Hoofdstuk 2. Internationale ervaringen met 
evaluatievermogen  
 
 
Er is internationaal veel ervaring opgedaan met het bouwen aan evaluatievermogen 
(evaluation capacity building of ECB). De inzichten specifiek gericht op het doen en 
gebruiken van wetsevaluaties zijn echter een stuk dunner gezaaid, ondanks dat veel 
evaluaties in de praktijk een wet- en regelgevingscomponent hebben. In dit 
hoofdstuk zetten we een aantal internationale ervaringen met het bouwen aan 
evaluatievermogen (al dan niet in relatie tot wetgeving) op een rij.  
In de internationale beleidspraktijk zijn, zeker het afgelopen decennium, in het 
kader van evaluatievermogen overweldigend veel handreikingen en gidsen, 
checklists, leidraden enzovoorts verschenen, zo leert de online zoektocht. Het is 
onbegonnen werk om dat alles in dit hoofdstuk systematisch aan bod te laten 
komen. We beperken ons daarom tot het belangrijkste, relatief algemene materiaal 
en de ervaringen van grotere internationale instellingen en landen in met name de 
EU en de VS. Dit materiaal hebben we gestructureerd naar het conceptuele schema 
van hoofdstuk 1: structuur, programmering, middelen, evaluatiekader, borging van 
35
kennis/ervaring en gebruik. 
 
 
2.1 Structuur  
 
Bij sommige internationale organisaties ligt de oorsprong van de aandacht voor 
evaluatievermogen in een sterke behoefte aan verantwoording (transparency & 
accountability; Wereldbank, 1992). Denk aan aanzienlijke uitgaven aan leningen en 
ontwikkelingsprogramma’s. Mede daarom bestaat bij de Wereldbank een van de 
langste ECB-tradities (vgl. de Inspectie Ontwikkelingssamenwerking en 
Beleidsevaluatie (IOB) in Nederland). Maar juist hier is in de loop der jaren ook een 
sterke beleidsinhoudelijk gedreven functie (‘leren’, als in enlightenment, 
embetterment) naar de voorgrond gekomen, getuige bijvoorbeeld de aandacht die 
het Wereldbank Institute en de huidige Independent Evaluation Group (IEG) aan 
leren besteden (Box 1).  
 
Box 1 Wereldbank: Independent Evaluation Group  
 
Van de Wereldbank en de betrokken ontwikkelingslanden wordt een hoge mate van 
accountability vereist; het gaat immers om de besteding van donorgelden. Als sinds de jaren 
tachtig werkt de Wereldbank dan ook aan monitoring & evaluatiesystemen (m&e) in 
ontwikkelingslanden (o.a. Wereldbank 2002, 2004a,b). De Bank is ervaringsdeskundige op het 
vlak van ECB. Het doel van evaluaties bij de Wereldbank is zowel verantwoorden als leren: om 
een objectieve beoordeling te geven van resultaten van de Wereldbank en om lessen te 
trekken uit ervaringen en die te verspreiden. 
De Wereldbank heeft (o.a.) een Independent Evaluation Group (IEG), die reguliere evaluaties 
36
verricht van wereldwijde of regionale programma’s en beleidsstrategieën. De IEG rapporteert 
rechtstreeks aan de Board of Directors van de WB Group. De IEG verricht behalve 
projectevaluaties en doeltreffendheidsstudies (impact evaluation) en onafhankelijke 
assessments, ook veel overige werkzaamheden, waaronder literatuurreviews, analytisch werk, 
landenstudies en surveys onder staf en stakeholders. Daarin vinden verschillende soorten 
evaluatie-analyses plaats: het toetsen van uitkomsten van verschillende typen beleid aan 
gestelde doelen, verwachtingen en benchmarks en voorts het onderzoeken wat gebeurd zou 
                                                
35
 Door de aard van het materiaal – vaak integrale benaderingen of programma’s – valt de 
inhoud niet altijd precies in deze zes topics in te passen.   
36
 www.ieg.worldbankgroup.org .  
  
 Pagina 29 van 146 
 

 
 
2.1.1 Evaluatie-instituties (organisatieonderdelen) 
 
Dat er instituties zijn die zich specifiek op evaluatieonderzoek richten, is beslist niet 
nieuw. Wel veranderen ze vaak van vorm en zijn er periodes waarin de 
evaluatiefunctie meer in de belangstelling staat. In een bundel zetten Mayne et al. 
(red., 1992) alweer meer dan twintig jaar geleden internationale modellen en 
ervaringen met evaluatiebeleid en –instituties bijeen. Bemelmans-Videc concludeert 
in haar bijdrage op basis van een globale landenvergelijking dat belangrijke 
condities waaronder evaluatie-instituties zich blijken te ontwikkelen, zijn: algemene 
politieke en economische context (bijv. bezuinigingsimpulsen) en de politiek-
bestuurlijke structuur, bijvoorbeeld: de aanwezigheid van centrale instituties zoals 
Rekenkamer of ministerie van Financiën die als change agent fungeren. 
Administratieve evaluatieprocedures zijn volgens haar met name veelbelovend als 
ze gekoppeld zijn aan het begrotingsproces. Maar de bestuurlijke cultuur – het 
kritisch tegen het licht willen houden van beleidsproducten – zou het meest 
bepalend zijn voor de snelheid waarmee evaluatievermogen zich ontwikkelt.  
De VS en Canada kennen al sinds de jaren ‘70 gecentraliseerde evaluatie-units en 
een systeem waarin evaluatie een duidelijke plek heeft in de bestuurlijke structuur 
40
en het budgetteringsproces. Canada hanteert sinds 1977 de norm dat alle 
departementen en agentschappen hun programma’s evalueren en de bevindingen 
gebruiken om programma’s te verantwoorden, te verbeteren of op te heffen 
(Bemelmans-Videc, 1992:11; Lahey, 2010). Het adjuncthoofd van de federale 
regering kreeg hiervoor de verantwoordelijkheid en de coördinatie is belegd bij de 
Program Evaluation Branch van de Comptroller General. Uitzonderingen daargelaten 
kende reeds rond 1990 elk Canadees beleidsdepartement en agentschap een 
corporate evaluation unit.  
In 2002 is door academici voor het eerst een International Atlas of Evaluation 
uitgebracht (Furubo et al., 2002). Hierin zijn 21 OESO-landen met elkaar vergeleken 
in termen van de mate waarin structureel aandacht bestaat voor beleidsevaluatie, 
en in hoeverre men een zekere ‘volwassenheid’ (maturiteit) heeft bereikt in termen 
van cultuur, systemen en organisatiecapaciteit voor evaluatie. De negen indicatoren 
uit de Atlas zijn:  
1. Evaluatie vindt plaats in meerdere beleidsdomeinen 
2. Er is aanbod van evaluatieonderzoekers vanuit verschillende disciplines  
3. Discussies en debatten naar aanleiding van evaluaties vinden plaats 
4. Er is een nationale evaluatiegemeenschap  
5. In de overheid bestaan institutionele voorzieningen voor het doen van 
evaluaties en voor de verspreiding van evaluatieresultaten  
6. Binnen het parlement bestaan evaluatievoorzieningen  
7. Pluralisme: binnen elk beleidsdomein houden verschillende mensen/ units 
zich met evaluatie bezig  
8. De (Algemene) Rekenkamer voert evaluatieactiviteiten uit  
9. Evaluaties richten zich niet alleen op input en output, maar ook op de 
outcome (einduitkomst). 
 
De Atlas-exercitie is in 2015 herhaald voor 19 OESO-landen, gebaseerd op een 
 
survey onder vier tot vijf evaluatie-experts per land, in verschillende domeinen en 
functies (Jacob et al., 2015).  
41
In dit laatste onderzoek vallen niet alleen de grote internationale verschillen in 
evaluatiematuriteit op, maar ook die tussen beleidsdomeinen. Er is binnen 
                                                
40
 Hoewel die in de VS van de jaren ’80 weer deels werden ‘wegbezuinigd’.   
41
 Landen met een grote mate van evaluatie’ maturiteit’ (n = 15; 79%) in termen van de 
negen indicatoren zijn: Australië, Canada, Denemarken, Finland, Frankrijk, Duitsland, Israël, 
  
 Pagina 31 van 146 
 

 
46
begeleiden en te bevorderen in Europa en verder. Zo geeft de EES een Nieuwsbrief 
uit, genaamd Evaluation Connections en zijn onder de tab resources net als bij de 
AEA evaluatiestandaarden te vinden van landelijke/regionale 
evaluatiegemeenschappen en daarnaast van internationale en supranationale 
organisaties. De EES organiseert onder andere trainingen en een tweejaarlijks 
congres.  
In Nederland is VIDE de Beroepsvereniging voor toezichthouders, inspecteurs, 
handhavers en evaluatoren. Deze vereniging is in 2001 opgericht (Jacob et al., 
2015, p. 17) ongeveer tegelijk met de Noorse en Zweedse. Binnen VIDE organiseert 
sinds enkele jaren het Evaluatorennetwerk bijeenkomsten over evaluatieproces en -
aanpak. Ook is de brancheorganisatie van bureaus en instituten, de Vereniging voor 
Beleidsonderzoek (VBO), actief op het vlak van kwaliteitsbevordering en 
47
professionalisering. 
 
Intern of extern? 
Bourgeois et al. (2011) concludeerden op basis van een literatuurreview dat interne 
onderzoekers geschikter lijken voor formatieve (tussentijdse, op leren gerichte) 
evaluaties, terwijl summatieve (ex post, eind-)evaluaties beter passen bij externe 
partijen. Sommige internationale evaluatiehandreikingen gaan kort in op de 
relatieve voor- en nadelen van interne dan wel externe evaluatie. Externe 
onderzoeksteams zullen vaker over specialistische expertise beschikken en als 
onafhankelijk worden beschouwd, wat de geloofwaardigheid van de evaluatie ten 
goede komt. Interne evaluatoren zijn zich meer bewust van institutionele kaders en 
vereisten en hebben sneller toegang tot informatie en sleutelinformanten. Maar zij 
beschikken mogelijk niet over specialistische expertise en zullen minder snel gezien 
worden als onafhankelijk (vgl. EC, 2013a, p. 39).  
 
 
2.2 Programmering en evaluatieverplichtingen  
 
Evaluatie als metgezel van beleid: beleidstheorie en monitoring 
Meer en meer worden beleidsprogramma’s vergezeld van evaluatieplannen (EC, 
2013a) en wordt benadrukt dat al bij het ontwerpen van het programma zelf 
rekening dient te worden gehouden met mogelijkheden voor latere evaluatie. Zo 
moeten programma’s voldoende gefocust zijn, met heldere en niet teveel 
doelstellingen (EC, 2015c; Witte Huis, 2014; HM Treasury 2011a).  
Veel van de evaluatiekwaliteit en precisie hangt af van de condities die worden 
gecreëerd door beleidsmakers.  
Tot die condities behoort allereerst het expliciteren van de beleidstheorie: hoe een 
beleid of wet tot de beoogde doelen moet leiden en wat daarbij de onderliggende 
aannames of principes zijn. Daarbij dienen de belangrijkste proces- en resultaat-
indicatoren te worden benoemd. ‘(...) the more reliable the programme theory, the 
clearer the indicators system, the easier and more precise will be the evaluation. 
(...)’ (EC 2015a, p. 30). Patton (2010) stelt dat evaluatie onderdeel is geworden van 
de ontwikkeling van het beleid en de achterliggende theorie: (...) ‘Whether 
evaluators are present or not, the very notion of theories of change has become so 
prominent that evaluative thinking becomes built into the program design process 
(...)’.  
Niet alleen via de beleidstheorie dragen beleidsmedewerkers bij aan de kwaliteit van 
evaluaties, zij doen dat ook via monitoring. Daarbij worden uitkomsten op cruciale 
indicatoren vanaf het begin van de looptijd periodiek bijgehouden. Monitoring 
embodies the regular tracking of inputs, activities, outputs, reach, outcomes, and 
                                                
46
 http://www.europeanevaluation.org/.  
47
 www.beleidsonderzoek.nl . 
  
 Pagina 33 van 146 
 

 
Nederland is niet het enige land met evaluatiebepalingen in wetten (Ar artikel 164). 
Volgens Jacob et al. (2015) is wereldwijd het gebruik van evaluatiebepalingen in 
zwang geraakt in landen als Denemarken, Duitsland, Nieuw Zeeland en Zwitserland. 
Soms maken deze deel uit van politieke onderhandelingen en helpen het tot stand 
brengen van controversiële wetgeving of coalitieakkoorden.  
Het werken met een evaluatiebepaling is in de VS al sinds de jaren ’60 van de 
vorige eeuw gebruikelijk. De aanvragers (of andere actoren) worden in de VS soms 
zelfs wettelijk verplicht tot het beschikbaar stellen van een budget en het vrijgeven 
van data. Ook worden de ‘ontvangers’ van beleid (met name subsidies) soms 
verplicht een rol in de evaluatie te vervullen (Witte Huis, 2014).   
Twintig OESO-landen zeiden in 2014 een soort van standaard evaluatieverplichting 
te hebben (OESO, 2015a,b). Volgens de OESO is sinds 2008-2009 over het geheel 
van aangesloten landen geen opwaartse ontwikkeling meer zichtbaar in 
automatische evaluatieverplichtingen in wet- en regelgeving – het werken met 
horizonbepalingen laat wel een groei zien.  
De OESO stelt verder dat ex post evaluatieverplichtingen in lidstaten vaak alleen 
van toepassing zijn op primaire wetten, niet op het totale regelgevingsbestand. 
Bovendien zijn wetsevaluatieverplichtingen in de helft van de landen te smal om ex 
post evaluatie van doelbereiking of doeltreffendheid te heten, omdat ze zich 
bijvoorbeeld alleen richten op specifieke (neven)effecten, zoals administratieve 
lasten. In nog minder landen is het verplicht om de externe consistentie van 
wettelijke normen met nationale en internationale regelbestanden te toetsen (in ons 
land evenmin). De OESO concludeert dat over de hele linie maar weinig OESO-
landen een ontwikkeld systeem hebben voor ex post evaluatie van wet- en 
49
regelgeving. 
 
 
2.3 Middelen  
 
Evalueren kan niet voor niets. Internationale instituties (onder andere) hebben 
strategieën en aanwijzingen geformuleerd om een evaluatiebudget vast te stellen en  
binnen de perken te houden.  
De EC acht het evaluatiebudget afhankelijk van beleid en evaluatieopzet, waarbij de 
bepalende factoren zijn: de omvang, duur, reikwijdte en complexiteit van de 
interventie, de omvang en aard van de doelgroepen, de kwaliteit van de beschikbare 
monitoringsystemen en reeds beschikbare -gegevens en beoogde 
evaluatiemethoden (EC, 2015c, p. 257). Om tot een budgettair-proportionele 
evaluatie te komen, kan het volgens de Commissie helpen om een paar in termen 
van reikwijdte en methodologie verschillende evaluatie-opties tegen elkaar af te 
wegen. Bij sociaaleconomische EU-programma’s (EC, 2013a, p. 38) acht men het 
lastig van tevoren in te schatten, hoeveel een evaluatie zal kosten. In geval van 
grootschalige maar relatieve ‘standaard’-beleidsprogramma’s geldt dat de evaluatie 
een klein aandeel van het totale beleidsbudget inneemt – meestal minder dan 1%. 
Bij innovatieve interventies (of pilots) is het leer- en participatieve element groter 
en kan het evaluatiebudget oplopen tot 10%. Het meest bepalend voor het budget 
acht de EC-gids de aard en reikwijdte van het vereiste evaluatiewerk.  
                                                
49
 Het uitwendige, formele systeem zegt echter niet alles. Er zijn enkele landen die, ondanks 
een algemene evaluatieverplichting, geen enkele ex post evaluatie hebben opgeleverd; 
omgekeerd zijn er ook landen die zonder evaluatieverplichting wel aan ex post evaluatie 
hebben gedaan. In 2014 publiceerde de OESO het Framework for Regulatory Policy Evaluation. 
Dit raamwerk staat landen bij in het systematisch evalueren van het ontwerpen en evalueren 
van wetgevings(kwaliteits)beleid, afgezet tegen strategische reguleringsdoelen. Daartoe 
behoren ook de systemen voor ex ante en ex post evaluatie (naast bijv. administratieve 
lasten-reductieprogramma’s).    
  
 Pagina 35 van 146 
 

 
Tabel 3 Criteria voor evaluatievermogen: product en proces  
Kwaliteitscontrole: criteria voor het Kwaliteitsborging: criteria voor het 
evaluatieproduct evaluatieproces 
Tegemoetkomen aan de vereisten in de Terms Coherente en evalueerbare doelen 
of Reference (ToR) of startnotitie  
Relevante scope en reikwijdte Goed opgestelde startnotitie / ToR (zie 
bijlage 3) 
Verdedigbaar onderzoeksdesign en methoden Adequaat proces van aanbesteding 
Betrouwbare data gebruikt Effectieve dialoog en feedback gedurende het 
hele evaluatieproces 
Gedegen analyse Adequate informatiebronnen beschikbaar 
Geloofwaardige conclusies die gebaseerd zijn Goed management en coördinatie door het 
op data en analyse evaluatieteam 
Objectieve conclusies zonder bias en op basis Effectieve verspreiding van rapporten / 
van juiste beoordeling outputs aan ‘evaluatiecommissie’ en 
beleidsmedewerkers / programmamanagers 
Helder rapport met managementsamenvatting Effectieve verspreiding aan (andere) 
en ruwe data in de bijlagen stakeholders 
Bron: EC (2013a).  
 
In het Verenigd Koninkrijk vervult het Magenta Book (HM Treasury, 2011b) een 
belangrijke rol in het tot stand brengen van evaluatievermogen. Het eerste deel is 
ontworpen voor beleidsmakers en legt de vereisten van een goede evaluatie uit, 
naast stappen die beleidsmakers kunnen zetten om die (beter) te faciliteren; deel B 
is technischer. Britse overheidsorganen / directies dienen ervoor te zorgen dat hun 
eigen handboeken of richtlijnen voor evaluatie consistent zijn met de principes in 
51
het Magenta Book. 
De United Nations Evaluation Group (UNEG) publiceerde de Norms for evaluation 
voor binnen het VN-systeem (2005a), aangevuld met de Standards for evaluation 
52
(UNEG, 2005b) over wat een evaluatie geacht wordt te zijn. De primaire 
verantwoordelijkheid voor goed en zinvol evalueren wordt bij de (deel)afdelingen 
binnen de verschillende VN-organisaties gelegd: zij dienen in de naleving van deze 
normen hun eigen evaluatiebeleid te voeren. Daar hoort onder meer bij: een 
expliciete evaluatie ‘missie-statement’, deugdelijke en voortuitziende 
evaluatieprogrammering, borging van onafhankelijkheid en onpartijdigheid. Ook 
dienen zij capaciteits(functie)eisen voor interne evaluatieonderzoekers vast te 
stellen en dient het hoofd van de evaluatie-unit over ervaring te beschikken met het 
uitvoeren en managen van evaluaties.  
De UNEG heeft de normen en standaarden in 2010 uitgewerkt in twee checklists, die 
zowel door onderzoekers als door aanvragers gebruikt kunnen worden bij 
startnotities – Terms of Reference (ToR) genoemd – en afgeronde 
                                                
51
 Het aanvullende Green Book (2011b) zit dichter op de beleidscyclus en benadrukt eveneens 
dat beleidsmakers al rekening kunnen houden met latere evaluatie vanaf het ontwerpstadium 
van beleid.  
52
 Het document met ‘standards’ is qua centrale boodschap vergelijkbaar als de ‘norms’, maar 
meer prescriptief en gedetailleerder. Hieronder vallen ook vereisten voor Terms of Reference 
(ToR) en vormvereisten voor de uiteindelijke evaluaties (schutblad, samenvatting) en voor 
ToR’s.  
  
 Pagina 37 van 146 
 

 
regulatory principles’ van de overkoepelende Raad van Australische regeringen 
(Regering van Australië, 2011, p.2).  
Het is de bedoeling dat beleidsambtenaren zelf de PIR opstellen. De guidance note 
bevat zowel korte als uitgebreide aanwijzingen voor het evaluatieproces, de 
minimum inhoudsvereisten (waaronder relevantie, impact in termen van 
(administratieve) kosten en einddoel) en vereisten ten aanzien van objectiviteit, 
zoals de aanwijzing om zowel positieve als negatieve uitkomsten te benoemen. Alle 
PIR’s worden na beoordeling gepubliceerd. Als doelgroepen worden beschouwd 
besluitvormers en andere stakeholders. 
Op de resulterende rapporten vindt stevige compliance monitoring plaats door een 
eenheid die rechtstreeks onder de centrale regering valt: the Office of best practice 
54
Regulation (OBPR). Naast controleren staat het OPBR departementen bij in het 
voldoen aan vereisten en vergroten van de kwaliteit van evaluatie van regelgeving – 
het vervult dus een tweeledige rol.  
In het Verenigd Koninkrijk in ongeveer dezelfde periode eveneens een format voor 
PIRs van wetgeving tot stand gekomen; sinds 2007 neemt men zich voor om het 
aantal ex post studies te vergroten, onder meer door middel van een Impact 
Assessment Guidance te gebruiken door de evaluatie-aanvrager. De aanleiding 
hiervoor is eerder onderzoek door de Britse Rekenkamer, gebaseerd op 233 
wetgevingsinstrumenten (National Audit Office (NAO), 2009). Hieruit bleek dat in 
maar de helft van de vooraf gedane evaluatietoezeggingen daadwerkelijk een PIR 
55
was uitgevoerd door beleidsdirecties. Er zijn anno 2016 op Internet slechts twee 
56
beknopte verslagen van Britse PIR’s terug te vinden. De kernvragen daarin zijn: 
(1) wat waren beleidsdoelen, succescriteria en beoogde effecten? (2) Hoe en in 
hoeverre zijn deze bereikt – geef daarbij ook onbedoelde effecten aan (3) Voor 
wanneer diende de evaluatie van dit beleid worden opgeleverd? Is die datum niet 
57
gehaald, waarom niet?  
 
Better/ Smart Regulation (EU)  
De EC-website over het programma Better Regulation – in 2010 omgedoopt tot 
Smart Regulation (Smismans, 2015) – kent een apart evaluatiegedeelte. De EC 
publiceert een algemene evaluatieplanning inclusief voor evaluatie van regelgeving, 
58
geordend naar DG. Door te evalueren onderzoekt de Commissie naar eigen 
zeggen kritisch of EU-beleidsinspanningen ‘fit-for-purpose’ zijn en of ze, tegen 
minimale kosten, de beoogde veranderingen bij bedrijven en burgers teweeg 
brengen en bijdragen aan de rol van de EU in de wereld. De Commissie past naar 
eigen zeggen het evaluate first-principe toe om zeker te stellen dat eerdere 
beleidservaringen worden meegewogen in de totstandkoming van nieuwe 
beleidsbesluiten (Smismans, 2015, p. 11). Evaluaties zijn daarmee een essentieel 
onderdeel geworden van de beleidscyclus. Ze zijn onder meer vereist op grond van 
e 59
de Financial Regulation (art. 30 4 lid) en wettelijke bepalingen. 
                                                
54
 De oordelen zijn: compliant of non-compliant. http://ris.dpmc.gov.au.  Het is deze website 
waarop ook de PIRs worden gepubliceerd.   
55
 In 45% van de verplichte (ex ante) Regulatory Impact Assessments (RIA) was die 
toezegging gedaan. Anderzijds is ook in 5% van de gevallen waarin geen toezegging was 
gedaan, toch ook een PIR verricht. In totaal is in ongeveer 54% van alle dossiers een soort 
van evaluatie verricht; vaak naar aanleiding van externe prikkels, zoals EU-verplichtingen. 
56
 www.legislation.gov.uk/ukia?stage=Post%20Implementation%20Review (aug. 2015). 
57
 In een aparte sectie wordt de aanpak van de PIR toegelicht, gevolgd door een beschrijving 
van het probleem en de rationale voor interventie, de kosten, de risico’s/aannames, en de 
‘wider impacts’. Dit alles wordt gevolgd door een conclusie. Een samenvatting van de 
antwoorden op de hoofdvragen is op het voorblad van de PIR te vinden.  
58
 Voor de programmering van 2015 zie: http://ec.europa.eu/smart-
regulation/evaluation/docs/evaluation_forward_plan_2015_en.pdf.  
59
 Bijvoorbeeld de Council Regulation (EU) 2015/323, van toepassing op het elfde Europese 
sociale ontwikkelingsfonds (ESF). 
  
 Pagina 39 van 146 
 

 
evaluaties hebben. Komt het door de wet dat we xx% doelbereiking hebben, of zijn 
andere factoren hiervoor verantwoordelijk?  
Daarentegen houden lerende (formatieve) evaluaties zich bezig met kennis- en 
inzichtvergroting en hebben daardoor vaak een verklarende component. Hoe kan 
het dat ...? Die verklaring valt bijvoorbeeld te bereiken via reconstructies van 
gebeurtenissen, of te onderzoeken of zich patronen voordoen van resultaten van 
beleid in verschillende contexten.  
 
Tabel 4 Black box versus clear box evaluatiebenaderingen  
 Black box  Clear box  
(toetsende, experimentele (verklarende designs) 
designs) 
Causaliteit  Attributie: in welke mate werkt Contributie: hoe draagt het beleid 
het beleid?  bij?  
Verklaring  Weinig aandacht voor hoe het Focus op mechanismen, ingebed 
beleid werkt in context  
Uitkomsten  Beoogde uitkomsten worden van Idem, en: afhankelijk van context  
tevoren gedefinieerd 
Context  De context wordt ‘gecontroleerd’ Context heeft verklarende 
om netto-effect van beleid te waarde 
destilleren What works for whom and under 
Does it work? what circumstances?  
Leren van evalueren Ja, maar ceteris paribus (a- Ja, maar moeilijk overzicht te 
contextueel) bewaren over diverse C, M, en O  
 
Bron: Pattyn & Verweij (2014), met aanpassing.  
 
Summatieve evaluatie (geassocieerd met verantwoorden)  
Aan de overkant van de oceaan, met name in de VS, wordt in ex post evaluatie 
traditioneel de voorkeur gegeven aan designs waarmee resultaten aan beleid 
kunnen worden geattribueerd – ook wel rigorous methods gedoopt (Wereldbank, 
2006; GAO, 2012; Witte Huis, 2014). Voor evaluatie van maatschappelijke 
(welvaarts)effecten wordt counterfactual-analyse voorgestaan: het vergelijken van 
resultaten bij een groep die wel aan de te evalueren beleidsinterventie(s) is 
‘blootgesteld’ (de experimentele groep) met een groep die dat niet is geweest (de 
controlegroep). Daarmee kan de counterfactual-vraag worden beantwoord: wat zou 
de uitkomst zijn geweest zonder het gevoerde beleid? Dit vergt experimentele of 
quasi-experimentele evaluatieaanpakken. Experimentele designs waarbij 
proefpersonen willekeurig (gerandomiseerd) aan de experimentele of de 
controlegroepen worden toegewezen, worden wel randomised controlled trials 
(RCT’s) genoemd. Sommigen beschouwen dit design als het hoogst haalbare, omdat 
dankzij die randomisatie zowel de theoretisch kenbare als niet-kenbare externe 
invloeden op de beleidsresultaten worden uitgeschakeld. De Wereldbank laat – met 
behulp van praktijkvoorbeelden – zien hoe behalve met RCT’s ook met quasi-
experimenten kan worden gewerkt, en hoe daarbij twee belangrijke 
63 
methodologische problemen, contaminatie en selectie-bias kunnen worden 
bestreden.  
Hoe en onder welke contextuele omstandigheden effecten tot stand zijn gekomen, 
blijft een black box; de kracht van experimentele designs zit hem in de relatieve 
zekerheid waarmee resultaten aan beleidsinspanningen kunnen worden 
toegeschreven. Natuurlijk kan hier ook van geleerd worden, in de zin van te weten 
                                                
63
 Bij contaminatie krijgt de controlegroep onbedoeld iets mee van het beleid; bij selectiebias 
verschillen controle- en experimentele groep systematisch van elkaar. In beide gevallen 
kunnen de netto-effecten van het beleid niet valide worden vastgesteld.  
  
 Pagina 41 van 146 
 

 
 
De EC voegt hieraan nog een op de praktijk toegesneden quasi-experimenteel design 
toe, dat handig gebruik maakt van drempelwaarden in ‘scores’ ter afbakening van 
doelgroepen van het beleid, waardoor die deelgroepen toch heel sterk op elkaar 
lijken. Zo is een valide vergelijking mogelijk van groep A (bijv. score 39, komt niet in 
aanmerking voor subsidie) met groep B (score 40, komt wel in aanmerking) waarbij 
groep A als controlegroep kan fungeren.  
Soms is het vormen van een controlegroep niet mogelijk. De GAO (2012) beschrijft 
twee alternatieve designs om toch inzicht te krijgen in de doeltreffendheid:  
1. Tijdreeksanalyse: door met behulp van meerdere metingen voor en na de 
beleidsinterventie statistisch te onderzoeken of er systematische verschillen 
zijn opgetreden na het introduceren van de interventie bij (alleen) de 
doelgroep. Naarmate er meer metingen in de tijd zijn, worden toevallige 
invloeden op de uitkomstmaat steeds meer uitgesloten.  
2. Een cross-sectionele studie (niet-longitudinaal), die gebruik maakt van 
uitkomstmaten bij deelgroepen van de doelgroep, die in verschillende mate 
aan de interventie zijn blootgesteld. Statistische controles dienen ervoor te 
zorgen dat alternatieve verklaringen voor verschillen in uitkomsten zoveel 
mogelijk worden uitgeschakeld (vgl. AEA, 2004).  
 
Niet-experimentele evaluatie  
Dat gerandomiseerd experimenteren in het toetsen van doeltreffendheid wel een 
nastrevenswaardige, maar niet altijd realistische evaluatieopzet is, blijkt onder 
andere duidelijk uit een krachtig Policy Statement van de Europese 
65
Evaluatiegemeenschap (EES) van december 2007 , waarin wordt gereageerd op de 
groeiende neiging in de evaluatiewereld om gerandomiseerd experimenteel 
onderzoek als de gouden standaard (o.a. Farrington, 1997) van 
doeltreffendheidsonderzoek te beschouwen. De EES geeft weer onder welke 
praktijkomstandigheden experimenteel werken vaak niet haalbaar is; zoals:  
- Bij complexe projecten, waarbij meerdere gelijktijdige, en/of snel 
veranderende interventies in het spel zijn. Wetgeving kan daar ook toe 
worden gerekend (vgl. Bussmann, 2010). 
- Als de contexten waarop hetzelfde beleid wordt toegepast (bijv. in geval 
van meerdere regio’s), onderling sterk verschillen. Dat maakt het 
uitschakelen van contexteffecten niet alleen minder haalbaar, maar ook 
minder relevant omdat resultaten maar beperkt generaliseerbaar zijn. Want 
als een effect is gevonden na controle voor context met betrekking tot 
deelbevolking A, wat weten we dan over het effect op deelbevolking B?  
- Als het vormen van experimentele en controlegroepen vanwege 
bijvoorbeeld ethische bezwaren niet haalbaar is (bijv. ontzeggen van hulp 
aan groepen slachtoffers; of het ontzeggen van wettelijke rechten aan een 
groep burgers, teneinde een controlegroep te vormen; vgl. AEA, 2004)  
 
Wijzend op de literatuur pleit de EES in dit soort situaties voor evaluatieonderzoek 
waarbij niet (of niet alleen) wordt ingezet op attributie maar op ‘plausibele 
contributie’: het inzichtelijk maken en zo goed mogelijk staven van verklarende 
mechanismen achter behaalde resultaten, zowel met betrekking tot beleid als 
externe factoren. De beleidstheorie is daarbij leidend.  
Dit verklaart het wel gebruikte antoniem clear box of white box–
evaluatiebenaderingen (o.a. Scriven 1994; Pawson & Tilley, 1997; Mayne, 2008; 
Astbury & Leeuw, 2010), die kunnen dienen als alternatief of ter aanvulling of 
vervanging van de experimentele, ‘black box’ benadering. Wat werkt hoe en in 
welke contexten cq. onder welke condities?  
                                                
65
 http://www.europeanevaluation.org/sites/default/files/EES%20Statement_0.pdf.  
  
 Pagina 43 van 146 
 

 
implementatie? 
Gegevensbronnen 
- Beleidsstatistieken (bijv. gemiddelde inkomensniveau uitkeringsdoelgroep) 
- Administratieve gegevens (bijv. ter toetsing van kwaliteit of snelheid van 
aanmeldings- en selectieprocedures) 
- Surveys onder een representatieve selectie van (lokale) uitvoerders (bijv. is het 
wettelijk kader / de verstrekte toelichting erop voldoende duidelijk?) 
- Combinaties van gegevenstypen verdienen de voorkeur (validiteit, diepgang 
waarmee vragen worden beantwoord)  
Bron: GAO (2012, p. 32).  
 
Fit-for-purpose 
Ongeacht de genoemde ‘stromingen’ is in de praktijk elk evaluatiedesign weer 
anders, omdat het wordt toegespitst op het voorliggende programma en de gestelde 
evaluatievragen (GAO, 2012). De EC (2013b, p. 27) stelt in een handreiking dat een 
solide evaluatieprogrammering, naast experimentele of black-box evaluatie óók 
bevat: theorie-gestuurde evaluatie, procesevaluatie, en doelmatigheids- of 
efficiency-analyse. Deze verschillende aanpakken worden gezien als complementair.  
Fit-for-purpose is steeds vaker het credo: internationale instituties en landen pleiten 
niet meer zozeer voor een bepaald type design, maar voor keuzes hieromtrent die 
goed aansluiten op, met name: (1) de gestelde beleidsdoelen en -prioriteiten, (2) 
het type instrumentarium, (3) het stadium waarin het beleid zich bevindt en (4) het 
evaluatiedoel (o.a. EC, 2013a). Het bewerkstelligen van een op de situatie 
toegesneden evaluatiedesign vergt een actieve rol van de opdrachtgever, die een 
bewuste afweging dient te maken. Onder meer in het Verenigd Koninkrijk (HM 
Treasury, 2011a,b) en bij de EC (2013a) wordt expliciet onderscheid gemaakt 
tussen verschillende te kiezen evaluatietypen afhankelijk van kennisbehoefte en 
situatie. Behalve de genoemde experimentele, proces- en verklarende 
evaluatiestudies op basis van de beleidstheorie staan ‘rapid’ evaluaties in het rijtje 
op basis waarvan relatief snel kan worden bijgestuurd (RIPI, Kautto & Similä, 2005; 
en recenter ook real-time evaluation, o.a. Wereldbank, 2010). 
 
Design van wet- en regelgevingsevaluatie  
In een expert paper voor de OESO pleit Coglianese (2012) voor meer robuuste 
uitspraken over de doeltreffendheid van wet- en regelgeving (regulation and 
66
regulatory policv). Overheden hebben betrouwbare indicatoren nodig die de volle 
breedte van zowel positieve als negatieve uitkomst(indicator)en afdekken. Gepleit 
wordt voor evaluatiedesigns met een experimentele inslag – of anders met 
statistische controles voor externe factoren – om uitkomsten te kunnen attribueren 
aan de wetgeving in kwestie. Daarnaast is er ook een pleidooi voor vaker (ex ante) 
experimenteren met wetgeving. De auteur maakt echter niet expliciet duidelijk voor 
welke specifieke uitdagingen de evaluator van wet- en regelgeving komt te staan in 
vergelijking met programma’s of projecten in het algemeen. Hoe valt bijvoorbeeld 
het zo veel bepleite experimentele design waar te maken als wetten al voor de hele 
bevolking zijn ingevoerd en het vormen van een controlegroep niet meer kan? Hoe 
om te gaan met eventuele ethische bezwaren – al dan niet geobjectiveerd?   
Ook de wetenschappelijke literatuur bevat weinig expliciete handvatten voor 
onderzoeksdesigns specifiek gericht op het evalueren van wet- en regelgeving. De 
spaarzame academische publicaties laten vooral de worstelingen zien bij pogingen 
om de aanpak van de juridisch georiënteerde wetsevaluaties te verrijken met meer 
algemene inzichten en standaarden uit de evaluatiewereld.  
Bussmann (2010) pleit voor wijze benaderingen in het evalueren van de complexen 
van regels en interventies die wetten heten. Maar ook al benadrukt hij het complexe 
                                                
66
 Met de term ‘regulatory’ wordt normstellend, als in wet- en regelgeving bedoeld.  
  
 Pagina 45 van 146 
 

 
o Meet nationale verbeteringen in overkoepelende output-indicatoren – deze 
gegevens kunnen dan ook fungeren als benchmark (richtpunt) voor 
gemeenten of bedrijven  
o (...)  
• Beleid en wetgeving houden vaak veelomvattende, algemene hervormingen in: dit 
vergt gecoördineerde actie in gemeenten in meerdere gebieden, bijvoorbeeld gericht 
op service-verbetering of systeemhervormingen, waarbij verschillende doelgroepen 
moeten worden betrokken.  
o Combineer meerdere evaluatiedesigns: (1) kwantitatief-experimentele, om 
een indruk te krijgen van de mate van (a) doeltreffendheid (b) nationale 
‘dekkendheid’ van beleidseffecten; met (2) representatief gekozen 
kwalitatieve case study- analyses, die het algemene beeld aanvullen door in 
te zoomen op ‘hoe’ de beoogde hervormingen zijn opgepikt en via welke 
processen het beleid een rol heeft gespeeld in de doelbereiking.  
• Sommige programma’s raken in de tenuitvoerlegging volledig ‘vermengd’ met de 
context (bijv. lokaal beleid binnen raamwetgeving). Hoe die context dan te scheiden 
van de effectmeting? 
o Verricht exploratieve case studies (bijv. representatief verdeeld over het 
land), gebaseerd op diepte-interviews en observaties, om een beeld te 
krijgen van wat per context werd bereikt, en hoe.  
• Er lopen verschillende beleidsprogramma’s tegelijk, gericht op dezelfde uitkomst. Dit 
vertroebelt de bijdrage van de afzonderlijke programma’s 
o Stel een logisch model op voor elk afzonderlijk programma 
o Specificeer daarna (a) de scope van de uitkomstmaat per programma zo 
smal mogelijk. Het vinden van resultaten op die smalle uitkomstmaat valt 
met meer zekerheid toe te schrijven aan het betreffende programma dan 
algemene uitkomsten; (b) bezie of het aannemelijk is dat de programma’s 
elkaar onderling beïnvloeden, en toets de resulterende hypothesen gericht 
(door variatie in uitkomsten aangevuld met ‘waarom’ surveys – bijvoorbeeld, 
waarom sommige groepen minder gemakkelijk aan gezondheidszorg komen.  
 
Ook uit de voorgestelde oplossingen blijkt dat evaluatiedesigns in de 
toepassingspraktijk vaak een mix zijn van kwantitatief-experimentele en 
kwalitatieve aanpakken.  
 
2.5 Borging van evaluatiekennis en – ervaring  
 
Communicatie van het evaluatiekader  
Al dan niet in aansluiting op het aangehaalde hulpmateriaal worden wereldwijd vele 
68
cursussen over evaluatie gegeven – die we hier niet allemaal kunnen noemen.  
 
Selectie van onderzoekers: competenties  
Kennis over evaluatieve vaardigheden van onderzoekers is voor aanvragers 
relevant, omdat zij – via aanbestedingsprocedures of anderszins – proberen te 
achterhalen hoe competenties in een team van onderzoekers verdeeld zijn en hoe 
die zich verhouden tot de uitdagingen die met de evaluatie worden verwacht (vgl. 
UKES).  
De EC (2013a, p. 71) benadrukt het belang van goede contacten met 
evaluatieonderzoekers voor de ontwikkeling van evaluatievermogen en 
hoogwaardige evaluatiestudies. De term professionalisering van evaluatie en van 
                                                
68
 Zie alleen al bij de VN bijv. het overzicht op de ILO website: www.ilo.org . Voorts biedt het 
Britse EPPI een workshop aan over programma-evaluatie, mede ontwikkeld voor aanvragers. 
Bijvoorbeeld hoe goede uitvoerders van een evaluatie te selecteren en hoe een 
evaluatievoorstel op te stellen: http://eppi.ioe.ac.uk/cms/Default.aspx?tabid=704.  
  
 Pagina 47 van 146 
 

 
do with findings. It is not apparent or natural to go from data to action and decision making, 
and use will mean different things for different evaluation purposes.’ 
De door Patton geschetste interactie vergt wel een adequate timing van evaluaties 
in de beleidscyclus. Het tijdig beschikbaar komen van evaluatieresultaten is in de 
praktijk niet evident. Het Witte Huis (2014, p. 277-79) geeft aan dat het soms lange 
tijdsverloop tussen de implementatie en het uitkristalliseren van eventuele effecten 
een belemmerende factor kan zijn, vooral voor het gebruik van ‘rigorous’ evaluaties. 
Men doet verslag van een praktijk, de Rapid Cycle- benadering, waarbij via 
vroegtijdige terugkoppeling van data – eerst alleen over proces- maar daarna ook 
resultaatindicatoren – de aanvrager zich zo snel mogelijk een beeld kan vormen van 
de werking van nieuwe interventies. Die resultaten worden vergeleken met die van 
alternatieve (bijv. de oude) aanpakken.  
Jacob et al. (2015) concluderen dat systematisch evaluatiegebruik in de 19 door hen 
onderzochte OESO-landen lastig te bewerkstelligen is. Wel noemen ze de Britse 
overheid als voorbeeld, waar het integreren van onderzoekers in de beleidsdirecties 
en strategische units van departementen een succesvolle manier is gebleken om 
evaluatiebevindingen in het doorlopende beleidsproces – van conceptie tot en met 
implementatie – te verwerken. Wel blijkt uit OESO-onderzoek (2015b) dat in 
nationale wet- en regelgeving steeds vaker gebruik wordt gemaakt van 
horizonbepalingen.  
 
Voor onderzoekers  
Ondanks de sterkere nadruk op de verantwoordelijkheid van de aanvrager is de 
evaluatie-onderzoeker nog steeds een belangrijke actor in de bevordering van 
gebruik. De UNEG (2005a) hanteert als norm dat evaluaties bijdragen aan 
structurele kennisopbouw en organisatieverbetering. Daartoe dienen ze in voor de 
gebruiker begrijpelijke taal te zijn opgesteld.  
Voor de sociaaleconomische EU-evaluaties vereist de EC (2013a, p. 48) niet alleen 
dat analyses en gevolgtrekkingen inhoudelijk gedegen en verantwoord zijn, maar 
ook dat rapporten helder en bondig zijn – rond de 100 pagina’s inclusief 
managementsamenvatting. Diepgaande case studies en kwantitatieve analyses 
horen in bijlagen en het gebruik van samenvattende tabellen en figuren wordt 
aangemoedigd. Aanbevelingen dienen duidelijk te zijn over de vereiste 
vervolgacties.  
 
Follow-up en kennisverspreiding 
De UNEG-normen (2005a) vergen dat (a) het management van de VN-organisatie 
die het betreft, expliciet reageert op aanbevelingen uit de evaluatie (b) dat in geval 
van resulterende actieplannen duidelijk verantwoordelijkheden worden 
weergegeven) (c) dat er systematische opvolging is in de implementatie van 
aanbevelingen (...) en (d) dat er een periodieke rapportage komt met betrekking tot 
de status van die implementatie aan het management.  
Soms worden programma’s afgeschaft als gevolg van negatieve evaluaties. Het 
Witte Huis (2014, p. 282 e.v.) beschrijft een voorbeeld hiervan, voortvloeiend uit 
een experimenteel evaluatiedesign. 
De UNEG hanteert voorts een bepaling die aanstuurt op het aanleggen van een 
repositoir van evaluaties vergezeld van een helder verspreidingsbeleid, ter 
bevordering van verdere kennisvergaring en leren (UNEG, 2005a, p.11).  
Ook in het Smart Regulation beleid van de EC dienen de lessen uit evaluaties 
beschikbaar te zijn en terug te vloeien naar het beginstadium van de beleidscyclus. 
Dit zou moeten gebeuren via het langer bestaande systeem van ex ante Regulatory 
Impact Assessments (RIA) (EC, 2015b,c; Smismans, 2015).   
 
Parlementen en gebruik 
  
 Pagina 49 van 146 
 

 
lange adem is. Er zijn geen snelle oplossingen en het gaat naast om technische 
vaardigheden en toepassing van administratieve instrumenten en richtlijnen óók om 
het kweken van een evaluatiecultuur. Vertrouwen tussen aanvrager en onderzoeker 
is daarbij van belang.  
 
Neveneffecten  
Eerder is in relatie tot (ex ante) Regulatory Impact Assessments ook al gesignaleerd 
dat in de EU en een aantal van haar lidstaten in aanzienlijke mate sprake is van 
justificatory use (Hertin et al., 2007). Men betwijfelt in hoeverre evaluatie een 
integraal onderdeel uitmaakt van de beleidscyclus, zoals in veel 
71
professionaliseringshandboeken als streven wordt benoemd. 
In 2005 publiceerden Schwartz & Mayne een internationaal overzicht van praktijken 
van jurisdicties om de kwaliteit van beleidsevaluaties (en die van audit-onderzoek) 
te monitoren en te verbeteren. Ze bestudeerden daartoe niet-representatief vier 
landen, Nederland, Frankrijk, Canada en Zwitserland, en twee internationale 
organisaties (de Wereldbank en de EU).  
In navolging van Power (1997) onderscheiden de auteurs twee perverse effecten 
van kwaliteitsmonitoring, te weten ontkoppeling en kolonisatie. Ontkoppeling houdt 
in dat kwaliteitsverhogende activiteiten losgekoppeld blijven of worden van de 
intrinsieke activiteiten. Er is dan sprake van ritualistische naleving van 
kwaliteitscriteria, zonder dat dit impact heeft op de dagelijkse praktijk. Bij 
kolonisatie, daarentegen, raken de kwaliteitsstandaarden juist ingebakken in de 
werkzaamheden. Hierdoor kan een soort van uitholling ontstaan, die zich uit in 
problemen zoals tunnelvisie, orthodox werken en gebrek aan innovatie. De 
organisatie zet alles in op voldoen aan de standaarden, ten koste van niet 
gemonitorde of niet meetbare kwesties.  
Een ander ongewenst neveneffect van evaluatievermogen – als dit de vorm 
aanneemt van resultaatgeoriënteerde M&E systemen – is dat ze 
beleidsontwikkelaars kunnen prikkelen om minder ambitieuze doelen te formuleren 
(Clements et al. 2008). Dit kan ontstaan als voorbij wordt gegaan aan de intrinsieke 
(maatschappelijke) waarde van het bereikte: ‘a program that reaches all its timid 
targets may be less cost effective than one that fails to reach ambitious goals’.    
 
2.8 Samenvatting  
 
In dit hoofdstuk zijn internationale ervaringen met bouwen aan evaluatievermogen 
bij een aantal grotere internationale instituties en westerse landen op een rij gezet. 
We bespraken ook enkele internationale studies (Jacob et al. en OESO) die in 
helicopterview belangrijke kenmerken van (wets)evaluatievermogen bespreken, 
gevolgd door verschillen en overeenkomsten tussen landen daarin.  
 
Evaluatie als onderdeel van het beleidsproces: de aanvrager als sleutelfiguur  
Meer en meer dringt de notie door dat de kwaliteit en het gebruik van 
evaluatieonderzoek niet alleen afhankelijk zijn van de onderzoekers, maar juist ook 
van de aanvragers ervan – de beleidsmedewerkers. Zij nemen voor evaluaties 
bepalende beslissingen, bijvoorbeeld over het budget, de reikwijdte en de 
onderzoeksvragen (in de opdracht) en ten aanzien van de verdere begeleiding van 
het evaluatieproces.  
Niet alleen als een evaluatie aanstaande is, maar ook daaraan voorafgaand worden 
randvoorwaarden voor evaluaties geschapen. De inhoud en opzet van het te 
                                                
71
 Anderzijds zijn evaluatiebevindingen slechts een uit een reeks factoren die meewegen in de 
ontwikkeling van beleid. Dit is zeker zo in landen/culturen waar draagvlak en 
stakeholderbetrokkenheid veel gewicht hebben – zoals Nederland (vgl. Leeuw, 2009). 
 
  
 Pagina 51 van 146 
 

 
Hoofdstuk 3 Praktijkervaringen met evaluatievermogen bij 
beleidsdepartementen  
 
 
3.1 Evaluatiebeleid en wetgevingskwaliteitsbeleid  
 
3.1.1 Verantwoording binnen het rijksbrede evaluatiestelstel  
 
In het centrale verantwoordingsstelsel dient evaluatieonderzoek bij te dragen aan 
een antwoord op de vraag, of en in hoeverre het beleid of de wet doelmatig en 
doeltreffend is geweest en welke gewenste en ongewenste effecten teweeg zijn 
gebracht.  
Op basis van een chronologische reconstructie constateert Leeuw (2009) dat 
centrale evaluatie-instituties en het centrale evaluatiebeleid – primair gericht op 
verantwoording van begrotingsuitgaven – zich in Nederland over de afgelopen 
decennia aanzienlijk hebben ontwikkeld. Voor een op basis van meerdere bronnen 
samengestelde tijdlijn zij verwezen naar bijlage 4.  
 
Beleidsdoorlichtingen en evaluatieonderzoek  
Met het in de Regeling Periodiek Evaluatieonderzoek (RPE) van 2006 
geïntroduceerde instrument van de beleidsdoorlichting wordt elk departement 
geacht om bevindingen uit verschillende doeltreffendheids- en andere evaluaties 
hierin samen te brengen (synthetiseren) op het niveau van de beleidsdoelen 
72
(artikelen) in de begroting.   
Departementen verantwoorden dus hun uitgaven aan de volksvertegenwoordiging 
op basis van de beschikbare informatie in de beleidsdoorlichting. De 
beleidsdoorlichting werkt op deze manier door in de ambtelijke evaluatiepraktijken 
waar beleidsevaluaties plaatsvinden. Idealiter stapelt zich hier in de loop van 
(maximaal) 7 jaar het basismateriaal op voor synthese tot de uiteindelijke 
beleidsdoorlichting.  
‘Het is (...) noodzakelijk ervoor te zorgen dat de evaluatieprogrammering aansluit bij de 
programmering van de beleidsdoorlichtingen. Idealiter is op het moment dat een 
beleidsdoorlichting gepland staat voldoende informatie beschikbaar (in de vorm van 
evaluatieonderzoek), zodat een goede beoordeling van de doeltreffendheid en 
doelmatigheid van het gehele beleidsartikel mogelijk is’ (Financiën, 2014). 
 
Volgens artikel 3 van de recentere RPE uit 2014 hoort een beleidsdoorlichting 
informatie over de volgende aspecten te bevatten: afbakening van de onderzochte 
beleidsterreinen; motivering voor het beleid en de beoogde doelen die de overheid 
ermee wil bereiken; beschrijving van het beleidsterrein en de financiële middelen 
die zijn ingezet; overzicht van het al bestaand onderzoek naar doeltreffendheid en 
doelmatigheid en de onderbouwing van de evaluatieprogrammering; de effecten van 
het gevoerde beleid, de doeltreffendheid en doelmatigheid hiervan en de effecten 
73
van het beleid op economische groei en regeldruk. 
In de eerste (tussen)evaluatie van het instrument beleidsdoorlichting – alweer een 
aantal jaren geleden – constateren externe onderzoekers dat ‘op een aantal 
ministeries het instrument beleidsdoorlichting goed blijkt te zijn opgepakt. Het gaat 
                                                
72
 De rapporten zijn openbaar op de website van de Rijksbegroting; 
www.rijksbegroting.nl/beleidsevaluaties/evaluaties-en-beleidsdoorlichtingen/2015. 
73
 In de beleidsdoorlichting horen ook suggesties voor de verhoging van de doeltreffendheid en 
doelmatigheid van het gevoerde beleid. Bovendien dienen de opstellers budgettair 
onderbouwde beleidsopties in kaart te brengen – en de consequenties daarvan af te wegen – 
voor als zich de hypothetische situatie van een daling van middelen met 20 procent voordoet.  
 
  
 Pagina 53 van 146 
 

 
Evaluatiebepaling en wetsevaluatiebeleid 
In 1993 is in de Ar een evaluatieartikel (art. 164) opgenomen in de vorm van een 
standaardformulering voor een in de wet op te nemen evaluatiebepaling. In een 
evaluatiebepaling wordt opgenomen dat – indien wenselijk – een wet éénmalig of 
periodiek wordt geëvalueerd en dat verslag wordt gedaan van de doeltreffendheid 
en de effecten van die wet, of van aspecten of onderdelen ervan. Volgens deze 
Aanwijzing ligt een evaluatietermijn van vijf jaar in de rede. De toelichting vervolgt 
dat voor evaluatie in aanmerking komen de verwezenlijking van gestelde doelen en 
neveneffecten en daarnaast ook voornoemde rits van kwaliteitseisen in de nota 
Zicht op Wetgeving. Welke evaluatievorm wordt gekozen, is volgens de Ar 
afhankelijk van ‘onder meer het gewicht van de betrokken regeling, de 
maatschappelijke betekenis ervan en de aan evaluatie verbonden lasten.’ De 
toelichting stelt dat ‘er een breed scala denkbaar is, van diepgaand 
wetenschappelijk onderzoek tot rapportage door uitvoeringsorganen’.  
In 2000 verscheen de nota Wetgevingskwaliteitsbeleid en wetgevingsvisitatie - 
onder meer naar aanleiding van Europese kwaliteitseisen en ICT-ontwikkelingen (TK 
2000-2001 27 475 nr. 2, p. 4). Wetten moeten vanwege deze ontwikkelingen 
regelmatig worden herzien. Daarnaast benadrukt het kabinet in de nota dat 
wetgeving slechts één van de instrumenten is waarover de politiek en het 
ambtelijke apparaat beschikken om veranderingen in de samenleving te 
bewerkstelligen. Het kabinet benadrukt voorts het belang van samenwerken en het 
benutten van de kennis die vanuit alle niveaus wordt aangereikt om de kwaliteit van 
76
de wetgeving te borgen.  
In de jaren nul van deze eeuw verschoof het accent van een integrale blik op 
kwaliteit naar het belang van uitvoerbaarheid (ibid, p. 5), met name dat nieuwe 
regelgeving geen grote lasten voor de uitvoering en handhaving mag veroorzaken.  
De organisatie van de wetgevingsfunctie per departement kan in theorie de inhoud 
en kwaliteit van wetsevaluaties mede bepalen (onafhankelijke Visitatiecommissie 
Wetgeving 2000). Dit zou kunnen leiden tot verschillen in de kwaliteit van 
wetgeving tussen beleidsdomeinen.  
Een van de onderdelen van het tien jaar later geïntroduceerde Programma 
77
Versterking Juridische Functie Rijk (2010) is het beter laten samenwerken van 
juristen en niet-juristen - zoals probleemoplossend vermogen integreren met 
gedegen juridische analyse en het kweken van wederzijds begrip. Dit zou ook van 
toepassing kunnen zijn op de totstandkoming van wetsevaluaties. 
 
Wetsevaluatiebeleid 
De uitgangspunten voor het huidige wetsevaluatiebeleid zitten besloten in de al 
genoemde oudere nota Wetgevingskwaliteitsbeleid en wetgevingsvisitatie (2000): 
destijds had, buiten de genoemde evaluatiebepaling om een algemeen 
wetsevaluatiebeleid geen prioriteit en werd niet nodig geacht. Inmiddels is de 
aandacht voor wetsevaluaties weer toegenomen, zoals ook beschreven in de 
inleiding van dit rapport.  
In de afgelopen 15-20 jaar zijn ‘aan de voorkant’ (ex ante) steeds meer 
aanwijzingen, checklists en toetsen geïntroduceerd om wetgevingsjuristen te helpen 
bepaalde afwegingen te maken bij het opstellen van wetgeving, variërend van 
administratieve lasten- tot uitvoerings- en handhavingstoetsen. Veel van die 
eerdere instrumenten zijn samen- en teruggebracht in het Integraal Afwegingskader 
                                                
76
 Zo dienen wetgevingsjuristen bij het opstellen van nieuwe wetgeving rekening te houden 
met alle betrokken actoren die een rol spelen in het wetgevingsproces: internationale actoren, 
actoren bij de centrale overheid, decentrale overheden zoals provincies, gemeenten, lokale 
rekenkamers, uitvoerders, handhavers en wetenschappers. 
77
 Dit programma is onderdeel van het programma versterking Rijksdienst en vloeit voort uit 
het rapport ‘Met recht verbonden’ (Commissie Hoekstra, 2007) – om het hoofd te kunnen 
bieden aan nieuwe ontwikkelingen, over de grenzen van beleidsvelden heen. 
  
 Pagina 55 van 146 
 

 
commissies of directies die fulltime of parttime een evaluatie-ondersteunende 
functie vervullen. Deze bespreken we in wat volgt.  
 
Directies Financieel-economische zaken  
De Directie Financiële en Economische Zaken (FEZ) hebben departementen 
gemeenschappelijk als het gaat om evaluaties. Dit is geen toeval: sinds de jaren ’90 
van de vorige eeuw zijn Directies FEZ verantwoordelijk voor het stimuleren en 
coördineren van de evaluatiefunctie (en voor het nagaan van kosten en opbrengsten 
van evaluaties). De FEZ-directie houdt zich primair bezig met 
verantwoordingsdoelen en –processen.  
Desalniettemin verschilt de rol die FEZ vervult in de evaluatiefunctie tussen de 
bestudeerde departementen. Zo heeft de directie bij zowel EZ als SZW een 
aanjagersrol inzake de planning en kwaliteitsborging van het evaluatieonderzoek in 
brede zin (interview 15 juli). Anders is het bij bijvoorbeeld VenJ, VWS en OCW, waar 
de rol van FEZ dichter bij de formele control-taken blijft in relatie tot het centrale 
evaluatiestelsel (interview 1 juli, 2 en 15 september). Het instrument van de 
beleidsdoorlichting is daarbij leidend.  
Bij OCW hebben medewerkers van de Auditdienst Rijk (ADR) in de paar jaar 
80
voorafgaand aan dit onderzoek de beleidsdoorlichtingen op zich genomen.  
FEZ-SZW, met name het cluster Evaluatie en Beleidsinformatie, waar zes personen 
werkzaam zijn, borgt de kwaliteit van de beleidsinformatie en –evaluaties en is 
verantwoordelijk voor het systematisch evalueren van SZW beleid/wetgeving 
(interview, 15 juli). Dit proces loopt net als bij andere departementen voor een 
belangrijk deel via de onderzoekscoördinatoren bij de beleidsdirecties. Voor 
afzonderlijke evaluaties en beleidsdoorlichtingen worden de inhoudelijk betrokken 
81
beleidsmedewerkers aangewezen. Ook ziet FEZ toe op de programmering, 
kwaliteit en de benutting van beleidsevaluaties en -doorlichtingen (SZW, 2009). Zo 
maakt de directie een evaluatieagenda voor evaluaties en beleidsdoorlichtingen, 
adviseert ze al dan niet in een begeleidende rol over de keuze van 
onderzoeksmethoden en draagt ze zorg voor de procesmatige kant van 
evaluatieonderzoek en beleidsdoorlichtingen. 
Ook bij EZ vervult FEZ een spilfunctie. Een aantal relatief jonge 
organisatieonderdelen richt zich in samenwerking met FEZ op de kwaliteitszorg voor 
beleid en evaluatie: de Beleidskwaliteit- en Evaluatie Commissie (BEC, sinds 2013), 
een tot de BEC-structuur behorende Regiegroep Monitoring en Effectmeting (sinds 
82
2011) binnen het DG Bedrijfsleven & Innovatie (DG B&I) en het Behavioural 
Insights Team (BIT, sinds 2014). De BEC houdt zich (onder meer) bezig met 
kwaliteitsborging, kennisdeling en kennisopbouw ter bevordering van de kwaliteit 
van effectevaluaties. Het secretariaat van de BEC is in handen van de Directie 
Algemene Economische Politiek (AEP) en FEZ – de directeur van laatstgenoemde 
directie is BEC-voorzitter. In de BEC zitten voorts portefeuillehouders van elk van de 
DG’s (B&I, Energie, Telecom en Mededinging en Agro & Natuur, de sector 
beleidsinformatie van de Rijksdienst voor Ondernemend Nederland (RVO.nl) naast 
vier externe leden: van CPB, PBL, de VU en de speciaal door EZ aangestelde VU-
hoogleraar beleidsevaluaties (BEC-secretariaat, p.32). De Nederlandse Voedsel- en 
Warenautoriteit (NVWA) (toezicht) en DWJZ zijn agendalid.  
                                                
80
 Bijvoorbeeld in 2015 Cultuur, en ‘Prestaties van leerlingen en studenten omhoog’ en in 
2014: Emancipatie en Mediabeleid. Zie www.rijksbegroting.nl/beleidsevaluaties/evaluaties-en-
beleidsdoorlichtingen.   
81
 Men heeft gebrainstormd over het centraliseren van de onderzoekscoördinatie in een 
afzonderlijke unit, maar uiteindelijk is de structuur gehandhaafd dat elke SZW-directie een 
onderzoekcoördinator heeft.   
82
 In 2016 is de Regiegroep M&E opgegaan in het beleidsanalistenteam (BAT) dat dezelfde 
naast andere functies vervult. 
  
 Pagina 57 van 146 
 

 
van doorlichtingen. Daarnaast wordt gewerkt aan een dekkende programmering van 
onderliggende evaluaties en aan kennisdeling tussen de verschillende directies.  
 
Tabel 7 geeft een overzicht van evaluatie-ondersteunende afdelingen bij 
departementen (exclusief externe onderzoeksinstituten). Niet expliciet genoemd in 
de tabel zijn de beleidsdirecties en daarbinnen de onderzoekscoördinatoren, 
aangezien die doorgaans aanhaken op genoemde organisatieonderdelen.  
Hetzelfde geldt voor directies WJZ bij andere departementen. Voor zover we hebben 
kunnen nagaan zijn wetgevingsdirecties nergens structureel betrokken bij ex post 
evaluatieonderzoek. De Hoofddirectie Bestuurlijke en Juridische Zaken (HBJZ) bij 
IenM heeft in het verleden wel wetsevaluaties gecoördineerd – de nadruk lag veelal 
op de juridische kant. Met de komst van het IAK in 2011 zijn ze wel vaker betrokken 
aan de voorkant, bij de totstandkoming van wetgeving en beleid.  
 
Tabel 7 Organisatieonderdelen bij beleidsdepartementen met taken op het vlak van 
evaluatie en evaluatiekwaliteit  
Departement Afdelingen/directies nauw betrokken bij evaluatieonderzoek 
• FEZ  
BZ 
• IOB - Inspectie Ontwikkelingssamenwerking  
en Beleidsevaluatie 
• FEZ: coördineert sinds september 2014 de evaluatiefunctie 
BZK 
• Voorheen vervulde de sinds 2011 ontmantelde directie 
Kennis & Strategie deze functie  
 
• Directie AEP - Algemene Economische Politiek 
EZ 
• FEZ - Financiële en Economische Zaken 
o BEC - Beleidsevaluatiecommissie 
•    Behavioural Insights Team (BIT)  
• DG B&I – Directoraat-Generaal Bedrijfsleven & Innovatie  
o Regiegroep Monitoring & Effectmeting (sinds 2016 
opgegaan in het Beleidsanalistenteam (BAT)  
• DG Rijksbegroting 
Financiën (stelsel) 
o IRF – Inspectie der Rijksfinanciën 
• FMC - Financiën, Management en Control  
IenM 
• BIT - Behavioural Insights Team (vooral ex ante)  
• KiM – Kennisinstituut voor Mobiliteitsbeleid e.v.a. ‘externe’ 
kennisinstituten 
• FEZ  
OCW 
• Directie Kennis 
 
• FEZ – Financiële en Economische Zaken 
SZW 
• Chief Science Officer (CSO) met werkgroep en andere 
kennisonderdelen  
• FEZ (vooral gericht op beleidsdoorlichtingen)  
VenJ 
• Wetenschappelijk Onderzoek- en Documentatiecentrum 
(WODC) 
o Afdeling EWB, Extern-wetenschappelijke 
betrekkingen (betrokken bij uitbesteding evaluatie- 
en overig onderzoek) 
o Interne onderzoeksafdelingen  
• FEZ  
VWS 
• ZonMw  
o CER - Commissie Evaluatie Regelgeving. Via het 
gelijknamige programma (PER) wordt een deel van 
de externe evaluaties van wet- en regelgeving 
extern verricht 
 
Kennisdirecties  
  
 Pagina 59 van 146 
 

 
sprake van een deels extern kennisnetwerk, zoals Wageningen UR met daaronder 
88
onderzoeksinstituten zoals Alterra of het LEI. Deze onderzoeksinstituten voeren 
soms evaluatieonderzoeken uit voor EZ. 
De situatie bij IenM laat zich vergelijken met EZ; er is een kennisprogramma 
gekoppeld aan de strategische kennis-en innovatieagenda (IenM, 2013). Een 
belangrijke rol – eveneens aan de voorkant – vervult voorts het Behavioural 
Insights Team (BIT), een netwerk van kennisinstellingen op het gebied van gedrag 
dat IenM ondersteunt bij de toepassing van gedragsinzichten in het maken en 
implementeren van beleid en wetgeving (Kennisplatform Crow, 2014). Verder is er 
89
een leergang Kennis voor Beleid. 
De Directie Kennis, innovatie en strategie voert de regie op de kennisfunctie bij 
IenM. Deze stuurt op een groot netwerk van kennisinstituten en wetenschappers 
90
bestaande uit een binnenring (KiM, RIVM, PBL, KNMI, RWS-WVL) en buitenringen. 
Bij het Kennisinstituut voor Mobiliteitsbeleid (KIM) is beleidsevaluatie de laatste van 
zes kennislijnen (KiM, 2015). Het PBL en het KiM verrichten voornamelijk ex ante 
evaluaties en monitoring en zijn minder actief op het vlak van ex post evaluatie 
91
(vgl. Berveling et al., 2009; interview 16 juli).  
Verbeterpunten die in een online presentatie genoemd worden t.a.v. de 
kennismanagementfunctie bij IenM zijn: landing van kennis in beleid, versnippering 
van budgetten en vraagsturing, zwakke vraagarticulatie, te veel focus op de korte 
termijn en beperkte samenwerking en concurrentie tussen kennisinstellingen 
(Riedstra, IenM Kenniscongres 2 april 2014). Dit lijken aandachtpunten die verder 
reiken dan alleen het domein van IenM.  
Onder het DG Bestuur en Koninkrijksrelaties van het ministerie van BZK valt in het 
huidige organogram het Bureau Verkenning en Onderzoek (BVO), dat naast 
strategische kennisontwikkeling en kennismanagement ook een rol vervult ten 
aanzien van evaluaties (en wetsevaluaties). BVO draagt zorg voor de DG-brede 
onderzoeksprogrammering en – budget en voor de procesmatige en inhoudelijke 
begeleiding va het onderzoek. Bij het DG Wonen en Bouwen is er de directie Kennis 
92
& Verkenningen, met een vergelijkbare functie (interview 13 oktober).  
Hoewel veel van de genoemde instituten vaak ook evaluaties verrichten en 
bijdragen leveren aan evaluatiekennis en -inzichten voor departementen, bevatten 
de verschillende geraadpleegde kennisagenda’s zelden verwijzingen naar 
evaluatieonderzoek.  
 
Onderzoeksinstellingen bij en voor departementen 
Onderzoeken waaronder evaluaties voor de ministeries van VenJ – en BZK – worden 
verricht door of via het ongeveer 95 fte tellende Wetenschappelijk Onderzoek- en 
Documentatiecentrum (WODC), dat een zelfstandige positie heeft ten opzichte van 
93
het aanvragende ministerie. Het WODC doet drie soorten onderzoek: 
                                                
88
 www.wageningenur.nl/nl/Expertises-Dienstverlening.htm biedt een overzicht.  
89
 Presentatie Riedstra, IenM Kenniscongres d.d. 2 april 2014 (online gepubliceerd).  
90
 SWOV, CROW, CPB, SCP, TNO, Deltares, NLR, IMARES en verder o.a. universiteiten en 
NWO. 
91
 In deze KiM-publicatie ‘Na het knippen van het lint’ pleiten de auteurs voor het vaker doen 
van (lichte) ex post evaluaties van infraprojecten om te bezien wat hier de meerwaarde van 
kan zijn. De nadruk ligt bij infrastructuurprojecten sterk op ex ante onderzoek (‘Wie een 
beslissing moet nemen over de aanleg van infrastructuur gaat niet over één nacht ijs’).   
92
 www.rijksoverheid.nl/ministeries/ministerie-van-binnenlandse-zaken-en-
koninkrijksrelaties/inhoud/organisatie/organogram/directoraat-generaal-wonen-bouwen-en-
integratie. Met de reorganisatie van BZK, vanaf april 2016, moeten BVO en K&V 
geïncorporeerd zijn binnen de dan nieuwe directies.  
93
 www.wodc.nl/organisatie. Soms verricht het WODC ook (evaluatief) onderzoek in opdracht 
van het ministerie van BZK, zoals bijvoorbeeld op het terrein van koninkrijksrelaties en als 
gevolg van verschuivingen tussen VenJ en BZK in departementale ‘portefeuilles’ tijdens de 
looptijd van evaluaties (bijv. op het vlak van migratie en politie).  
  
 Pagina 61 van 146 
 

 
Een overeenkomst tussen de bestudeerde departementen is dat de 
wetgevingsdirecties weinig betrokken zijn bij ex post evaluaties van wetten (alle 
interviews). Zo stelt een respondent bij IenM met ongeveer 30 jaar ervaring als 
wetgevingsjurist dat hij slechts enkele wetsevaluaties voorbij heeft zien komen, 
ondanks dat deze zijn warme belangstelling hebben (interview 24 augustus).  
Wel is DWJZ nogal eens de formele opdrachtgever als meerdere beleidsdirecties bij 
een wetsevaluatie betrokken zijn (sectoroverstijgend) (interview, 1 juli). Een 
voorbeeld zijn de opeenvolgende evaluaties van de Awb (par. 3.5).  
Directies Wetgeving zijn bovendien meer nadrukkelijk betrokken bij het begeleiden 
van ex ante onderzoek en met het toetsen van nieuwe wetsvoorstellen aan de IAK-
richtlijnen (interviews 9, 16 en 24 juli).  
 
Intern uitgevoerde versus extern uitbestede evaluaties 
Zoals op basis van eerder onderzoek verwacht (H1), is uit alle gehouden interviews 
gebleken dat evaluaties en wetsevaluaties op rijksniveau voor het (over)grote 
merendeel uitbesteed worden aan externe partijen zoals academische 
onderzoekteams en onderzoeksbureaus.  
Voor het uitbesteden van evaluaties zijn een drietal verklaringen gegeven door 
respondenten: (1) Het ontbreekt directies aan tijd en capaciteit; het verrichten van 
evaluaties komt bovenop de andere taken van beleidsmedewerkers. ‘Het is 
eenvoudiger om er euro’s tegenaan te gooien’ (interview 26 augustus en 15 
september); (2) De uitvoering van evaluaties wordt vaak aan externe onderzoekers 
overgelaten omdat specialistische kennis vereist is; en ten slotte (3) wordt extern 
onderzoek gezien als bevorderlijk voor objectiviteit (merendeel interviews).  
Bij OCW worden evaluaties standaard extern uitgezet. Het grootste deel hiervan 
komt tot stand via het raamcontract. Raam- of mantelcontracten worden voor een 
vooraf bepaalde periode afgesloten met een vaste groep van onderzoeksinstellingen 
en –bureaus. De selectie, die verloopt via een meerledige procedure, is gebaseerd 
96
op de competenties en ervaring die de partijen bij aanvang in huis hebben. VenJ 
en SZW werken niet met raamcontracten, binnen BZK gebeurt dit her en der wel 
(interviews 15 juli en 13 oktober).  
Een klein deel van het onderzoek voor OCW wordt uitgezet via het Nationaal 
Regieorgaan Onderwijsonderzoek (NRO) (interview 1 juli). Het NRO is opgericht in 
2012 met als doel praktijk, wetenschap en beleid samen te brengen; het valt onder 
de verantwoordelijkheid van de Nederlandse organisatie voor wetenschappelijk 
onderzoek (NWO) (interview 1 juli). Sinds 1 januari 2014 is het NRO volledig 
operationeel met een eigen programmering – van voornamelijk fundamenteel en 
97
praktijkgericht onderzoek – en een structureel jaarbudget van bijna 15 miljoen.  
Voor VenJ, BZK en VWS worden (wets)evaluaties ook door WODC en ZonMw en SCP 
uitgevoerd. Bij het WODC wordt een relatief groot deel van de evaluaties via de 
afdeling EWB weer ‘door’-uitbesteed (interviews 8 juni).  
Nieuwe organisatieonderdelen en structuren ter bevordering van ex post evaluaties 
vergen volgens meerdere respondenten een wenperiode: het is zoeken naar een 
rolverdeling die niet alleen op papier maar ook in de praktijk werkt (interviews 1 
juli, 9 juli, 26 augustus).  
 
3.2.3 Samenvattend over structuur 
 
                                                
96
 Zo hebben beoordelaars bestaand onderzoekswerk van inschrijvende partijen bestudeerd of 
deze inhoudelijk voldoende ‘in huis hebben’ – toegespitst op één van de 15 expertisegebieden 
of ‘percelen’. Dit gebeurde na een aanvankelijke beoordelingsronde met behulp van een reeks 
uitsluitingsgronden, minimumeisen en ‘knock-out’-criteria (noodzakelijke voorwaarden) 
(comm. 16 feb 2016 en deskmateriaal). De OCW-Kennisagenda 2015 bevat overzichten van 
gecontracteerde en gesubsidieerde partijen.   
97
 http://www.nro.nl/.    
  
 Pagina 63 van 146 
 

 
organen – hier naar eigen zeggen gerichter op gaan sturen. Idealiter is de 
programmering dus zo opgebouwd, dat deze leidt tot evaluatierapporten waarin 
voldoende informatie staat om onderbouwde uitspraken over doeltreffendheid en 
doelmatigheid mogelijk te maken. Bij sommige departementen is dit de afgelopen 
jaren een worsteling gebleken, wat zich weerspiegelt in de inhoud en kwaliteit van 
beleidsdoorlichtingen (interviews 1 juli en 16 juli). Informanten bij de centrale 
evaluatie-instituties van het ministerie van Financiën en de Algemene Rekenkamer 
98
hebben dit bevestigd (26 augustus en 1 september). Veel respondenten gaven aan 
dat niet al het onderzoek dat in de evaluatie- en onderzoeksprogrammering van de 
Rijksbegroting staat, ook daadwerkelijk bruikbaar is bij de invulling van de 
beleidsdoorlichtingen – bijvoorbeeld omdat het zich niet primair richt op 
doeltreffendheid en doelmatigheid. Dit laatste hoeft ook niet steeds het streven te 
zijn, er zijn immers vele typen evaluatieonderzoek – waaronder proces- en 
werkingsevaluaties – die bruikbare kennis voor beleid genereren.  
Onder meer bij OCW bestond behoefte aan een overzicht van welk afgerond 
evaluatieonderzoek voor beleidsdoorlichtingen zou kunnen worden gebruikt; hier is 
dan ook met mensen van de ADR aan gepuzzeld (interview 1 juli). Maar deze 
respondenten stellen ook, dat veel vooruitgang is geboekt op het gebied van 
doeltreffendheidsevaluatie (interview, 1 juli).  
FEZ-SZW voert gesprekken met beleidsdirecties over de evaluatieagenda en - 
planning. Dit gebeurt vooral in het verband van beleidsdoorlichtingen en met als 
doel gedegen doeltreffendheidsevaluaties te laten verrichten die daarvoor relevant 
en bruikbaar zijn. Ook helpt FEZ overzicht te scheppen in de programmeringen van 
verschillende directies (interviews 15 juli, 13 oktober).  
Bij EZ heeft FEZ een interne Handreiking evaluatieonderzoek (FEZ-EZ, z.j.) 
opgesteld (zie verder par 3.5), waarin ook richtlijnen staan voor de 
evaluatieprogrammering. ‘Reeds bij het opzetten van een instrument wordt idealiter 
nagedacht over het evalueren van het instrument. Voor ieder instrument zou 
duidelijk moeten zijn wanneer de evaluatie plaatsvindt’ (p.9). De Regiegroep 
Monitoring en Evaluatie (M&E) van DG B&I – inmiddels BAT geheten – en de BEC-
contactpersoon (bij de andere DG’s) vervullen daarbij een sturende rol. Een goede 
evaluatieprogrammering is vereist om de effecten van subsidies en beleid door 
middel van evaluatie te kunnen aantonen. De programmering van het 
evaluatieonderzoek wordt afgestemd op de beleidsdoorlichtingen, zowel qua timing 
als qua afdekking van beleidsartikelen.  
Een subsidieregeling vervalt automatisch na maximaal 5 jaar – of wettelijk anders 
bepaalde termijn, tenzij de Tweede Kamer voorafgaand aan het aflopen van de 
subsidieregeling heeft ingestemd met een voorstel om de regeling voort te zetten. 
Het is dan ook zaak om, voorafgaand aan het voorstel tot eventuele voortzetting, 
een mede op een evaluatie gebaseerde onderbouwing te hebben. Ook voor 
beleidsdoorlichtingen wordt bij EZ in de praktijk een cyclus gehanteerd van 5 jaar.  
Omdat ex post evaluatie bij EZ primair als een beleidsaangelegenheid en onderdeel 
van de beleidscyclus wordt gezien, speelt DWJZ geen expliciete rol bij de 
evaluatieprogrammering – wel is de wetgevingsdirectie actief aan de voorkant, 
onder meer via betrokkenheid bij het IAK en door deelname aan de 
Monitorcommissie, waar nieuw en aangepast beleid wordt getoetst. De 
beleidsdirecties zijn verantwoordelijk voor hun eigen evaluatieplanning en stemmen 
dit af met FEZ (FEZ-EZ z.j., p. 9).  
BZ heeft een evaluatieprogrammering voor vijf jaar, die jaarlijks wordt bijgesteld. 
Deze bevat alleen beleidsdoorlichtingen en andere evaluaties naar doelmatigheid en 
doeltreffendheid – geen ‘overige onderzoeken’ (interview, 2 september). Ook hier 
                                                
98
 Zie ook 
http://verantwoordingsonderzoek.rekenkamer.nl/2014/rijksbreed/beleidsresultaten/beleidsdoo
rlichtingen-nog-te-weinig-zicht-op-doelmatigheid-en  
  
 Pagina 65 van 146 
 

 
Het aandeel proces- en ex post evaluatie binnen de door ons geraadpleegde 
102
strategische kennisagenda’s is verwaarloosbaar. 
De laatste jaren zou – volgens een onderzoekscoördinator bij tenminste één van de 
DG’s van OCW– de onderzoeksprogrammering budgettair onder druk zijn komen te 
staan (26 augustus). Het niet teveel willen belasten van de organisaties in het 
onderwijsveld speelt daarbij eveneens mee.  
Bij het ministerie van VenJ worden jaarlijks twee programmeringsrondes van 
evaluatie- en ander onderzoek geïnitieerd door het WODC (WODC, 2015). Voor 
iedere programmeringsronde worden de onderzoekwensen – al dan niet via de 
coördinatoren onderzoek – bij de beleidsdirecties en DWJZ geïnventariseerd aan de 
hand van een door het WODC opgestelde standaardvragenlijst. Het resulterende 
overzicht van onderzoeksvoorstellen wordt in het MT-WODC besproken. Dan wordt 
bepaald welke onderzoeken de komende periode zullen worden verricht en wordt de 
keuze gemaakt of het onderzoek WODC-intern wordt uitgevoerd of via de afdeling 
EWB wordt uitbesteed. De conceptprogrammering wordt in de Bestuursraad 
geaccordeerd en vervolgens gepubliceerd via de website van het instituut. De 
directie FEZ speelt geen nadrukkelijke rol in de programmering van individuele 
VenJ-evaluaties, behalve dat in de WODC-vragenlijst onder ‘evaluatieonderzoek’ een 
item is opgenomen waarin directies wordt gevraagd naar of en hoe het 
voorgenomen evaluatieonderzoek past binnen de meerjarenplanning van de 
beleidsdoorlichtingen – die uiteraard wel door FEZ worden gecoördineerd.  
 
3.3.3 Evaluatiebepalingen in wetten 
 
Tien tot 20% van recent geëvalueerde wetten bevat een evaluatiebepaling, een 
wetsartikel dat vaststelt wanneer de wet zal worden geëvalueerd om inzicht te 
103
geven in de werking ervan (Veerman, 2013). Dat gebeurt in de regel vijf jaar na 
inwerkingtreding, hoewel een tussentijdse evaluatie ook mogelijk is in het geval dat 
de implementatie van de wet anders verloopt dan verwacht, of de Tweede Kamer 
eerder om informatie vraagt. Een wet kan eenmalig of periodiek worden 
geëvalueerd (Aanwijzing 164 voor de regelgeving).  
Evaluatiebepalingen worden volgens ingewijden vaker dan voorheen ingezet als 
politiek ‘wisselgeld’. Voor wetten die geen evaluatiebepaling bevatten, kan de 
Tweede Kamer, indien noodzakelijk, ad hoc opdracht geven tot evaluatie 
(Bemelmans-Videc, 2002). Dit laat zien dat ook wetsevaluaties zich in een politieke 
context bevinden en hun aanleiding soms direct ligt in de politieke realiteit.  
Hoewel enkele respondenten menen dat evaluatiebepalingen in de praktijk vanwege 
uitstel of afstel niet altijd worden nageleefd, is hier geen systematisch onderzoek 
naar verricht. Bij OCW houdt DWJZ een kalender bij met daarin de 
evaluatietermijnen van wetten naar aanleiding van de evaluatiebepalingen (e-mail 
104
14 juli). 
De termijnen in de evaluatiebepalingen – of naar aanleiding van een 
Kamertoezegging – worden door respondenten bij Hoofddirectie Bestuurlijke en 
Juridische Zaken (IenM) vaak te kort gevonden. Zo concludeerden onderzoekers 
naar aanleiding van de eerste – door artikel 5.9a verplichte evaluatie van de 
effecten van de Crisis- en Herstelwet (‘binnen twee jaar’) dat na die termijn slechts 
twee rechterlijke uitspraken te vinden waren en toch geforceerd conclusies zijn 
                                                                                                                             
beschikbaar is en komt, in context te plaatsen en beter te benutten.’(OCenW, 2015).  
102
 Geraadpleegd zijn de (strategische) kennisagenda’s van de ministeries van SZW (2013); 
OCW (2015b); IenM (2013) en BZK (2010).  
103
 Veerman (2013), mondelinge communicatie. Volgens sommige respondenten neemt het 
aandeel wetten met een evaluatiebepaling toe (o.a. interviews, 16 juli 2015); dit is niet nader 
onderzocht.  
104
 Dit gebeurt via PARIS, het departementale systeem van toezeggingen aan de TK.  
  
 Pagina 67 van 146 
 

 
Berveling et al. (2009:5-6) zien vier typen oorzaken van weinig animo voor ex post evalueren 
– naar aanleiding van infrastructuurprojecten bij IenM (vgl. interviews 2 september ...).  
1. Vanuit de politiek-bestuurlijke en beleidscontext speelt de wens om vooruit te kijken een 
belangrijke rol. Het ligt in de aard van beleidsmakers om bezig te zijn met de toekomst, niet 
met het verleden. 
2. Daarnaast staan bepaalde psychologische processen de wil tot evalueren in de weg. Bij de 
start van een project kan een zogenaamde ‘optimism bias’ een rol spelen. Dit houdt in dat 
betrokkenen (ex ante) vaak een te optimistisch beeld hebben van een project. Dit leidt ertoe 
dat men zich later vaak verzet tegen alles dat het positieve beeld verstoort. Meer in het 
bijzonder kan een negatieve evaluatie leiden tot reputatieschade. 
3. Organisatorische belemmeringen kunnen de praktische uitvoering van evaluaties in de weg 
staan. Projectdirecties worden vaak direct ‘na het knippen van het lint’ opgeheven. Bovendien 
kan een gebrek aan geld en capaciteit er toe leiden dat ex-postevaluaties worden verdrongen 
door meer urgente zaken. Daarnaast kan het natuurlijk personeelsverloop, in combinatie met 
de lange doorlooptijden van infrastructuurprojecten, leiden tot verwatering van kennis en 
expertise.  
4. De belangrijkste methodologische problemen zijn dat het moeilijk is om effecten van 
projecten te isoleren en om te weten hoe de wereld er zonder het project had uitgezien. 
Daarnaast is het moment van evalueren een punt dat aandacht verdient. 
 
3.3.5 Samenvattend over programmering  
 
Beleids- en wetsevaluaties vinden voor een belangrijk deel plaats in het kader van 
het centrale verantwoordingsstelsel. Daarnaast zijn er de (periodieke) 
evaluatieverplichtingen in de wetgeving zelf; die zo eveneens een programmering 
inhouden. Beleids- en wetsevaluaties kunnen, eenmaal afgerond, door directies 
worden gebruikt als basismateriaal voor een beleidsdoorlichting.  
Eventuele extra of parallelle onderzoeksprogrammeringen, zoals strategische 
kennisagenda’s, bevatten nauwelijks ex post evaluatieonderzoek en zijn vooral 
toekomstgericht, zoals ex ante- en probleemverkennend onderzoek.  
In de context van verantwoording zijn de afgelopen paar jaar door genoemde 
organisatieonderdelen verschillende initiatieven genomen om 
evaluatieprogrammeringen proactiever te maken, vooral met het oog op 
aankomende beleidsdoorlichtingen en het meer doordacht onderzoeken van 
doeltreffendheid/doeltreffendheid en doelmatigheid. We hebben niet overkoepelend 
kunnen vaststellen wat hier de vruchten van zijn – bijvoorbeeld in termen van 
minder lacunes in beleidsdoorlichtingen.  
Een Panel van Advies signaleerde in 2011 dat de nadruk in de 
evaluatieprogrammering van BZ en IOB sterk op verantwoorden ligt (het afdekken 
van de begroting) en weinig selectief tot stand komt, bijvoorbeeld via raadpleging 
van stakeholders. Dat verantwoorden in evaluatieprogrammeringen meer nadruk 
krijgt dan leren, lijkt relevant voor meer bestudeerde departementen.  
De politieke context werkt door in de evaluatieprogrammering. Sommige 
respondenten relativeren het feitelijke belang van een programmering: zij stellen 
dat evaluaties vooral samenhangen met veranderingen in kabinetssteun voor een 
beleid of met beleidsaanpassingen en -ontwikkelingen.  
 
3.4 Middelen  
 
Voor (adequaat) evaluatieonderzoek zijn, naast de aanwezigheid van voornoemde 
aspecten van evaluatievermogen, ook middelen nodig.  
De meeste departementen stellen eerst een integraal budget voor onderzoek vast. 
Alle uit te voeren evaluaties moeten binnen dat financieel kader passen. In de 
praktijk gaat men uit van de bestedingsplannen, voortkomend uit de jaarlijks in 
  
 Pagina 69 van 146 
 

 
Tabel 8 Evaluatiekaders (ex post) bij de onderzochte departementen 
(momentopname 2015; niet uitputtend) 
Departement Programma’s, handreikingen en richtlijnen voor Openbaar of 
evaluatieonderzoek intern 
• Handreiking Evaluatieonderzoek ex post 
Centraal / Openbaar   
(Financiën, 2003) 
overkoepelend   
• Handreiking Beleidsdoorlichtingen (Financiën, 
2016) 
• Cursussen bij Academie voor Wetgeving over 
IAK en over ex post evalueren 
• Rijksacademie voor Financiën, economie en 
bedrijfsvoering (2015). Beleid in perspectief   
 
• Evaluatiebeleid en richtlijnen voor evaluaties, 
BZ Openbaar  
(IOB, 2009)  
• Meerdere evaluatiehandreikingen op het vlak 
van ontwikkelingssamenwerking 
• Commissie Theeuwes (2012). Durf te meten 
EZ Intern  
Eindrapport Expertwerkgroep Effectmeting  
 
• FEZ-EZ (z.j.) Handreiking evaluatieonderzoek 
 
EZ  
• BEC-secretariaat (z.j.), Handreikingenboekje  
beleidskwaliteit en beleidsevaluatie  
 
• Evaluatiecursus op basis van deze en andere 
materialen – later overgeheveld naar de 
Rijksacademie voor Financiën, economie en 
bedrijfsvoering 
• Tot 2010: Programma Structurele Evaluatie 
IenM Openbaar  
Milieuwetgeving (STEM) 
Intern  
• Begrotingsevaluatieprogramma (nota 2013).  
• CPB (2011). Notitie Zicht op effectiviteit. 
OCW, directie Intern  
Evaluatieontwerpen voor onderwijs- en 
Kennis   
wetenschapsmaatregelen, bedoeld om de 
 
quasi-experimentele design te stimuleren  
Openbaar  
• Kohnstamm Instituut, Pater et al. (2012). 
Methodiek voor Verklarende evaluatie   
 
 
• FEZ-SZW (2014). Handreikingen en 
SZW Intern  
sjablonen voor proces en inhoud 
Nb overlapt sterk met 
beleidsdoorlichtingen. Met o.a. formats voor 
latere Handreiking 
de uitbesteding van het onderzoek, het 
opstellen van de startnotitie, planning, beleidsdoorlichtingen 
aandachtspunten bij onderzoeksvragen, de 
(Financiën, 2016) 
hoogte van vergoeding voor onafhankelijk 
onderzoek, de afspraken met onafhankelijke 
deskundigen en het opstellen van de nota 
aan minister en de kabinetsreactie. 
Procesmatig    
VenJ  
• Wegwijzer WODC-EWB (2012) bij 
Openbaar  
contractonderzoek in opdracht van het WODC 
 
• Handboek WODC-EWB (2008)- met name 
bedoeld voor nieuwe EWB-medewerkers  Intern  
Inhoudelijk 
 
• Kautto & Similä (2005). RIPI’s: Recently 
Openbaar  
Introduced Policy Instruments and 
  
Interventions Theories  
• Van Ooyen-Houben en Leeuw (2010). 
Evaluatie van justitiële (beleids)interventies    
• Van Gils (2014) Real time evaluation  
• Overige wetenschappelijke bronnen 
• Programma Evaluatie Regelgeving (PER, 
VWS Openbaar  
1997). Ontwikkeld door ZonMw. Evaluatie 
 
door Olsthoorn-Heim (2003) waarvan de 
 
aanbevelingen in vervolgprogramma zijn 
 
verwerkt.  
• Brochure Programma Evaluatie Regelgeving 
 
  
 Pagina 71 van 146 
 

 
Belangrijkste aanbevelingen  
- Heldere opdrachtverlening met de juiste deskundigheid en in de juiste infrastructuur 
- Vroeg beginnen met het voorbereiden van een wetsevaluatie op het departement; 
bijvoorbeeld aan de hand van een nulmeting, vroeg experimenteren en ook het 
overwegen van thematische (wetsoverstijgende) evaluatie. Tegelijkertijd: geduld 
betrachten bij het plannen van een evaluatie van doelbereiking of doeltreffendheid – 
de werking moet zich eerst kunnen uitkristalliseren.  
- Voor meer uniformiteit/vergelijkbaarheid: het beter bewaken van de (nieuwe) 
programmatekst met onderscheid tussen eerste en volgende evaluaties en nieuwe en 
bestaande wetten; en het door de BC laten toetsen van wetsevaluaties aan de 
programmatekst.  
- Kansen en mogelijkheden voor de thematisering van wetsevaluaties  
- Het uitbreiden van expertises in de CER van gezondheidsrecht en - zorg, met 
expertise op het vlak van wetgeving, bestuurskunde en rechtssociologie; en ‘buiten’ 
ook het werven van de verschillende disciplines voor het indienen van 
projectvoorstellen. 
- Meer uitgewerkte eisen aan de aanpak of onderzoeksmethode – zonder de 
onafhankelijkheid en creativiteit van onderzoekers te bedreigen, zoals: schetsen van 
theoretisch kader, vaststellen van beoordelingsmaatstaven, heldere vraagstelling, 
verantwoord evaluatiedesign, voldoen aan vereisten van 
validiteit/betrouwbaarheid/consistentie. 
- Bijeenkomst ter afsluiting – met samenvatting op de website – ten behoeve van 
bredere verspreiding en eventueel publicatieplan van betrokken onderzoekers.  
 
De aanbevelingen voortvloeiend uit deze evaluatie zijn na de evaluatie in het nieuwe 
112
PER verwerkt. Het PER vat procesmatige en inhoudelijke criteria voor evaluatie 
van wet- en regelgeving samen in de volgende punten:  
(1) wetsevaluaties moeten door een multidisciplinaire groep worden uitgevoerd met 
aandacht voor zowel het empirische als voor het juridische deel;  
(2) kennisnemen van de beginsituatie – zo mogelijk een nulmeting – moet 
113
onderdeel zijn van de evaluatie ;  
(3) wetsevaluatie beperkt zich niet tot de te evalueren wet, maar houdt ook 
rekening met de internationale en Europese context;  
(4) wetsevaluaties kijken zowel naar gewenste als ook ongewenste effecten;  
(5) wetsevaluaties moeten aandacht schenken aan de relatie tussen wetgeving en 
zelfregulering – een criterium verbonden aan het subsidiariteitsbeginsel;  
(6) elke nieuwe evaluatie moet naast nieuw onderzoek ook gebruik maken van 
bestaande kennis, ter voorkoming van verspilling van tijd en geld; en tot slot  
(7) wetsevaluaties besteden aandacht aan diversiteit en aan de toegankelijkheid van 
alle bevolkingsgroepen tot de wet (ZonMw, 2013, p. 4-5).  
 
Op de website van ZonMw verschijnt een oproep tot het indienen van subsidie-
aanvragen. Deze aankondiging bevat de project- of thematekst en de deadline voor 
114
het indienen van een aanvraag. Bij de selectie van het winnende project hanteert 
de CER een aantal wetenschappelijke kwaliteitscriteria. Daaronder vallen de 
helderheid, originaliteit en haalbaarheid van de vraagstelling, de aansluiting van het 
plan van aanpak op de vraagstelling, de theoretische en de empirische 
onderbouwing. In het licht van de evaluatiekwaliteit zijn daarnaast van belang: de 
kennis en ervaring van de indieners, de haalbaarheid van het evaluatieproject en de 
begroting. Deze criteria zijn voor potentiële indieners toegankelijk en dienen als 
richtlijnen bij het schrijven van offertes (ZonMw, 2013; interview 9 april).  
                                                
112
 Zo meldde de toenmalig minister Hoogervorst van VWS in een brief aan de Kamer (TK, 
2003–2004, 29 200 XVI, nr. 281).  
113
 Hiertoe wordt ook het expliciteren van de beleidstheorie gerekend (interview 8 april 2015).  
114
 De ‘project- of thematekst’ is de goedgekeurde vertaling van de opdracht van de 
opdrachtgever aan ZonMw. Deze tekst vormt voor aanvragers het kader waarbinnen 
zij een aanvraag kunnen indienen. Indien er een specifieke aanleiding bestaat, kan voor een 
zg. top-down procedure worden gekozen. Hierbij worden volgens de ‘procedures ZonMw’ 
potentiële aanvragers van wetsevaluatieonderzoek rechtstreeks uitgenodigd om een aanvraag 
in te dienen. 
  
 Pagina 73 van 146 
 

 
Sinds 2005 zijn bij STEM 35 studies opgeleverd, die zijn te downloaden op 
www.evaluatiemilieuwetgeving.nl. Behalve om ex post evaluatieonderzoeken gaat het ook om 
ex ante studies.  
 
Lessen uit de praktijk  
Een aantal ervaren onderzoekers (Uylenburg et al., 2011) blikt terug op het STEM-programma, 
zowel op de keuze van onderwerpen als op de inhoud en aanpak van de resulterende 
rapportages, en trekt hier de volgende lessen uit.  
(1) Een koppeling tussen de wet en empirisch onderzoek ontbreekt vaak: doeltreffendheid 
in juridische zin is iets anders dan in de empirische betekenis;  
De Boer komt terugblikkend tot de conclusie dat de functie van het evaluatieonderzoek sterker 
gericht was op het geven van inzicht in het onderwerp, dan strikt op het bepalen van de 
effecten van een regeling. Uylenburg constateert dat lang niet altijd is ingegaan op de relatie 
tussen de doeltreffendheid van de wet in de juridische betekenis en het daadwerkelijk bereiken 
van milieudoelen – en mist daarmee een empirische component. ‘Onderzocht is of de 
regelingen de noodzakelijke voorzieningen bevatten, en de wijze van uitvoering van die 
voorzieningen op een zodanige wijze plaatsvindt, dat het doel kán worden bereikt.’ 
(2) Het blijkt lastig om bij een wetsevaluatie aan alle criteria uit de Ar (tegelijk) recht te 
115
doen  
Oosterhuis merkt in dit verband op: ‘(....) [E]en standaard-set van te hanteren 
[evaluatie]criteria is in een zo divers beleidsterrein een illusie en ook niet zinvol. Daarentegen 
kan het wel de moeite waard zijn om een checklist van potentieel relevante criteria te hebben 
om na te gaan welke daarvan in een concrete evaluatie relevant zijn. Dat vermindert de kans 
dat bepaalde aspecten over het hoofd worden gezien.’ 
(3) Een meer systematische en multidisciplinaire aanpak had gevolgd kunnen worden; niet 
alleen gericht op actuele vraagstukken.  
Oosterhuis bepleit een – ook ten opzichte van STEM – meer structurele benadering van de 
evaluatie van wetgeving. Evaluaties zouden bijvoorbeeld niet beperkt moeten worden tot die 
onderwerpen waarover belangengroepen of politici zich het meest opwinden; bij een 
systematische evaluatiebenadering kan het leereffect van de evaluatieonderzoeken worden 
vergroot. Een multidisciplinair onderzoeksteam past bij een meer systematische aanpak. 
Daarbij geldt, evalueer met mate: ‘(...) Een hoge frequentie van evaluaties en daaruit 
voortvloeiende aanpassingen staat welhaast garant voor verminderde doeltreffendheid op 
langere termijn. Het Nederlandse stimuleringsbeleid voor duurzame energie is een bekend 
voorbeeld.’  
(4) Ten einde een effectieve doorwerking te bereiken is ‘aansluiting bij de agenda van de 
wetgevers van het ministerie’ de belangrijkste factor voor succes.  
Met betrekking tot het evaluatie-gebruik in het STEM-programma is Uylenburg van mening dat 
‘de doorwerking van de onderzoeken zeer zou verbeteren wanneer het (collectieve) geheugen 
van de wetgevende ambtenaren groter zou zijn. Veel van de uitgevoerde onderzoeken 
behouden immers hun waarde.’  
 
Bron: Uylenburg et al. (2011).  
 
Behalve het STEM-programma zijn ook de opeenvolgende evaluaties van de 
Algemene wet bestuursrecht (Awb) een voorbeeld van een rond één (majeure) wet 
georganiseerd (tijdelijk) evaluatievermogen (zie Box 6). Dat komt voort uit de tot 1 
januari 2013 in artikel 11:1 lid 2 Awb opgenomen verplichting die de ministers van 
Justitie en BZK opdroeg elke vijf jaar ‘een verslag over de wijze waarop zij is 
toegepast’ naar de Staten-Generaal te zenden (Michiels 2010, p. 42). 
                                                
115
 Rechtmatigheid, doeltreffendheid en doelmatigheid, subsidiariteit en evenredigheid, 
uitvoerbaarheid en handhaafbaarheid, en de eenvoud, duidelijkheid en toegankelijkheid van de 
regeling.   
  
 Pagina 75 van 146 
 

 
Anders dan in geval van de ECWM en het latere STEM (programmaraad) hebben vooral de 
verantwoordelijke ministeries de topics van de Awb-evaluatie bepaald. De vraag is of dit tot 
voldoende objectieve keuzes heeft geleid; Michiels stelt de vraag of de sterke rol van het 
ministerie tot het juridische karakter en de sterke focus op rechtsbescherming heeft geleid.  
- Er is lang gewacht met het voorbereiden van de eerstvolgende evaluatieronde, 
waardoor de evaluaties een (te) korte doorlooptijd hebben gehad (vgl. interview 11 
augustus). Ook de afweging ten aanzien van de vaststelling van 
evaluatieonderwerpen is volgens Michiels (2010, p. 45-46) niet voldoende voldragen 
geweest. De deelonderzoeken, hoe adequaat ook, zijn volgens hem hierdoor 
afzonderlijke partjes binnen het grote geheel geworden, met weinig onderlinge 
dwarsverbanden. Soms bestaat zelfs overlap in onderwerpskeuzes die door 
verschillende onderzoeksgroepen zijn behandeld.  
- Voor een betere communicatie tussen onderzoeksgroepen en evaluatiecommissie – 
want er is vanuit de onderzoeksgroepen wel kritiek geleverd op de 
evaluatiecommissies – acht Michiels het nuttig als eerst een conceptadvies wordt 
besproken. 
Inhoud: lessen  
- De praktijkervaringen met de eerste drie evaluaties van de Awb wijzen op het belang 
van een multidisciplinaire, ‘op maat’-benadering – net als de eerder besproken 
ervaringen met het programma STEM. Niet-juridische invalshoeken, zoals de 
rechtseconomische, zijn tot dusverre nauwelijks aan bod gekomen; het betrof 
beleidsgerichte, juridische wetsevaluatie (Michiels, 2010, p. 44; vgl. Eijlander 2003 en 
interview, 11 augustus). Zo schreef Eijlander naar aanleiding van de tweede Awb-
evaluatie: ‘De beleidstheorie en de vooronderstellingen die ten grondslag liggen aan 
de Awb zijn niet ter discussie gesteld. De evaluatie heeft zich gericht op de toepassing 
en de werking van de Awb in de praktijk en de mogelijkheden om daar eventueel 
verandering in te brengen. (….) Er heeft ook geen expliciet onderzoek plaatsgevonden 
naar de vraag of de doelstellingen van de Awb in de praktijk daadwerkelijk zijn of 
worden gerealiseerd (…) bijvoorbeeld, of met de Awb de bestuurswetgeving inderdaad 
uniformer en eenvoudiger is geworden. (…) Nu is dat ook complex (…) Een zg. goal 
free evaluation is dan beter uitvoerbaar.’  
- Een verwant punt betreft de maatstaven waaraan met de evaluatie wordt getoetst. 
Systematische evaluatie van wetgeving houdt in ‘het ex post beoordelen van de in 
empirisch onderzoek waargenomen inhoud, uitvoering en effecten van een wet aan de 
hand van bepaalde criteria’ (Winter, 1996). Evaluatiecriteria of – maatstaven zijn 
echter in geval van de Awb-evaluaties bij voorbaat niet expliciet gehanteerd. Volgens 
Michiels zijn gezaghebbende evaluatiemaatstaven nodig, die hetzij door wetgever zelf, 
hetzij door de regering in overeenstemming met het parlement, zijn geformuleerd 
(p.48). Wie de maatstaven definieert en welke dat dan zijn, is een lastig vraagstuk – 
en ook veel bediscussieerd in de literatuur over de Awb. Zo is in de literatuur wel als 
maatstaf genoemd: ‘bestuursproces van voldoende kwaliteit’ – maar de vraag is hoe 
men kwaliteit dan weer definieert. Hier raakt men aan het verder liggende doel dat de 
Awb zou dienen: de uitgangspunten van de rechtsstaat? Primair codificatie?, of 
richting geven aan nieuwe bestuursrechtelijke ontwikkelingen?  
- Het accent lag tot nu toe vooral op rechtsbescherming (bezwaar) in plaats van op de 
primaire besluitvorming. Michiels acht het laatste een minstens zo belangrijk 
evaluatieonderwerp.  
 
Na de derde evaluatie van de Awb volgde nog een vierde (2007-2011), echter zonder 
evaluatiecommissie; deze ging over het extern klachtrecht (titel 9.2 Awb) en de 
bezwaarprocedure (titel 7.2 Awb). Beide onderzoeken zijn in 2011 aan de TK gezonden 
(Bex et al., 2010; De Waard, red. 2011). Met ingang van 2013 is, met de inwerkingtreding 
van de Wet aanpassing bestuursprocesrecht (Wab), het artikel 11:1 over de verplichte 
  
 Pagina 77 van 146 
 

 
evaluatieonderzoek, dat in opdracht van het ministerie OCW wordt verricht, te 
verbeteren en de methodologische opzet van dit type onderzoek voor zover mogelijk 
te harmoniseren’ (OCW, 2015a; vgl. interview 1 juli).  
De directie Kennis zet zich in voor complementariteit tussen experimenteel 
onderzoek en verklarende evaluaties (ibid.). In geval van potentieel geëigende 
beleidsinterventies laat Kennis een onderzoeksdesign opstellen door CPB of SCP. De 
toon hiervoor is gezet in 2011, toen op verzoek van OCW onderzoeksdesigns voor 
een negental (clusters van) nieuwe beleidsinterventies op onderwijs- en 
wetenschapsgebied zijn opgesteld (CPB, 2011). Het betrof zowel verklarende als 
experimenteel georiënteerde effectevaluaties.  
‘Op verzoek van het ministerie zijn voor alle interventies zowel ontwerpen gemaakt 
voor een verklarende evaluatie als voor een effectevaluatie. De verklarende evaluatie 
richt zich op de factoren die het succes of het ontbreken van het succes van de 
interventie bepalen (zie bijlage voor een nadere toelichting op de verklarende 
evaluatie). Anders gezegd, wat gebeurt er precies binnen scholen als gevolg van de 
interventie. Wat is de Theory of Change en welke intermediaire uitkomsten bieden 
hiervoor inzicht? De ontwerpen voor de effectevaluatie richten zich op het zo 
overtuigend mogelijk vaststellen van de effecten van de nieuwe beleidsinterventies.’ 
(CPB, 2011, p.4).  
 
Beleidsdirecties kunnen gebruik maken van het betreffende onderzoeksdesign bij 
het opstellen van offerteaanvragen voor evaluaties – maar zijn daartoe niet 
verplicht (OCW, 2015a). Bijlage 4 bevat een uitwerkingsvoorbeeld (centrale toetsing 
zwakke scholen) van een gecombineerd verklarende – toetsende opzet van een 
evaluatie met een juridische component. Een ander mogelijk leerzaam voorbeeld 
van een doeltreffendheidsevaluatie betreft het ontwerp van de evaluatie van de 
Lerarenbeurs – ingesteld in 2008 om leraren in staat te stellen een Bachelor- of 
Masteropleiding te volgen. In het evaluatiedesign is gebruik gemaakt van een 
discontinuïteit in de kans op toekenning van de beurzen, op basis van volgorde van 
binnenkomst. ‘Rondom het moment waarop het budget uitgeput raakte, zijn in 
principe gelijke kandidaten de ene keer wel, en de andere keer niet in aanmerking 
gekomen voor een beurs. Vergelijking van de uitkomsten voor kandidaten geeft 
vervolgens een effectmeting.’ (CPB, 2015, p.5). Dit strookt met internationale tips 
voor effectevaluaties (Hoofdstuk 2).  
Per 2013 is door de Kennisdirectie het in 2010 gestarte programma Zicht op 
effectiviteit voortgezet, zij het alleen voor beleid dat aan drie criteria voldoet: (1) 
het biedt een goede kans op een robuuste effectevaluatie, gekoppeld aan een goede 
verklarende evaluatie; (2) het moet tot de kern van het OCW-beleid horen; (3) 
beleidsdirecties moeten serieuze belangstelling hebben voor deze aanpak (OCW, 
2015a). Dit heeft de directie Kennis zo besloten om het specifieke karakter van het 
kwaliteitsprogramma te onderstrepen. De Directie Kennis is betrokken bij al het 
quasi-experimentele effectonderzoek; bij de overige evaluaties bepaalt de 
verantwoordelijke beleidsdirectie of zij directie Kennis betrekt bij het denken over 
de aanbesteding van offertes en het evaluatiedesign, of als lid van de BC. In en na 
120
2013 zijn door CPB en SCP alweer nieuwe onderzoekdesigns voor OCW opgesteld.  
In de praktijk gaan de aanvragers – die beslissen – nog relatief weinig over tot de 
toepassing van quasi-experimentele designs (interviews 1 juli en 26 augustus). De 
indruk bij de directie Kennis is dat het programma Zicht op effectiviteit, bedoeld 
voor de stimulering van doeltreffendheidsevaluaties, ‘in een omgeving landt waar 
niet gepland wordt nagedacht over het onderzoek dat gedaan moet worden’ 
(interview 1 juli; vgl. 26 augustus).  
                                                
120
 www.cpb.nl/publicatie/ontwerpen-voor-effectevaluatie;  
www.scp.nl/Publicaties/Alle_publicaties/Publicaties_2014/Op_zoek_naar_bewijs_II.  
  
 Pagina 79 van 146 
 

 
batenanalyse (MKBA) en in de MvT de aannames opgenomen ten aanzien van de 
verwachte bijdrage van het (wets)instrument aan het gestelde doel, binnen 
benoemde termijnen. In het kader van beleidsdoorlichtingen participeert FMC in de 
Interdepartementale Begeleidingsgroep Prestatiegegevens en Beleidsevaluatie 
(IBP), een netwerk van directies FEZ en het Ministerie van Financiën (interviews 16 
juli en 1 september; intern document 2013).  
 
3.5.3 Handreikingen, richtlijnen, standaarden 
 
Naast, of in het kader van, genoemde programma’s zijn – vooral door de genoemde 
actieve units bij departementen zoals FEZ en kennisdirecties – handreikingen, 
richtlijnen en standaarden over evalueren ontwikkeld voor beleidsmedewerkers, 
zowel wat betreft inhoud als proces.  
 
Handreiking beleidsdoorlichtingen  
De Handreiking Beleidsdoorlichting van het ministerie van Financiën (2016) is 
bedoeld als een departement-overstijgend hulpmiddel bij het opstellen van dit 
verantwoordingsdocument. De Handreiking gaat zowel in op methodologisch-
inhoudelijke als op procesmatige aspecten. Omdat beleidsdoorlichtingen een 
synthese-document zijn van het onderliggend evaluatieonderzoek, bespreken we 
deze hier verder niet. Niettemin zijn er veel overeenkomsten met methodologie en 
proces van ‘regulier’ evaluatieonderzoek, dus de Handreiking kan ook hiervoor een 
hulpmiddel zijn.  
Een deel van de input voor deze Handreiking is afkomstig van wat individuele 
departementen eerder ontwikkelden – via het IBP, het interdepartementaal ‘FEZ’- 
overleg. Zo beschikt(e) SZW al over een reeks van eigen gedetailleerde 
handreikingen en modelbrieven en – formulieren met betrekking tot 
beleidsdoorlichtingen (interview 15 juli en ontvangen intranet-stukken).  
Bij BZK worden de RPE en de binnen het departement ontwikkelde algemene 
handreiking ‘Evaluatie als fundering voor beleid’ (Bakker, 2012) meegegeven aan 
uitvoerders van evaluaties, naast de inmiddels verschenen rijksbrede handreiking 
voor beleidsdoorlichtingen (interview 13 oktober).  
 
Handreikingen EZ  
Het ministerie van EZ heeft in 2012-2013 twee gidsen voor het doen van 
evaluatieonderzoek opgesteld. De eerste, ‘Handreikingenboekje beleidskwaliteit en 
beleidsevaluatie’, dekt de gehele beleidscyclus en is bedoeld – uiteindelijk – voor 
departement-breed gebruik (BEC-secretariaat, z.j.).  
‘Met enige regelmaat wordt het secretariaat van de BEC benaderd door beleidsmakers 
met een verzoek om hulp bij het uitdenken van de onderbouwing van voorgenomen 
beleid. Het gaat dan om de analyse van het beleidsprobleem en de voorgenomen 
beleidsinterventie en het systeem van nulmetingen, monitoring en evaluaties. Deze 
gesprekken maken duidelijk dat beleidsmakers op zoek zijn naar basiskennis over 
beleidskwaliteit en beleidsevaluatie. Het gaat dan vaak om aan de hand van een aantal 
concrete stappen en met praktische tips op weg te worden geholpen. Dit boekje met 
handreikingen voorziet in deze behoefte.’ (p.1). 
 
Het boekje bevat concrete suggesties voor evaluatieopstellers in elke fase van ‘de 
cyclus van beleidskwaliteit en beleidsevaluatie’: probleemanalyse, instrumentkeuze, 
gevolgenbeoordeling, politieke beslissing, uitvoering en ex post evaluatie. Voor elke 
fase bevat het een zelfstandig leesbare handreiking. Bij ex post evaluaties gaat het 
volgens respondenten om basisprincipes (i.c. van de experimentele benadering) en 
‘tips & tricks’ voor aanvragers om deze in staat te stellen het gesprek met de 
(kwantitatief) onderzoeker aan te gaan en kritische vragen te stellen (9 juli). 
Behalve over ex post evaluatie biedt het boekje ook aanwijzingen en tips bij het 
opstellen van (a) vroege beleidsexperimenten en (b) het opzetten van een 
  
 Pagina 81 van 146 
 

 
 
Recapitulatie van deze paragraaf is niet eenvoudig; voor een overzicht van 
programma’s en handreikingen of richtlijnen verwijzen we nog eens naar Tabel 8 en 
voor concrete hulpbronnen naar Bijlage 4. Wel zijn een paar grove lijnen te trekken. 
De inhoudelijke evaluatiefocus verschilt per departement en past vaak bij de rol van 
de op het evaluatievlak meest actieve organisatieonderdelen. Is FEZ zeer actief, dan 
weerspiegelt zich dat in een meer summatieve, op verantwoording gerichte aanpak 
(bijv. bij EZ en SZW). Bij EZ werkt men bijvoorbeeld met uitleg en checklists voor 
evaluatieopzetten (procesmatig en inhoudelijk), offertes en offertevergelijking en 
rapporten. De ervaring met de WBSO (EZ) leert bijvoorbeeld hoe een (gedeeltelijke) 
wetsevaluatie ook kwantitatief kan worden opgezet; het fiscale instrumentarium 
keert als element in meerdere wiskundige formules terug. Is een directie Kennis of 
een onafhankelijk onderzoeksinstituut relatief actief, dan uit dat zich vaak in 
verschillende soorten voorgestane aanpakken, zowel experimenteel of black box als 
verklarend (o.a. VenJ, OCW). Het onderscheid is dus niet zwartwit; ook 
respondenten bij financiële directies (o.a. IenM en EZ) en bij het ministerie van 
Financiën hameren steeds vaker op het belang van toetsing van aannames in de 
beleidstheorie.  
Bij EZ zijn evaluatie-handvatten geïntegreerd in een handreiking voor 
beleidskwaliteit – het overkoepelende doel is effectiever beleid te bewerkstelligen, 
en evalueren is één van de middelen. Deze handreiking beslaat alle fasen van de 
beleidscyclus, van probleemanalyse en instrumentkeuze en ex ante 
gevolgenbeoordeling, tot en met politieke beslissing, uitvoering en – uiteindelijk – 
ex post evaluatie. Daarnaast bevat de gids aanwijzingen en tips bij het opstellen 
van (a) vroege beleidsexperimenten en (b) het opzetten van een 
monitoringsysteem, met concrete voorbeelden. 
Het IAK wordt nauwelijks genoemd in het verband van ex post evaluatie; bij IenM 
probeert men bij ex post beleidsdoorlichtingen terug te grijpen op de resultaten van 
het ex ante te gebruiken IAK voor beleid en wetgeving. 
Veel van de genoemde programma’s en richtlijnen of handreikingen zijn nog pril. Er 
is beperkt zicht op hoe vaak inzichten worden toegepast en dus ook op wat er 
vervolgens in de praktijk mee is bereikt. We kunnen dus niet zeggen of met de 
geschetste initiatieven de beleidsdoorlichtingen bijvoorbeeld minder lacunes zijn 
gaan bevatten, of dat de kwaliteit van evaluatieonderzoek is verstevigd.  
Juist ten aanzien van wetgeving zijn enkele programma’s besproken die tastbare 
ervaringslessen hebben opgeleverd. Wat de inhoud betreft leert de praktijk van 
zowel de tweede en derde Awb-evaluatie, het STEM- en het Programma evaluatie 
regelgeving (PER) bij ZonMw dat multidisciplinariteit in de aanpak onontbeerlijk is: 
een combinatie tussen juridisch onderzoek naar gelding en empirisch onderzoek 
naar o.a. doelbereiking en doeltreffendheid. Bij ZonMw is men tot zeven algemene 
aanpakcriteria gekomen – die in de praktijk ook al langere tijd worden toegepast.  
 
3.6 Borging van evaluatiekennis en -ervaring  
 
Eerder is al besproken hoe, via de verschillende organisatieonderdelen, 
procesmatige en methodologische evaluatiekennis de aanvrager zou moeten 
bereiken. Evaluatievermogen gaat daarnaast ook over: de mate waarin en wijze 
waarop het evaluatiekader binnen de organisatie wordt gecommuniceerd, hoe men 
voor een concreet evaluatieproject de benodigde kennis bijeen krijgt, en hoe 
eenmaal opgedane evaluatiekennis en –ervaring binnen de departementale 
128
organisatie wordt verankerd en gedeeld.  
                                                
128
 We beperken ons hier tot de inspanningen van de (departementale) organisaties. De veelal 
externe onderzoekers komen alleen aan bod waar het gaat om de manier waarop zij door 
aanvragers voor  evaluatieprojecten worden geselecteerd.  
  
 Pagina 83 van 146 
 

 
beleidsmedewerkers, door collega’s te stimuleren aan het Masterprogramma mee te 
doen (interview 1 juli). 
Volgens respondenten bij de Algemene Rekenkamer zijn naar aanleiding van het 
kritische Rekenkamer-onderzoek naar subsidie-evaluaties (2011) clubjes 
beleidsambtenaren (in totaal 100) getraind in de planning en het design van 
evaluatieonderzoek – waarmee men al rekening dient te houden bij het design van 
de subsidies zelf (interview 1 september). Het betreft wederom de cursus ‘Beleid in 
perspectief’ bij de Rijksacademie voor Financiën, Economie en Bedrijfsvoering, 
waarvan de Rekenkamer het deel beleidscontrol voor haar rekening neemt. 
Bij VenJ liepen in 2015 zeker twee cursussen met een evaluatiecomponent, bij de 
Beleidsacademie en via het Centrum voor Criminaliteitspreventie en Veiligheid 
(CCV). Verder is er het cursusaanbod bij de Academie voor wetgeving en 
132
overheidsjuristen, met daarbinnen eveneens aandacht voor wetsevaluaties.  
Op basis van internationale ervaringen met evaluatievermogen constateerden we in 
het tweede hoofdstuk dat het bouwen hieraan een zaak van de lange adem is. Bij de 
communicatie en bestendigheid van kennisverruimende initiatieven binnen 
departementen vallen wel nog enkele vraagtekens te plaatsen. Zo bleek enkele 
keren dat bestaande hulpmiddelen niet bekend waren bij onderzoekscoördinatoren 
of DWJZ (interviews 15 en 16 juli, 26 augustus). Wetgevingsjuristen bleken 
bijvoorbeeld niet bekend met het instrument beleidsdoorlichting.  
 
3.6.2 Selectie van onderzoekers  
 
Een andere belangrijke manier om de kwaliteit van evaluaties bij ministeries te 
borgen is via gerichte selectie van onderzoeksteams. Wat de opleiding en kennis en 
expertise van de uitvoerende onderzoekers van evaluaties en wetsevaluaties 
betreft, stellen beleidsdirecties geen strenge of vaste eisen. Competitie tussen 
kandidaat-uitvoerders voor evaluatieprojecten boven de aanbestedingsgrens 
fungeert als belangrijk selectiemechanisme.  
Bij VenJ (WODC-EWB) is een database opgezet met de specifieke (a) 
contactpersonen en (b) expertises van alle onderzoeksbureaus en 
onderzoeksgroepen (universitair, particulier of anderszins) die ooit een offerte 
hebben ingediend of een evaluatie hebben verricht. Dit helpt het overzicht te 
houden over welke teams op welk gebied expertise hebben (interviews 8 juni en 15 
juli). Niettemin kost het zeer veel moeite om het bestand ‘op peil’ te houden in een 
dynamische personele omgeving.  
Onder andere bij het ministerie van SZW wordt gebruik gemaakt van formulieren 
met betrekking tot de opleiding en ervaring van het uitvoerende onderzoeksteam. 
Steeds meer wordt binnen SZW gestimuleerd dat hoogleraren en/of ervaren 
onderzoekers betrokken zijn in het onderzoeksteam (interview 15 juli). Ook hier 
werkt men aan een bestand waarin de ervaringen met onderzoeksbureaus worden 
vastgelegd.  
Bij EZ en OCW is met de initiatieven om doeltreffendheidsonderzoek te stimuleren, 
al enige ervaring opgedaan met de overdracht van inhoudelijke 
evaluatiestandaarden aan onderzoekers. Soms reageerden externe onderzoekers 
hierop terughoudend: niet iedereen beschikt over de expertise die voor kwantitatief/ 
experimenteel onderzoek vereist is (interviews 1 en 9 juli). Bij EZ zou op dit punt 
een communicatiestrategie zijn ontwikkeld (gebaseerd op overtuiging en meer 
betrekken van onderzoeksbureaus).  
 
Multidisciplinariteit bij wetsevaluaties  
Alle departementen gaven aan de multidisciplinariteit van het onderzoeksteam heel 
sterk te waarderen. EWB-projectbegeleiders bij het WODC zien een duidelijk verschil 
                                                
132
 http://academievoorwetgeving.nl/landingpage/cursussen.  
  
 Pagina 85 van 146 
 

 
onafhankelijke uitvoering van het onderzoek borgen (interview 8 juni). De formeel 
omschreven functie van de ‘BC’ is een inhoudelijke bijdrage leveren aan de 
uitvoering en de kwaliteit van het onderzoek. Het betrekken van veldorganisaties en 
het creëren van draagvlak is geen afdoende argument voor deelname aan de BC 
(WODC, 2012, p. 15). De leden nemen op basis van persoonlijke deskundigheid 
deel, niet als vertegenwoordiger van de organisatie waar zij werken. De voorzitter is 
vaak een hoogleraar of UHD met (diepgaande) kennis op het terrein van het 
evaluatieonderwerp. Naast de voorzitter, de projectleider van WODC-EWB en een 
vertegenwoordiger van de aanvrager, bevat de BC vaak een methodoloog met 
expertise over de te gebruiken waarnemings- experimenteer- of analysetechnieken. 
Ten slotte bevat de BC vaak één of twee externe, vaak wetenschappelijke 
deskundige(n) op het te onderzoeken terrein. Binnen het genoemde kader bestaan 
geen nadere regels over de samenstelling van BC’s; het is maatwerk en van veel 
factoren afhankelijk (WODC, 2012; 8 juni). 
Projectbegeleiders geven aan dat het vaak lastig is om mensen voor BC’s te vinden 
die zowel methodologische als domein-specifieke kennis hebben. Bij specifieke 
onderwerpen, waaronder wetsevaluaties op het snijvlak van recht en empirie, is de 
groep die aan deze twee voorwaarden voldoet vaak klein. Bovendien hebben 
wetenschappers met kennis van sociaalwetenschappelijke methoden over het 
algemeen weinig belangstelling voor het begeleiden van juridisch georiënteerde 
evaluaties.  
De CER bij ZonMw stelt bij elke uit te voeren wets- of thematische evaluatie een BC 
in. Dit is volgens de CER (interview 8 april) één van de belangrijkste factoren in de 
borging van evaluatiekwaliteit. De voorzitter en eventueel ook de vicevoorzitter van 
de BC zijn lid van de CER. Verder worden de voor de betreffende wet relevante 
disciplines afgevaardigd in de commissie, zoals rechten, ethiek, gezondheidszorg, 
psychologie. In de regel woont een waarnemer van het (of de) betrokken 
ministerie(s) de vergaderingen van de BC bij (ZonMw, 2013, p.8).  
’  
3.6.4 Delen en verankeren van ervaringen met evaluatie (aanvragers)  
 
Dat eerdere ervaringen met evaluatieprojecten worden benut voor de aanvragende 
organisatie kan op verschillende manieren worden bereikt, onder meer via de 
aanstelling van medewerkers, via het uitwisselen van ervaringen, binnenshuis of 
met andere departementen, of met inzet van de vaste constructies zoals FEZ en 
Kennisdirecties. De praktijkervaringen die op deze punten zijn opgedaan bespreken 
we in deze deelparagraaf.  
 
Selectie van beleidsmedewerkers en andere betrokkenen 
Eerdere ervaring met het uitbesteden en begeleiden van onderzoeks- en 
evaluatietrajecten is een criterium in de werving van de projectbegeleiders bij 
WODC-EWB. Dat is niet het geval bij FEZ-SZW, maar wel beschouwt men 
wetenschappelijke ervaring, zoals een promotie in de sociale wetenschappen, als 
een pre (interviews 15 en 16 juli).  
 
Overdracht en specialisatie  
Het is een complexe opgave, ten minste volgens respondenten bij SZW en VenJ, om 
ervaringskennis en expertise aanwezig in de hoofden van dossierhouders vast te 
houden en te benutten. Personen binnen de departementen weten veel meer van 
evaluatieonderzoek en het uitbestedingsproces, dan wat geregistreerd wordt – en 
redelijkerwijs kan worden (interviews 8 juni en 15 juli). Veel ervaringskennis zit bij 
  
 Pagina 87 van 146 
 

 
praktijk vaak aan het (externe) onderzoekersteam overgelaten (interviews 1 juli en 
26 augustus).  
Een vergelijkbare rol als voornoemde vervult de vertegenwoordiger van de 
beleidsdirectie of linking pin in de Regiegroep Monitoring en Effectmeting bij EZ: die 
geeft desgewenst tips over methoden, geschikte onderzoeksbureaus en mogelijke 
BC-leden (BEC-secretariaat, z.j., p. 26). Achterin het BEC-boekje is per DG de 
betreffende ‘hulplijn’ (contactpersoon) te vinden.  
Bij het WODC is de rol van de aanvrager deels ‘uitbesteed’ aan de EWB-
projectbegeleiders, die het proces na de inventarisatie van de onderzoekwensen bij 
de aanvrager goeddeels overnemen.  
Het bestendigen van evaluatiekennis- en ervaring is een uitdaging gebleken in 
dynamische personele omgevingen. Zo is bij BZK een van de kennisdirecties na 
136
2011 praktisch geheel ontmanteld (interview, 13 oktober). ‘Verloop’ van kundige 
beleidsmedewerkers is fnuikend voor het behoud van expertise (interviews 24 en 26 
augustus). Rond wetsevaluatie is wel geklaagd over verloop van verantwoordelijke 
wetgevingsjuristen: de betrokkene bij de totstandkoming van een wet en 
evaluatiebepaling is vaak niet meer betrokken zodra de wetsevaluatie op handen is. 
Als het gaat om nieuwe werkwijzen en handreikingen in het kader van 
evaluatievermogen, lijken de beoogde gebruikers hier bovendien niet altijd ‘klaar 
voor’ (interviews 1 juli en 26 augustus). Hierdoor kunnen initiatieven in een weinig 
137
vruchtbare omgeving landen. Ten slotte dreigen ontwikkelingen op aanpalende 
terreinen de opgebouwde structuren rondom evalueren soms uit te hollen. De 
centralisering van inkooppunten was bedoeld om het offerte- en 
aanbestedingstraject, ook rond evaluaties, door standaardisering transparanter te 
maken (Rijksoverheid, z.j.). Maar deze transparantieslag heeft een uithollende 
werking op aanvrager-ondersteunende formats die specifiek voor evaluaties zijn 
ontwikkeld (interviews 1 en 9 juli). De uitdaging volgens respondenten bij EZ is dat 
138
de nieuwe inkoopcentra de op evaluatiegebied ontwikkelde formats en kaders 
inhoudelijk volgen.  
 
3.6.5 Samenvattend over borging  
 
In lijn met wat we in hoofdstuk 2 internationaal signaleerden, wordt de evaluatie-
aanvrager ook in ons land steeds vaker gezien als een belangrijke schakel in 
procesverloop en kwaliteit. In deze paragraaf is beschreven, op welke manieren bij 
beleidsdepartementen is geborgd dat aanvragers kennis hebben van het 
evaluatieproces en -inhoud om deze in goede banen te kunnen leiden. Bij de meeste 
DG’s en directies vervullen de onderzoekscoördinatoren hierin een spilfunctie; zij 
hebben relatief veel onderzoeksexpertise en - ervaring en fungeren als vraagbaak 
voor collega’s. Daarbuiten verschilt de ondersteuning bij departementen sterk, 
afhankelijk van de betrokken organisatieonderdelen. Zo heeft bij SZW de Chief 
Science Officer (CSO) inhoudelijk een ondersteunende rol; bij OCW is dat de directie 
Kennis en bij EZ de Regiegroep M&E. Bij VenJ is de rol van de aanvrager voor een 
                                                
136
 Bij het DG Wonen en Rijksdienst van BZK bestaat nog wel de directie Kennis & Strategie.  
137
 Eerder stierven initiatieven ter borging van evaluatiekennis een stille dood; waaronder een 
rijksbreed initiatief aan het begin van deze eeuw om een evaluatie-overzicht (EOR- 
evaluatieoverzicht Rijk) op te zetten en steeds te actualiseren, met daarin alle door de 
departementen opgeleverde evaluaties waaronder ook wetsevaluaties. Het EOR hield werd niet 
systematisch bijgehouden en bevatte veel dubbelingen.  
138
 Sinds 1 januari 2014 zijn er 20 nieuw ingestelde ‘inkoopuitvoeringscentra’ (IUC’s) verspreid 
over het land. Deze moeten het inkoopproces voor alle partijen meer doelmatig maken. Sinds 
2008 is sprake van categoriemanagement; dit betekent voor het Rijk dat departementen niet 
alleen voor zichzelf bepaalde producten of diensten inkopen, maar gecoördineerd 
per productgroep of categorie voor elkaar, ter bevordering van specialisatie. Uit onderzoek van 
Gfk (2015) bleek dat driekwart van de ambtenaren niet bekend is met het nieuwe 
inkoopstelsel.  
  
 Pagina 89 van 146 
 

 
vergaderingen van de BC. Ook de timing en tijdigheid van oplevering en de politieke 
of ambtelijke besluitvormingscontext zijn doorslaggevend gebleken.  
Dit beeld wordt door de meerderheid van gesproken respondenten bevestigd. 
Contact en overleg tussen aanvrager en onderzoekers gedurende het 
evaluatietraject – al dan niet via BC-vergaderingen – is volgens meerdere 
respondenten van aanvragerszijde een belangrijke manier om de kans op 
evaluatiegebruik te vergroten. Om het contact in te kaderen, geldt bij PER-
wetsevaluaties: ‘Beide partijen spreken hun wederzijdse bedoelingen en 
verwachtingen helder en nadrukkelijk uit en komen gemaakte afspraken na voor 
effectieve uitvoering en realisatie van de opdracht. De aard van de relatie is 
gelijkwaardig en niet vrijblijvend’ (ZonMw, 2013, p.8). De onderzoekers zijn 
eindverantwoordelijk voor de evaluatieresultaten. ZonMw kan alleen besluiten een 
rapport niet vast te stellen of kanttekeningen maken in de begeleidende brief 
(interview 8 april).  
In het kader van ‘timing’ in relatie tot beleidsontwikkeling of besluitvorming vinden 
meerdere respondenten dat sommige wetsevaluaties te vroeg worden verricht voor 
het kunnen trekken van adequate conclusies (zie ook hoofdstuk 2 en par. 3.3). 
Anderzijds vindt men een evaluatietraject soms te laat afgerond om nog tot 
doorwerking te kunnen leiden.  
 
3.7.2 Wijze van presentatie, aanbevelingen  
 
Wetsevaluaties bevatten niet altijd geschikte aanbevelingen. Soms behoeft de 
wettekst zelf geen aanpassing, maar omliggende structuren of flankerend beleid 
(interview 24 augustus; vgl. Michiels, 2010). Een wetswijziging naar aanleiding van 
een evaluatie vergt zeer veel energie en tijd; bovendien dient een wijziging te 
passen binnen het grotere systeem van wet- en regelgeving. De baten van een 
wetswijziging zouden tegen deze kosten moeten opwegen.  
Een aanvrager bij VenJ stelt dat aanbevelingen in evaluatieonderzoek hetzij vaak 
ontbreken, hetzij niet adequaat zijn. Hij schrijft dit toe aan het gegeven dat de 
beleids- en de onderzoekerswereld verschillend in elkaar zitten (interview 15 juni).  
De aanvragers van WODC-evaluaties en andere onderzoeken geven de 
bruikbaarheid van aanbevelingen en adviezen gemiddeld een ruime voldoende. De 
score voor een ‘goede’ bruikbaarheid is in een substantieel deel van de gevisiteerde 
periode gedaald en heeft zich in 2012 weer iets hersteld (WODC, 2014, p. 17-18). 
Naar aanleiding van de laatste visitatie is de bruikbaarheid voor beleid van WODC-
onderzoek, inclusief evaluatieonderzoek, benoemd als blijvend aandachtspunt voor 
de toekomst – onder andere via scherpe vraagarticulatie en methodenkeuzen, het 
‘nazorgtraject’ van onderzoek en het betrekken van stakeholders van binnen en 
buiten de departementen van VenJ en BZK.  
 
3.7.3 Gebruik  
 
Een belangrijke vraag is, uiteindelijk, in hoeverre evaluatieresultaten hun weg terug 
weten te vinden naar het proces van ambtelijke besluitvorming of de politieke 
arena. Als evaluatieresultaten niet worden teruggekoppeld, kan men zich immers 
afvragen wat de zin van de hele exercitie is geweest. In hoofdstuk 1 gebruikten we 
in dit verband het begrip ‘regulatory cycle’: is de beleids- of wetgevingscyclus rond? 
Alle departementen volgen op hoofdlijnen dezelfde uitwendige procedure na de 
afronding van (wets)evaluaties. De opdrachtgever of aanvrager, zoals de beleids- of 
wetgevingsdirectie, moet binnen drie maanden na de oplevering van het 
evaluatierapport een beleidsreactie formuleren en eventueel – bijv. in geval van een 
toezegging – naar de Tweede Kamer sturen. Daarin moet worden gemeld wat de 
minister van plan is met het onderzoek te gaan doen. Het rapport en de 
kabinetsreacties worden in de meeste gevallen gepubliceerd en kunnen anders op 
  
 Pagina 91 van 146 
 

 
de WODC onderzoeken wordt voor overleggen met Tweede en Eerste Kamer 
gebruikt.  
 
Box 8 Onderzoek naar gebruik bij BZ (Van Gils en Leeuw, 2010) 
 
Rond 2010 is onderzoek verricht naar het gebruik van 30 evaluaties bij het ministerie van BZ. 
Er zijn veel typen evaluatiegebruik. Evaluaties zijn gebruikt ter verantwoording van beleid 
onder meer via toezending van evaluaties van de IOB, voorzien van een beleidsreactie, aan de 
Tweede Kamer, door publicatie van rapporten op de website van het ministerie en door 
gebruik als informatiebron voor bijvoorbeeld het jaarverslag van het ministerie.  
Andere gevonden vormen van gebruik zijn: ter ondersteuning van overdracht bij 
functiewisselingen (op individueel initiatief), in de externe communicatie: (a) ter legitimering, 
als basis voor overleg in het kader van fondsenverstrekking en (b) ter promotie, d.w.z. de 
beleidsprestaties op de kaart zetten.  
Tussen DG’s bestaat verschil in het directe, lerende gebruik van evaluaties: ‘[v]eel evaluaties 
binnen (...) Internationale Samenwerking (DGIS) gaan gepaard met een herziening van het 
geëvalueerde beleid.’ Causaal en lineair is het verband tussen evaluatieresultaten en 
beleidsaanpassingen volgens de onderzoekers niet; evaluaties blijken zeker niet de enige bron.  
Wel is duidelijk geworden dat een BC ‘de best denkbare leeromgeving is om iets van een 
evaluatie op te steken’. Hierdoor heeft direct gebruik (in beleid) vaak al voor de officiële 
publicatie plaats gevonden.  
 
Hoewel verplichte evaluaties vaak wel afgerond worden, komt het voor dat ze na 
verloop van tijd als minder politiek relevant worden gezien (interview 2 september); 
dan blijft gebruik vaak beperkt tot kennisname. Niettemin percipiëren enkele 
respondenten dat de Tweede Kamer toenemende belangstelling laat zien in ex post 
evaluaties, met name in het licht van de verantwoordingsfunctie en financiële lasten 
141
(interviews 15 en 16 juli). Naar aanleiding van kabinetsreacties op 
evaluatierapporten en beleidsdoorlichtingen stellen Tweede Kamerleden vaker 
vragen over wat de minister van plan is met de bevindingen en eventuele 
aanbevelingen te doen.  
Over het gebruik van de derde evaluatie van de Awb gaat box 9.  
 
Box 9 Gebruik van de derde Awb-evaluatie  
 
De aanbevelingen van de derde evaluatie werden onderverdeeld in aanbevelingen aan de 
wetgever, de regering, bestuursorganen, rechters en onderzoekers. Voor groepen zoals 
rechters moge duidelijk zijn dat de aanbevelingen nauwelijks een verplichtend karakter hebben 
(Michiels, 2010). Lang niet alle aanbevelingen waren gericht op aanpassing van de Awb.  
Naar aanleiding van het verschijnen van het verslag van de evaluatiecommissie is op 8 maart 
2007 een symposium gehouden voor alle bij de evaluatie betrokkenen (onderzoekers, leden 
van de BC’s, etc). Voordien was het rapport voorgelegd aan een groot aantal betrokken 
partijen, waarop vervolgens een drietal reacties zijn ontvangen. De latere kabinetsreactie heeft 
rekening gehouden met de uitkomsten hiervan en met commentaren die in de juridische 
vakliteratuur zijn verschenen (Tweede Kamer, vergaderjaar 2009–2010, 29 279, nr. 111, p.3). 
Uit de kabinetsreactie op de derde Awb-evaluatie valt op te maken dat aan een groot aantal 
overige aanbevelingen reeds geheel of gedeeltelijk gevolg is gegeven (Tweede Kamer, 
vergaderjaar 2009–2010, 29 279, nr. 111). Niettemin constateerde Michiels (2010) dat het al 
beperkte aantal tot de wetgever gerichte aanbevelingen slechts beperkt heeft doorgewerkt in 
                                                
141
 Deze perceptie wordt echter niet alom gedeeld, getuige het verslag van een bijeenkomst 
van VIDE over het centrale evaluatiestelsel en de beleidsdoorlichtingen, november 2015: ‘De 
behandeling van beleidsdoorlichtingen in de TK is veelal summier en is (te) vaak opgenomen in 
een volle AO-agenda.’  www.videnet.nl/download/?id=17773737.  
 
  
 Pagina 93 van 146 
 

 
Van Gils en Leeuw (2010) vonden dat bij de beleidsdirecties van BZ geen sprake 
was van een echt leergierige cultuur: ‘Het is niet gebruikelijk om actief kennis uit 
evaluaties te zoeken’ (p.12). Volgens de auteurs heeft (nieuwe) kennis over hoe 
gedrag te beïnvloeden een grotere generaliseerbaarheid en aantrekkingskracht dan 
terugblikkende, op processen gerichte ex post evaluaties. Verder reikten de ambities 
met betrekking tot leren binnen het ministerie verder dan wat tot dan toe 
fragmentarisch was gerealiseerd; individueel leren is nog geen institutioneel leren. 
Eerste aanbeveling was dan ook: ‘schep meer duidelijkheid over wat bij het 
ministerie onder leren verstaan wordt.. (....) en hoe organisatie en medewerkers 
geprikkeld kunnen worden tot leren’ (p.13). 
Om de leerfunctie van evaluaties te versterken, heeft de toenmalige Minister voor 
Ontwikkelingssamenwerking de Tweede Kamer op 19 februari 2009 toegezegd een 
143
Panel van Advies (PvA) van externe deskundigen in het leven te roepen – met 
een mandaat dat uiteindelijk tot en met 2014 heeft geduurd. Het Panel had tot taak 
de bruikbaarheid en het gebruik van evaluatiestudies voor beleid en praktijk te 
bevorderen. Er zijn drie opeenvolgende adviezen aan de bewindspersoon 
uitgebracht, waarin verschillende vormen van gebruik van IOB rapporten onder de 
loep zijn genomen en in het kader van ‘leren’ ook een wetenschappelijk-inhoudelijke 
kwaliteitstoets is verricht – met nadruk op de beleidstheorie. Box 10 geeft enkele 
adviezen weer.  
 
Box 10 Adviezen van het Panel van Advies inzake Bruikbaarheid van IOB-rapporten (BZ) 
 
- Baken evaluatieobjecten en probleemstellingen zo precies mogelijk af; 
- Maak meer gebruik van (internationale) wetenschappelijke literatuur over de verschijnselen 
en hun oorzaken die in de evaluaties centraal staan; 
- Zorg voor een evaluatiedesign dat aansluit bij de vraagstelling (...) en dat geldt ook voor 
mointoring;  
- Draag er aan bij dat er baselinedata beschikbaar zijn voordat evaluaties naar (nieuw) beleid 
worden aangevangen; 
- Sluit zoveel mogelijk aan bij de state of the art op het gebied van de methodologie van 
synthesestudies: ‘systematic reviews’, meta-analyses en ‘realist syntheses’; 
- Draag bij aan het via een ‘portal’, resp. ‘repository’ snel en efficiënt toegankelijk maken van 
resultaten uit evaluaties voor beleid, control en samenleving; 
- Maak meer gebruik van de ervaringen die zijn opgedaan bij eerdere, te vergelijken, 
evaluaties, óók op andere beleidsterreinen of in andere landen, mits de ingezette ‘tools of 
government’ en de daaraan ten grondslag liggende gedragsmechanismen (de 
interventietheorie) en de context(en) redelijk vergelijkbaar zijn;  
- Draag bij aan een gearticuleerde theorie over wat BZ/ontwikkelingssamenwerking verstaat 
onder ‘leren van evaluaties’; 
- Bepaal van te voren wie de gebruikers van de evaluatie kunnen zijn en hoe zij het beste 
geïnformeerd kunnen worden; 
- Zorg dat de omvang van de rapporten het gebruik niet vermindert of belemmert; 
- Zorg dat voor de gebruiker duidelijk is waar welke informatie in een rapport te vinden is; en 
- Maak in het rapport duidelijk wat men van deze evaluatie kan leren en zo mogelijk hoe dit 
kan worden gedaan. 
 
3.7.5 Samenvattend over bruikbaarheid en gebruik 
 
                                                
143
 In het expertpanel zullen de volgende onderscheiden en complementaire expertises 
vertegenwoordigd zijn: 1. kennis van het functioneren van overheidsorganisaties en in het 
bijzonder de betekenis van de evaluatiefunctie binnen de overheid; 2. wetenschappelijke 
kennis op het terrein van ontwikkelingssamenwerking, relevante onderzoeksactiviteiten en 
methoden en technieken van toetsend onderzoek. 
  
 Pagina 95 van 146 
 

 
Hoofdstuk 4. Slotbeschouwing  
 
4.1 Inleiding  
 
Evalueren is iets waar men principieel bijna niet tegen kan zijn; het is een stap in 
het bereiken van verbetering van, of afleggen van verantwoording over, eerdere 
plannen en acties. Dat geldt ook voor het evalueren van beleid en wetgeving. 
Vragen zoals ‘hoe is het precies uitgepakt’, ‘zijn de beoogde doelen bereikt’? kunnen 
moeilijk als irrelevant worden afgedaan, helemaal als men wil leren voor 
toekomstige initiatieven. Maar dergelijke vragen beantwoorden kost geld en tijd; de 
omwentelingssnelheid van evaluaties ligt meestal een stuk lager dan die van politiek 
en beleid. 
Willen evaluaties daadwerkelijk een bijdrage kunnen leveren aan de kwaliteit van 
wetgeving, dan is het van belang om: (1) te zorgen dat ze worden gedaan; (2) ze 
zó op te zetten dat ze bruikbare en tevens valide en betrouwbare kennis en 
inzichten genereren; (3) dat ze beleidsmakers en eventueel wetgevingsjuristen 
bereiken, die ze vervolgens gebruiken in de terugkoppeling naar hun producten.  
Maar dit is in de praktijk niet vanzelfsprekend – en niet alleen vanwege de tijd en 
energie die evalueren kost. In de eerste plaats zeggen veel individuele aanvragers – 
van ‘beleid’ en ‘wetgeving’ – niet precies te weten wat hen te doen staat als er een 
wetsevaluatie voor de deur staat. Afgezien van de algemene begrippen in de 
Aanwijzingen voor de regelgeving – waaronder doeltreffendheid, doelmatigheid, 
uitvoerbaarheid, handhaafbaarheid, evenredigheid en duidelijkheid – is er geen 
centraal wetsevaluatiebeleid en bestaat niet zoiets als een IAK voor ex post 
evaluaties. In de tweede plaats is wetgeving een complex evaluatieobject. Ook al 
gaan beleid en wetgeving hand in hand, het evalueren van wetten of 
wetswijzigingen brengt een extra dimensie met zich mee. Wetten zijn doorgaans 
een mix van regels, normenkaders en beleidsinterventies. Behalve empirische en 
beleidsevaluatie-vaardigheden vereisen wetsevaluaties dus ook een juridische blik.  
Het ontbreken van een algemeen evaluatiekader, gecombineerd met de complexiteit 
van wetten als evaluatieonderwerp kan ertoe hebben geleid dat wetsvaluaties in de 
praktijk heel verschillend worden aangepakt: zowel qua inrichting van het 
aansturingsproces als methodologisch-inhoudelijk. Dat hoeft helemaal niet 
problematisch te zijn – de ene wet is immers de andere niet. Maar problematisch 
wordt de gevarieerde praktijk mogelijk wel, als de validiteit en betrouwbaarheid van 
wetsevaluaties sterk gaan variëren. Hier zijn in eerder meta-onderzoek 
aanwijzingen voor gevonden (Klein Haarhuis en Niemeijer, 2008; Veerman, 2013).  
De gevarieerde praktijk van wetsevaluatie houdt mogelijk verband met verschillen 
in evaluatievermogen bij departementen: het vermogen om wetsevaluaties te 
verrichten en ze bovendien te gebruiken (Bourgeois & Cousins, 2013). In hoeverre 
zijn de voorwaarden aanwezig om het doen en gebruiken van evaluaties mogelijk te 
maken?  
Bij die voorwaarden kan zowel gedacht worden aan kenmerken van het 
evaluatieproces als aan de inhoud van evaluaties. Kenmerken van het proces 
betreffen bijvoorbeeld: of er een speciale organisatie-eenheid voor evaluatie is, of 
gewerkt wordt met evaluatieprogrammering en -budget, wat de beschikbare tijd en 
capaciteit is en tevens, wat geregeld is omtrent het gebruiken en leren van 
evaluaties. Randvoorwaarden voor de evaluatie-inhoud zijn voorschriften of 
handreikingen voor de aanpak (bijv. evaluatiedesign), en inhoudelijke criteria zoals 
kwaliteit, relevantie of toegankelijkheid van wetgeving of beleid voor potentiële 
gebruikers.  
Het doel van dit onderzoek was om een beeld te krijgen van bestaande 
departementale praktijken rond het verrichten en gebruiken van evaluaties – met 
name wetsevaluaties. Hier aan ten grondslag ligt een streven bij de aanvrager van 
voorliggend onderzoek, de directie wetgevingskwaliteitsbeleid (WKB) van het 
  
 Pagina 97 van 146 
 

 
doorwerken in de inhoud en het gebruik van evaluaties (Van Aeken, 2007; Bovens 
et al., 2008). 
In de literatuurbespreking in het eerste hoofdstuk lieten we zien dat er twee 
ideaaltypische evaluatiemotieven zijn, namelijk (a) verantwoorden en (b) motieven 
die we samen ‘leren’ noemen – met name kennisvergroting of enlightenment en 
beleidsaanpassing of embetterment (Scriven 1994; Vedung 1997). Het motief om te 
leren komt vaak tot uiting in formatieve evaluatie waarbij bruikbaarheid het 
belangrijkste beoordelingscriterium is. Zijn we op de goede weg? In summatieve 
evaluaties (ibid.) wordt een ‘eindoordeel’ over de doeltreffendheid van een beleid 
gegeven, passend bij het motief van verantwoorden. Niet bruikbaarheid, maar 
objectiviteit en transparantie staan daarbij voorop. Vaak staat de doeltreffendheid 
centraal, het kunnen toeschrijven van resultaten aan beleidsinspanningen 
(attributie). 
Verantwoording en leren kunnen overlappen. Zo kan een evaluatie die aanvankelijk 
werd verricht met een verantwoordingsmotief alsnog worden gebruikt om lessen 
voor beleid te trekken, en omgekeerd.  
Het bouwen aan evaluatievermogen is uit te leggen als tegenwicht bieden aan 
korte-termijn politieke belangen, door een schil om het evaluatieproces aan te 
brengen. Door te werken met bijvoorbeeld een (meerjarige) 
evaluatieprogrammering en standaarden voor de evaluatie-inhoud kan weerstand 
worden geboden aan – onder meer – zaken als tijdsdruk en de politieke ‘waan van 
de dag’. Maar, zeker op langere termijn vormt het politieke klimaat óók een 
belangrijke randvoorwaarde voor de verdere ontwikkeling van evaluatievermogen 
145
(Cooksy, 2012; Nielsen et al., 2011). 
 
4.2 Internationale ervaringen met evaluatievermogen (onderzoeksvraag 1) 
 
In hoofdstuk 2 zijn, gestructureerd naar de zojuist genoemde aspecten van het 
conceptuele raamwerk, mogelijk leerzame internationale ervaringen met het 
bouwen aan evaluatievermogen (evaluation capacity building, ECB) op een rij gezet, 
waaronder met name evaluatiegidsen en andere instructies voor aanvragers en 
onderzoekers. Het betreft documenten van grotere internationale organisaties 
(Europese Commissie, VN, Wereldbank) en een aantal grotere, ‘westerse’ landen 
(Verenigd Koninkrijk, VS). ECB betreft in de praktijk vooral beleid in het algemeen, 
hoewel her en der wel enige aandacht uitgaat naar regulatory evaluation. De 
academische belangstelling voor evaluatie van wet- en regelgeving lijkt nog kleiner.  
Bij sommige internationale organisaties ligt de oorsprong van evaluatievermogen in 
een sterke verantwoordingsbehoefte. Denk aan aanzienlijke uitgaven aan leningen 
en ontwikkelingsprogramma’s door bijvoorbeeld de Wereldbank. Maar juist hier is al 
snel ook een sterke beleidsinhoudelijk gedreven functie naar voren gekomen. Ook in 
het centrale overheidsapparaat van de VS en Canada vindt ECB al meerdere 
decennia lang plaats, wat in de literatuur wel is verklaard uit het feit dat sociale 
wetenschappers hier al langere tijd onderdeel van uit hebben gemaakt. Dit zou 
contrasteren met Europa, waar juristen aanvankelijk domineerden en waar 
bestendiging van evaluatiepraktijken sinds eind jaren ’80 op gang is gekomen (o.a. 
Stern, 2009).  
 
Evaluatie als onderdeel van het beleidsproces 
Internationaal is de notie doorgedrongen dat de kwaliteit en het gebruik van 
evaluaties niet alleen afhankelijk zijn van wat de onderzoekers opleveren, maar juist 
                                                
145
 Dit vertoont analogie met het Mattheüs-effect in de sociaalwetenschappelijke literatuur: 
daar waar de (politieke) randvoorwaarden vooraf gunstig zijn, is vaak al een basisniveau van 
evaluatievermogen aanwezig en genereren nieuwe inspanningen bovendien meer opbrengsten 
dan waar gunstige randvoorwaarden niet vooraf aanwezig zijn.     
  
 Pagina 99 van 146 
 

 
evaluaties als méér dan alleen een eindrapport. Zo voorziet het Smart Regulation 
programma van de Europese Commissie op papier in terugkoppeling van 
evaluatieresultaten naar de regelgevingscyclus via het instrument van de verplichte 
ex ante toetsing. Bij de UNEG, de evaluatiegroep van de VN, is gebruik van 
evaluaties geprotocolliseerd tot en met de implementatie van resultaten in 
actieplannen en het aanleggen van evaluatie-‘kennisbanken’. Tegelijkertijd blijkt uit 
de update van de Evaluation Atlas op basis van individuele OESO-landen dat 
systematisch evaluatiegebruik zeer lastig te bewerkstelligen is (Jacob et al., 2015).  
Het sterkere accent op de aanvrager neemt niet weg dat de evaluatieonderzoeker 
nog steeds als belangrijke schakel wordt gezien in de bevordering van gebruik. Dit 
blijkt bijvoorbeeld uit leidraden voor onderzoekers om bevindingen bondig en 
toegankelijk te presenteren. Voorts blijkt het uit competentiebeschrijvingen: niet 
alleen moeten evaluatoren methodologisch-inhoudelijk competent zijn, maar ook 
interpersoonlijk en communicatief vaardig, flexibel en gevoelig voor de politieke en 
organisatorische context.  
Behalve op aanvragers en onderzoekers wordt internationaal gewezen op het belang 
van ‘aanjager’-instituties in de bevordering van evaluatievermogen van buitenaf, 
zoals evaluatiegemeenschappen, kennisinstituten en parlementen.  
 
Werkt evaluatievermogen? 
Hoewel de literatuur over evaluatievermogen omvangrijk is, is vooralsnog niet 
wetenschappelijk bewezen dat ECB tot betere beleidsresultaten leidt. Wel zijn 
uiteenlopende (positieve) case studies opgeleverd. Ook is gewaarschuwd voor 
neveneffecten, zoals dat het aanleggen en handhaven van standaarden 
opportunistisch gedrag of ritualisering in de hand kan werken waarbij het eigenlijke 
doel uit het oog wordt verloren.  
  
  
 Pagina 101 van 146 
 

 
van evaluatieonderzoek is. Primair gericht op verantwoorden is het document van 
de beleidsdoorlichting zelf.  
Standaarden hanteren bij (de beoordeling van) verantwoordingsevaluaties gebeurt 
binnen het centrale stelsel in toenemende mate, bijvoorbeeld door de Rekenkamer 
in 2011 en 2012 met meta-onderzoek naar doeltreffendheidsevaluaties van 
subsidies en vervolgens alle vindbare doeltreffendheidsevaluaties. Bovendien heeft 
het Ministerie van Financiën in 2016 een Handreiking beleidsdoorlichting 
147
geïntroduceerd. Het synthesedocument van de doorlichting zelf moet aan de RPE-
standaarden voldoen en tevens dient de doorlichting een kritische bespreking van 
het onderliggende evaluatiemateriaal te bevatten. Voor adequate verantwoording 
zou volgens Financiën strikt genomen moeten worden onderzocht of een wet of 
beleid de oorzaak is van resultaten in de praktijk. In de nieuwe Handreiking 
Beleidsdoorlichting klinkt echter niet alleen ‘hard maken’ van die oorzakelijkheid 
door maar tevens ‘plausibel maken’, bijv. met behulp van reconstructie van de 
aannames achter beleid (beleidstheorie). 
 
4.3.1 Structuur: organisatieonderdelen voor evaluatie 
 
Binnen beleidsdepartementen vervullen verschillende organisatieonderdelen een rol 
in het managen van evaluaties. Doorgaans ligt het initiatief voor ex post evaluaties - 
wetgevings- en beleidsevaluaties – bij de verantwoordelijke 
onderzoekscoördinatoren van beleidsdirecties of DG’s. Bij de meeste departementen 
is er geen aparte evaluatieafdeling en zeker geen afdeling uitsluitend gericht op 
wetsevaluaties. Elk departement kent wel één of meerdere stafafdelingen units of 
directies die de evaluatiefunctie ondersteunen en stimuleren.  
Een gemeenschappelijke is de directie Financiële en Economische Zaken (FEZ). Wat 
evalueren betreft, vullen directies FEZ hun rol per departement verschillend in – 
hoewel verantwoording uiteraard wel steeds het hoofddoel is. Zo heeft FEZ bij SZW 
een stimulerende en aanjagende rol in zowel de planning, de kwaliteitsborging als 
het gebruik van evaluatieonderzoek en beleidsdoorlichtingen, terwijl andere 
directies FEZ dichter op hun formele controletaken blijven. Binnen EZ is er sinds 
2012-2013 een netwerk van organisatieonderdelen (waaronder BEC en de voorm. 
Regiegroep M&E) inclusief FEZ, dat zich specifiek richt op de kwaliteit van 
doeltreffendheidsevaluaties. Doel is het bevorderen van effectief beleid. Op het vlak 
148
van ex post evaluaties heeft FEZ bij IenM , naar aanleiding van een kritische 
inventarisatie van doeltreffendheidsevaluaties bij beleidsdepartementen (Algemene 
Rekenkamer, 2012, 2013), de regie meer in handen genomen waar het de planning 
van het evaluatieonderzoek betreft, vooral in relatie tot beleidsdoorlichtingen.  
Soms is de stimulans en ondersteuning voor evaluatie- (en overig) onderzoek 
behalve bij FEZ ook bij andere onderdelen belegd, bijvoorbeeld in de 
149
‘kennishoek’. Bij zowel EZ als bij SZW (Chief Science Officer) is in aanvulling op al 
genoemde organisatieonderdelen een hoogleraar aangetrokken voor extra 
inhoudelijke ondersteuning, bij OCW vervult de directie Kennis die rol. Een 
gezamenlijk doel is om de kwaliteit van onderzoek naar doeltreffendheid te 
stimuleren. Bij OCW is dat behalve black box nadrukkelijk ook clear box of 
                                                
147
 Een ander, nieuw aspect van de RPE van 2015 en de nieuwe Handreiking is dat 
departementen veel aandacht dienen te besteden aan de hypothetische situatie dat 20% 
korting op het beleidsbudget plaatsvindt. Doelmatigheid krijgt dus meer nadruk.  
148
 Daar FMC geheten: Financiën, Management en Control.  
149
 Bij BZK is een van de directie Kennis sinds 2011 nagenoeg ontmanteld. Hierdoor is sinds 
2014 het zwaartepunt bij FEZ komen te liggen wat het stimuleren van de 
evaluatieprogrammering ten behoeve van de beleidsdoorlichtingen betreft. Bij IenM ligt het 
accent vooralsnog sterk op ex ante onderzoek. Beleidsmedewerkers worden bijgestaan door 
onder meer het Kennisinstituut voor Mobiliteitsbeleid (KiM) en het Behavioural Insights Team 
(BIT-IenM). Overigens heeft ook EZ een BIT.  
  
 Pagina 103 van 146 
 

 
Bij verschillende departementen en directies bestaat behalve de reguliere 
evaluatieprogrammering ook een kennisagenda. Ex post evaluatie speelt in de 
geraadpleegde kennisagenda’s echter een verwaarloosbare rol.  
 
De politieke context werkt in de praktijk door in de evaluatieprogrammering. 
Genoemde oorzaken van bijstellingen van evaluatieprogrammeringen zijn 
veranderingen in kabinetssteun voor een bepaald beleid, of nieuwe 
maatschappelijke ontwikkelingen. Sommige respondenten relativeren het feitelijke 
belang van een evaluatieprogrammering, door aan te geven dat evaluaties vooral 
worden gedaan als het bewindspersonen goed uitkomt of als er om wordt gevraagd. 
Gechargeerd spreken sommigen van evaluaties als politiek instrument (vgl. Bovens 
et al., 2008).  
Ook het al of niet opnemen van een evaluatiebepaling in wetten (artikel 64 Ar) is 
onderhevig aan politieke onderhandelingen – ook internationaal is dit waargenomen.  
In hoeverre evaluatiebepalingen worden nageleefd hebben we niet nader 
onderzocht; hierover bestaan verschillende percepties. Bij OCW heeft DWJZ een 
kalender met daarin de evaluatietermijnen van wetten naar aanleiding van de 
evaluatiebepalingen. 
 
Om werking en doeltreffendheid van een (wettelijke) interventie goed in kaart te 
brengen moet men idealiter beginnen met de gegevensregistratie en -verzameling 
al vóór invoering van de wet of maatregel, of meteen daarna. Respondenten stellen 
dat veel dossierhouders pas over de vraag- en probleemstelling van een 
evaluatieonderzoek beginnen na te denken op het moment dat de evaluatie in beeld 
komt – zoals bij veel wetsevaluaties tegen het aflopen van de in de 
evaluatiebepaling gestelde termijn. Op dat moment is het vaak te laat om tot een 
onderbouwde doeltreffendheidsstudie te komen. Het ontbreken van de hiervoor 
benodigde nulmeting en een eventueel monitoringsysteem wordt vaak geweten aan 
tijds- en capaciteitsrestricties – die op hun beurt kunnen samenhangen met 
ontwikkelingen in de politieke context. Volgens sommigen verklaart dit waarom veel 
ex post evaluaties de facto geen doeltreffendheids- maar proces- en 
doelbereikingsevaluaties zijn (vgl. Klein Haarhuis en Niemeijer, 2008 ten aanzien 
van wetsevaluaties). Omdat attributie aan beleid of wetgeving niet meer te 
realiseren is, beperkt men zich tot reconstructie van de implementatie ervan.  
 
4.3.3 Middelen  
 
Adequaat evaluatieonderzoek vergt voorts voldoende middelen. De meeste 
departementen stellen eerst een integraal budget voor onderzoek vast en daaronder 
vallen ook de evaluaties. Alle uitgevoerde evaluaties moeten binnen dat financiële 
kader passen. In de praktijk gaat men vaak uit van de bestedingsplannen, 
voortkomend uit de vastgestelde budgetten. Bij het uitzetten van evaluaties voor 
extern onderzoek bepaalt het departement het budget waarbinnen de offertes 
moeten blijven. Rijksonderzoeksinstellingen beheren vaak hun eigen 
onderzoeksbudget. We hebben niet onderzocht of middelen toereikend zijn of 
worden gevonden.  
Wat de niet-financiële middelen betreft, melden veel respondenten dat binnen 
beleidsdirecties weinig tijd en personele capaciteit beschikbaar is voor evaluaties en 
onderzoek. Zo wordt de capaciteit voor de onderzoekscoördinatie door meerdere 
respondenten gering bevonden. Bij een zeker departement betreft die op papier 1 
fte op een directie van 60 personen, maar komt het in de praktijk op minder dan 0,5 
fte neer. Dit kan onder meer doorwerken in een laat tijdstip waarop evaluaties 
worden uitgezet, wat (zoals al aangegeven) doorwerkt in hun opzet en validiteit.  
 
4.3.4 Evaluatiekader: inhoud en proces 
  
 Pagina 105 van 146 
 

 
aanpakken, hetzij via clear-box benaderingen waarbij het begrijpen en verklaren 
van gebeurtenissen centraal staat.  
De inhoudelijke evaluatiefocus verschilt per departement en sluit vaak aan bij de rol 
van de op het evaluatievlak meest actieve organisatieonderdelen. Zijn FEZ – en 
eventuele gerelateerde organisatieonderdelen – zeer actief, dan weerspiegelt zich 
dat bijvoorbeeld in een meer summatieve aanpak – heeft het gewerkt?. Bij EZ werkt 
men bijvoorbeeld met uitleg en checklists ten behoeve van evaluatieopzetten 
(procesmatig en inhoudelijk), offertes en offertevergelijking en rapporten. Zijn 
andere organisatieonderdelen actief, dan blijkt dat uit een sterkere aandacht voor 
verklarende evaluatiebenaderingen. Bij Kennis-OCW ziet men bijvoorbeeld de 
meeste kracht in een combinatie van black-box- en clear-box-aanpakken, men 
beschouwt deze als complementair.  
Veel van de genoemde programma’s en richtlijnen of handreikingen zijn nog vers, 
waardoor weinig zicht bestaat op de resultaten ervan. We weten dus niet of de 
beleidsdoorlichtingen minder lacunes zijn gaan bevatten (verantwoording), of dat de 
programmering en kwaliteit van evaluatieonderzoek in het algemeen is verbeterd. 
Niettemin hebben, juist ten aanzien van wetgeving, de beschreven programma’s 
tastbare ervaringslessen opgeleverd. Wat de inhoud betreft leren de praktijken van 
zowel de tweede en derde Awb-evaluatie, het programma STEM als het programma 
evaluatie regelgeving (PER, ZonMw) dat multidisciplinariteit in de aanpak 
onontbeerlijk is. Dat vergt een combinatie tussen juridisch onderzoek naar gelding 
en empirisch onderzoek naar doelbereiking en doeltreffendheid. Heel vernieuwend is 
dit niet maar toch een bevestiging. ZonMw is het gelukt om tot zeven algemene 
aanpakcriteria bij wetsevaluaties te komen – die in de PER-praktijk al langere tijd 
worden toegepast.  
 
(1) Wetsevaluaties moeten door een multidisciplinaire groep worden uitgevoerd met 
aandacht voor zowel het empirische als voor het juridische deel;  
(2) kennisnemen van de beginsituatie – zo mogelijk een nulmeting – moet 
onderdeel zijn van de evaluatie. Hieronder schaart men ook de beleidstheorie;  
(3) wetsevaluatie beperkt zich niet tot de te evalueren wet, maar houdt ook 
rekening met de internationale en Europese context;  
(4) wetsevaluaties kijken zowel naar gewenste als ook ongewenste effecten;  
(5) wetsevaluaties moeten aandacht schenken aan de relatie tussen wetgeving en 
zelfregulering – een criterium verbonden aan het subsidiariteitsbeginsel;  
(6) elke nieuwe evaluatie moet naast nieuw onderzoek ook gebruik maken van 
bestaande kennis, ter voorkoming van verspilling van tijd en geld; en tot slot  
(7) wetsevaluaties besteden aandacht aan diversiteit en aan de toegankelijkheid van 
alle bevolkingsgroepen tot de wet.  
 
Enkele in het derde hoofdstuk beschreven initiatieven sluiten naadloos aan bij de 
internationaal toegenomen nadruk op de verantwoordelijkheid van de aanvrager 
voor adequaat evalueren, niet alleen in het evaluatieproces zelf maar reeds bij de 
onderbouwing van het beleidsinstrumentarium en met de tijdige aanleg van 
nulmeting- en monitoringsystemen. Bij EZ zijn evaluatie-handvatten geïntegreerd in 
een gids voor beleidskwaliteit – het overkoepelende doel is: effectiever beleid 
bewerkstelligen. Deze gids beslaat alle fasen van de beleidscyclus, van 
probleemanalyse en instrumentkeuze en ex ante gevolgenbeoordeling, tot en met 
politieke beslissing, uitvoering en – uiteindelijk – ex post evaluatie. Zo bevat de gids 
aanwijzingen en tips bij het opstellen van (a) vroege beleidsexperimenten en (b) het 
opzetten van een monitoringsysteem, met concrete voorbeelden. Dit benadert sterk 
het internationale ‘ideaal’ – en dat van de aanvrager van onderhavige studie – dat 
de beleids- of wetgevingscyclus via evaluatie en terugkoppeling wordt rondgemaakt.  
Het in 2011 gelanceerde IAK wordt (nog) niet vaak genoemd. Bij IenM probeert 
men sinds kort bij beleidsdoorlichtingen terug te grijpen op resultaten van het ex 
  
 Pagina 107 van 146 
 

 
veel moeite om de database in een dynamische personele omgeving up-to-date te 
houden. Bij SZW is steeds vaker het streven dat hoogleraren en/of ervaren 
onderzoekers in het onderzoeksteam betrokken zijn.  
 
Begeleidingscommissies  
Om de kwaliteit van (wets)evaluaties verder te bevorderen, wordt veel gewerkt met 
begeleidingscommissies. Deze monitoren het verloop en de voortgang van het 
evaluatieonderzoek en komen een paar keer tijdens het traject bijeen. Door 
respondenten genoemde functies van begeleidingscommissies zijn kwaliteits- en 
voortgangsbewaking door het stellen van kritische vragen, het tijdig ingrijpen en 
bijsturen als het onderzoek niet goed verloopt. Ook het creëren van draagvlak voor 
het onderzoek wordt genoemd. Aan tafel zit meestal iemand namens de 
opdrachtgever, een vertegenwoordiger van de uitvoeringspraktijk en inhoudelijke 
experts.  
Bij de meeste departementen bestaan bij ex post evaluatieonderzoek geen vaste 
richtlijnen voor de samenstelling van begeleidingscommissies, met uitzondering van 
WODC-EWB, waar een senior-wetenschapper standaard voorzitter is, en ZonMw 
waar dit een CER-lid is. Bij beleidsdoorlichtingen neemt de directie FEZ meestal 
zitting in de begeleidingscommissie of klankbordgroep.  
 
Ervaringen verankeren  
Via de verschillende organisatieonderdelen dient procesmatige en methodologische 
evaluatiekennis bij de aanvrager terecht te komen. Dit loopt veelal via de 
onderzoekscoördinatoren bij DG’s of directies, die – bij sommige onderzochte 
departementen – extra inhoudelijke ondersteuning kunnen vragen, bijvoorbeeld 
van: de CSO bij SZW of de Regiegroep M&E (inmiddels BAT) die een 
vertegenwoordiger in elk van de B&I- beleidsdirecties heeft – en de directie Kennis 
van OCW.  
In een klein aantal gevallen worden beleids- en andere medewerkers of 
projectbegeleiders voorafgaand aan hun aanstelling geselecteerd op ervaring met 
het uitbesteden en begeleiden van evaluatieonderzoek; dit is bijvoorbeeld het geval 
bij de projectbegeleiders van WODC-EWB en bij FEZ-SZW. Meestal is dit echter niet 
het geval, omdat evaluatie niet als core business wordt beschouwd.  
Ter vervanging of ter compensatie van bestaande structuren vindt vaak informeel 
overleg plaats – bijvoorbeeld tussen junior-dossierhouders en 
onderzoekscoördinatoren of met dan wel binnen kennisdirecties en -instituten. Op 
deze manier worden ervaringen met evaluaties alsnog doorgegeven. Toch weten 
medewerkers binnen departementen vaak meer van evaluatietrajecten en de 
praktische en concrete uitdagingen in het uitbestedingsproces, dan wat 
geregistreerd en benut wordt. Het is een uitdaging gebleken om evaluatiekennis en 
-ervaring aanwezig in de hoofden van individuele dossierhouders in het collectieve 
geheugen vast te leggen en voor andere projecten te benutten. Met het vertrek van 
personen kan die bijvoorbeeld uit het departement verdwijnen. Voor de langere 
termijn kan men zich daarom de vraag stellen of informeel advies inwinnen en 
ervaring delen toereikend is – hoe flexibel en nuttig vaak ook.  
Een van de verklaringen voor dit geringe institutionele geheugen – en leren – is de 
beperkte tijd en capaciteit voor bijvoorbeeld coördinatie van onderzoek. Het 
ontwikkelen van specialisaties is bovendien lastig bij een lage evaluatie-omzet; 
alleen bij een op projectbegeleiding gerichte afdeling, zoals EWB bij het WODC lukt 
dit nog aardig.  
De Interdepartementale Begeleidingsgroep Prestatiegegevens en Beleidsevaluatie 
(IBP), onder coördinatie van het ministerie van Financiën, is een forum waarin FEZ-
vertegenwoordigers van elk departement samen met Financiën werkzame 
ervaringen met en aanpakken van evaluaties uitwisselen. Meerdere (FEZ) 
respondenten bestempelden dit als nuttig.  
  
 Pagina 109 van 146 
 

 
‘Nieuwe’ kennis over hoe gedrag te beïnvloeden heeft in dat opzicht meer te bieden, 
ook omdat die vaak beter te generaliseren is.  
Een tweede leerfunctie van (wets)evaluaties is volgens respondenten (niet zijnde 
aanvragers) dat ze de impliciete aannames waarop wetgeving of beleid rust, boven 
water halen en toetsen aan de praktijk.  
‘Lerend’ gebruik van evaluaties valt om uiteenlopende redenen niet af te dwingen, 
maar blijft nastrevenswaardig zolang niet-gebruik toe te schrijven is aan de 
bruikbaarheid van studies, of aan gebrekkige communicatie met aanvragers. Kennis 
uit evaluaties zinkt vaak nog weg, terwijl ze later weer nodig kan worden. Dit maakt 
het raadzaam om te investeren in (thematische) bundeling van evaluatieresultaten 
153
in meta-onderzoeken en resultaatoverzichten. Bij VenJ/WODC is onlangs 
bijvoorbeeld de interactieve Kennisbank justitiële interventies in gebruik genomen. 
Ook de evaluaties zelf kunnen thematisch worden ingericht, mede om ritualisering, 
maar ook overbelasting van de beleids- en wetgevingscyclus, tegen te gaan (vgl. de 
thematische wetsevaluaties van ZonMw).  
Box 11 Globale vergelijking internationale en nationale observaties  
 
Zowel internationaal als in ‘Den Haag’ is een accentverschuiving zichtbaar van de onderzoeker 
naar de verantwoordelijkheid die de evaluatie-aanvrager heeft in het tot stand brengen en 
gebruiken van evaluaties. Althans, in de rolomschrijvingen op papier.  
Dat de helderheid en de onderbouwing van beleid en wetgeving de latere kwaliteit van 
evaluaties beïnvloeden, komt in de Nederlandse praktijk vooralsnog wat minder uit de verf. Zo 
ver als de Europese Commissie, die stelt dat het formuleren van minder doelstellingen van 
beleid bijdraagt aan gefocuste en budgettair proportionele evaluaties, gaat men in Den Haag 
bijvoorbeeld (nog) niet. Maar recente ‘integrale’ hulpmiddelen zoals een EZ-gids en enkele 
cursussen voor aanvragers, zijn wel aanwijzingen in die richting. Hierin wordt evaluatie 
geïntegreerd behandeld met de andere onderdelen van de beleidscyclus, met effectiever beleid 
als einddoel. Ook is door sommige respondenten een relatie gelegd tussen evaluaties en 
beleidsdoorlichtingen en de aandachtspunten van het ex ante IAK.  
 
 
4.4 Evaluatievermogen in context  
 
4.4.1 Politiek en verantwoording  
 
De politieke realiteit van alledag werkt volgens de literatuur door in de behandelde 
aspecten van evaluatievermogen. Zo kan de politieke dynamiek strategisch of 
legitimerend evalueren in de hand werken. Is bij de politieke of ambtelijke leiding 
bijvoorbeeld weinig ruimte voor tegenspraak – of wordt dit alleen al zo 
gepercipieerd door ambtenaren – dan zullen evaluaties gericht op kennisvergroting 
154
en beleidsverbetering geen voet aan de grond krijgen. Op de langere termijn kan 
dit gevolgen hebben voor het evaluatievermogen, zoals in termen van 
organisatiecapaciteit, budget en tijd. 
Dat aanhoudende politieke druk op langere termijn een remmende werking kan 
hebben op de ontwikkeling van het evaluatievermogen, hebben we met dit 
onderzoek niet hard kunnen toetsen, maar op basis van de interviews zijn er 
aanwijzingen voor. De kracht ervan hangt samen met de evaluatieonderwerpen en 
hoe politiek of maatschappelijk gevoelig die liggen. Sommige beleidsdirecties 
                                                
153
 In lijn met de in Hoofdstuk 2 aangehaalde internationale initiatieven van bijvoorbeeld EPPI, 
UNEG en Campbell, en met adviezen van het Panel van Advies bij BZ-IOB (2011, 2013, 2014).  
154
 Dit ligt wel genuanceerd. Er zijn meerdere lagen: de politieke leiding legt weer 
verantwoording af aan de Kamer. De Kamer kan soms hameren op lerende evaluaties, maar 
juist ook rappe koerswijzigingen in beleid aanjagen, waarbij de stap van evaluatie wordt 
overgeslagen. 
  
 Pagina 111 van 146 
 

 
voorlopige werking van de wet zou de termijn mogelijk wel korter kunnen dan bij 
summatieve evaluaties. 
Verder draagt een adequate toelichting op de wet in de Memorie van Toelichting bij 
aan de werking van de evaluatiebepaling: wat is de bedoeling van de wetgever; wat 
zijn de aannames, welke resultaten van de wet worden op basis hiervan verwacht? 
Op welke punten kan een wetsevaluatie hier over x jaar aan bijdragen? WKB heeft 
inmiddels een schrijfwijzer voor Memories van Toelichting voltooid, die mede is 
bedoeld om de onderbouwing van wetgeving en beleid (beleidstheorie) nader te 
157
verhelderen. 
 
4.4.3 Organisatiecontext 
 
Zoals al beschreven is het verankeren van inhoudelijke en evaluatie-gerelateerde 
kennis lastig gebleken, alleen al vanwege het verloop van ervaren 
beleidsmedewerkers. Dit is volgens enkele respondenten fnuikend voor het behoud 
van evaluatiekennis en -ervaring. Hier komt bij dat het evaluatiekader, in de zin van 
handreikingen om bepaalde werkwijzen tot stand te brengen, niet altijd goed ‘landt’ 
bij beleidsdirecties – en ook externe onderzoekers. Men is bijvoorbeeld nog 
onvoldoende vaardig of overtuigd om er gebruik van te maken. Hierdoor landen die 
initiatieven in een weinig vruchtbare omgeving. Nieuwe, geüniformeerde 
inkoopprocedures zijn ook een potentiële bedreiging voor de reeds ontwikkelde 
initiatieven.  
 
4.5 Wetsevaluaties: een vak apart?  
 
Is het nodig om wetsevaluaties een aparte plek te geven in de organisatie van 
evaluatievermogen? Wetten zijn sterk verweven met beleid. Dat de wet niet alleen 
empirisch of sociaalwetenschappelijk maar ook juridisch-georiënteerd onderzoek 
vergt, is toegelicht in het eerste hoofdstuk. Beleids- en FEZ-medewerkers zien 
wetsevaluatie echter niet als een afzonderlijke tak van sport, zo komt naar voren uit 
dit onderzoek. Dit geldt eveneens voor enkele respondenten bij wetgevingsdirecties.  
We hebben geen handreikingen aangetroffen specifiek voor wetsevaluaties. Ook 
onder academici en internationale organisaties is maar beperkte belangstelling voor 
158
evaluatie van wet- en regelgeving in het bijzonder. Over nut en noodzaak hiervan 
lijken respondenten van mening te verschillen. Beleidsdirecties zijn in het huidige 
evaluatiesysteem in the lead en bepalend voor wat, wanneer en hoe geëvalueerd 
wordt – en tevens voor het evaluatiegebruik. Maar ook daar ontbreekt het – op 
zeker een aantal plaatsen – aan evaluatiekennis en -ervaring. Het lijkt ook om die 
reden zinvol om initiatieven in het kader van evaluatievermogen breder te trekken 
dan wetsevaluatie alleen.  
Het bijzondere karakter van de wet als evaluatie-onderwerp is echter wel een 
belangrijk punt van aandacht. Wetgevingsjuristen en andere juridisch specialisten 
zijn van waarde voor evaluaties met een juridische component; zuiver 
sociaalwetenschappelijke aanpakken zijn dan te arm. Eerder genoemde 
praktijkervaringen met STEM, Awb en ZonMw onderstrepen het belang van een 
multidisciplinaire – juridische én sociaalwetenschappelijke – benadering van 
wetsevaluaties, blijkend uit in een gemengde samenstelling van het 
onderzoeksteam en een meerledig onderzoeksdesign.  
Directies Wetgeving en/of juridische zaken lijken op grond van dit onderzoek echter 
niet of nauwelijks aangehaakt bij ex post beleidsevaluaties. Meerdere medewerkers 
                                                
157
 www.kcwj.nl/kennisbank/schrijfwijzer-memorie-van-toelichting .   
158
 Men kan zich afvragen: is het complex van regels en interventies dat ‘wet’ heet wel een 
geschikte analyse-eenheid voor stapeling van kennis? Bij enkelvoudige beleidsinterventies lijkt 
de ‘vergelijkbaarheid’ die de directie WKB nastreeft, eenvoudiger te realiseren.  
  
 Pagina 113 van 146 
 

 
- Is het moment gepasseerd om nog een controlegroep te vormen dan kan 
men, onder voorwaarde van een monitoringsysteem, terugvallen op 
kwantitatieve tijdreeksanalyses om te toetsen of beleid of wet tot 
‘trendbreuken’ heeft geleid. Bij een dergelijke aanpak dient zoveel 
mogelijk gecontroleerd te worden voor andere mogelijke verklaringen.  
- Ten slotte kan men variatie tussen regio’s, gemeenten of specifieke 
doelgroepen benutten, om beter te begrijpen hoe beleid in de praktijk is 
uitgepakt. Dit geïnspireerd op de methodiek van multiple case study 
onderzoek: de onderzoeker maakt gebruik van variatie in kenmerken 
tussen groepen (cases) die onder dezelfde wet of beleid vallen, om zo tot 
een beter begrip van werking en resultaten te komen. Wat zijn 
onderscheidende kenmerken van verschillende gemeentes of doelgroepen 
en hoe werkt dit door in de resultaten? Bij welke kenmerken / onder 
welke condities worden de meeste resultaten behaald en wat zijn 
plausibele verklaringen hiervoor? Dit past bij de eerder onderscheiden 
white box-stroming in evaluatiedesigns.  
 
Kennisvergroting en geïnformeerde beleidsaanpassing komen uiteraard niet alleen 
via ex post evaluaties, de focus van dit onderzoek, tot stand. Leren kan deels ook 
worden bereikt met behulp van ex ante evaluaties en met vroege 
beleidsexperimenten of pilots. In geval van wetten kan experimenteerwetgeving 
ethische en andere knelpunten in het werken met experimentele en controlegroepen 
wegnemen. In een latere beleidsfase kunnen dergelijke ‘vroege’ studies bovendien 
de verantwoording ondersteunen.  
 
4.6 Slotconclusies (onderzoeksvraag 4)  
Het evaluatievermogen is – los van de basistaken van directies FEZ en de rol van 
onderzoekscoördinatoren – niet centraal geregeld, maar heeft per departement 
verschillend vorm gekregen. Dit betreft zowel actieve organisatieonderdelen als 
procesmatige en inhoudelijke evaluatie-initiatieven en -propgramma’s. Veel van de 
in dit rapport beschreven constructies, procedures en aanpakken zijn nog vers. 
Hierdoor konden we niet nagaan in hoeverre ze structurele verbeteringen hebben 
opgeleverd in termen van leren en verantwoorden; laat staan of ze – het doel bij EZ 
aanhalend – uiteindelijk tot effectiever beleid hebben geleid. Uit ervaringen elders is 
opgemaakt dat evaluatievermogen ook doorzettingsvermogen is. Dit maakt het 
interessant om de verdere ontwikkelingen te volgen.  
 
Tabel 9 Belangrijkste conclusies in een notendop  
Motieven  Aspecten van evaluatievermogen en hun kenmerken  
(politieke context)  
 Structuur: organisatieonderdelen gericht op evaluatie en hun rol 
 • Het beeld is zeer wisselend. FEZ is in elk departement standaard 
 betrokken voor controletaken, maar de reikwijdte van haar rol 
 wisselt. Daarnaast zijn er meestal coördinatoren onderzoek en voor 
 een aantal departementen aanvullende, per departement 
 verschillende ondersteunende (kennis)directies en organen. Vaste 
 instituten bij het departement alleen bij VenJ, BZ en IenM 
 • De aanvrager cq. beleidsmedewerker krijgt in internationale en ook 
 Nederlandse leidraden een steeds belangrijker rol toebedeeld in (a) 
 evaluatieproces en –inhoud en (b) het zorgen voor evalueerbaar 
 (helder) beleid.  
 •  
 Evaluatieprogrammering en –verplichtingen 
  
 Pagina 115 van 146 
 

 
bevatten. Verloop van medewerkers en wisselingen op dossiers 
werken negatief uit. Wel vindt informeel overdracht van 
leerervaringen plaats en soms via geijkte organisatieonderdelen, of 
interdepartementaal (IBP)  
•  
Bevordering van bruikbaarheid en gebruik 
• Er zijn – buiten horizonbepalingen in subsidieregelingen om – geen 
institutionele garanties voor lerend gebruik, nationaal noch 
internationaal. Wel zijn bruikbaarheidsinitiatieven genomen en vindt 
veel verspreiding in de vorm van primaire en secundaire publicaties 
plaats, ook richting de Tweede Kamer 
• De tamelijk veel gehanteerde begeleidingscommissies vervullen een 
belangrijke rol in termen van bruikbaarheid en feitelijk gebruik, soms 
al tijdens de evaluatie  
• De politieke context is cruciaal voor gebruik. Een sterke focus op 
verantwoording en legitimering staat mogelijk aan leren in de weg.  
 
Dit rapport en zijn conclusies zijn aanleiding geweest voor een expertmeeting, 
gehouden op 6 april 2016. Deze had als doel om de discussie en de 
gedachtenvorming – in het bijzonder over het evaluatievermogen rond wetgeving – 
160
een stap verder te brengen. 
  
                                                
160
 Prof. dr. P. Van der Knaap verzorgde daar een referaat over het onderhavige rapport. Het 
verslag van deze expertmeeting is te vinden op de website van het KCWJ.   
  
 Pagina 117 van 146 
 

 
We used this conceptual frame to, first, report a non-exhaustive number of 
approaches and experiences by leading international organisations and larger 
(western) countries. Second, desk study combined with 35 interviews with key 
informants were conducted to collect information from departmental practice as 
regards the six aspects.  
Evaluations, including those of laws, do not take place in a vacuum. They are 
conducted from one or more of a variety of possible motives, e.g.: political 
accountability, for example, addressing a parliamentary question; financial 
accountability, to complete the budget cycle; to learn, i.e. to close the policy or 
regulatory cycle and use evaluation knowledge to feedback into new or adjusted 
polices. Evaluations may also serve a strategic goal. Two ideal-typical evaluation 
motives are discerned in the literature: (a) accountability and (b) learning, as in 
enlightenment and/or policy embetterment. The political context influences which of 
these motives is dominant. In turn, the evaluation motive affects how evaluations 
are dealt with and used. What is more, there is a correlation between motive and 
evaluation type. Summative evaluations, focused on the question ‘did it work’, more 
often suit the accountability motive, whereas formative evaluations, focused on 
understanding (intermediate) results from policies or laws, particularly fit the 
learning motive.  
 
International experiences with evaluation capacity (research question 1)  
 
A variety of international handbooks, guides and related materials offered by 
(among others) the UN, European Commission (EC), US, UK and World Bank 
demonstrate that policy officials – often the commissioners – have been assigned a 
significantly larger role in the commissioning and management of evaluations over 
the past decennium.  
First, commissioners make crucial decisions about the evaluation process. They co-
determine evaluation content and quality – as well as use. For example, they have a 
say or determine the overall evaluation plan / program and budget as well as the 
scope, focus and main evaluation questions. A multitude of criteria and guides 
circulate on formulating Terms of Reference (ToRs), the process of commissioning, 
supervision, judgement and utilization of evaluations (see Annex 3 to this report). 
These guides often also include information on suitable evaluation designs and 
methodologies, particularly for evaluations of effectiveness. Here, two strands can 
be discerned: black-box and clear-box evaluation approaches. Black-box approaches 
address the attribution problem: to what extent did the policy or law under 
evaluation actually trigger the results aimed for – rather than factors external to it? 
How this happened is outside the scope of study. In this strand an emphasis is put 
on experimental designs – including randomized controlled trials (RCTs) or quasi-
experimental ones, where confounding factors are filtered out as much as possible. 
Clear-box approaches address the question: how did a policy or law work out, and 
why or under which circumstances? The two strands are not mutually exclusive; 
they may complement one another. In this regard, fit-for-purpose is an emerging 
mantra; ‘the right’ evaluation design hardly exists. Whether a design is adequate 
depends on, for example, type of policy, policy stage, topic, main questions to be 
answered and target group type(s). For example, the US General Audit Office (GAO) 
published a guide for evaluating highly complex programs: how to make the most 
out of an effectiveness evaluation when a program can hardly be disentangled from 
its context?  
By making policy officials more aware of the relative strengths and limitations of 
different evaluation designs, they are enabled to make better informed decisions in 
relation to evaluation – so it is thought.  
Second, apart from key players in the evaluation process, policy officials also help 
create evaluation preconditions by shaping their policies in the process of policy 
  
 Pagina 119 van 146 
 

 
which the Research and documentation centre (WODC) at the Ministry of Security 
and Justice. All policy departments have a financial-economic division, responsible 
for accountability and control. To varying extents, these financial-economic divisions 
help stimulate the planning, quality and use of policy evaluation research. For 
example, at the Ministry of Social affairs, this role is quite pronounced as opposed to 
other departments. Within the Economic Affairs department, since 2012-2013 a 
network of divisions and units, including the financial-economic division, has 
addressed the quality of monitoring and evaluation (m&e) activities, mostly in 
relation to effectiveness evaluations.  
As opposed to financial-economic divisions, ‘knowledge divisions’ are more 
concerned with strategic, future-oriented research than with ex post evaluations. 
The latter account for just a fraction of their research activities. 
Several of the organizational structures and units are quite fresh and require time to 
take shape in everyday practice. An exception is the Commission for Regulatory 
Evaluation (CER) at ZonMw, which has been operating in the domain of evaluations 
of health law since almost two decades. 
 
Evaluation programming and obligations 
In all annual departmental budgets, an evaluation program is included and 
published. Besides this, almost no parallel programs were retrieved. These 
evaluation programs partly also result from obligations in about 10-20 per cent of 
concrete laws, to (periodically) evaluate the law in question, most often in terms of 
effectiveness and efficiency. As indicated, these evaluation studies form the base 
material for the periodic policy appraisals, in which findings are to be synthesized to 
account for budget spending. The process of evaluation programming seems to be 
dominated by accountability motives and political incentives. Most departments first 
determine an overall research budget, within which individual evaluations ought to 
fit. Research institutions like the WODC or the Netherlands Institute for Social 
research (SCP) administer their own budget.  
As concerns non-financial means, respondents often reported that policy divisions 
spend a limited percentage of their time on research and evaluation, judging from 
(among other things) limited capacity for the co-ordination of research activities. 
This is not without consequence: it puts pressure on the programming of 
evaluations. What is more, time pressure makes policy officials to postpone the start 
of an evaluation, so that measurement opportunities are missed, like forming a 
control group, performing a baseline measurement and/or setting up a monitoring 
system.  
Over the past few years, financial-economic divisions and sometimes other types of 
organizational divisions have tried to make evaluation programs more pro-active, 
particularly in view of the obligatory periodic policy appraisals and the strive for 
establishing policy effectiveness. It is too early to establish the results of these 
initiatives.  
 
Evaluation frameworks & programs (process and content)  
In chapter 3, we described a number of programs that serve to manage or enhance 
both the process and content of evaluation studies. Three of these programs 
particularly addressed legislation – in the domains of the environment, health and 
administrative law, respectively. The evaluation programs are examples of 
(temporary) evaluation capacity and offer lessons as regards the programming and 
demarcation of evaluation questions, design and evaluation criteria. One 
overarching lesson resulting from these experiences is the importance of a 
multidisciplinary, empirical-legal approach when evaluating legislation. Effectiveness 
in a legal sense does not imply that a law will achieve the intended societal effects 
in practice – and vice versa. In the Program for Regulatory Evaluation (PER, 
  
 Pagina 121 van 146 
 

 
contracts. At the Ministry of Security and Justice, the WODC keeps a database 
containing characteristics and capacities of external research teams. Particularly in 
case of evaluations of laws, a multidisciplinary team is crucial. 
Once the evaluation process has started, supervisory committees are regularly used 
to monitor progress, quality and content. In some policy departments, this is 
standard practice and there are procedures as regards the role and composition of 
these committees. For example, the WODC requires the chair of the supervisory 
committee to be a senior academic. 
The exchange of departmental experiences with evaluation projects is often done by 
informal means, for example, in bilateral or division meetings. Respondents doubt 
whether this will safeguard ‘collective evaluation memory’. Between departments, 
periodic exchange of evaluation experiences takes place between at least the 
financial-economic divisions and the Ministry of Finance (often in the light of policy 
appraisals). This was valued positively by the respondents involved.  
The aforementioned ‘fresh’ evaluation structures and programs appear to be 
vulnerable when taking into account: departmental cultures as regards research and 
evaluation, reorganizations and large turnover of policy staff. This is in line with 
foreign experiences that enhancing and consolidating evaluation capacity is a long-
winded task that requires perseverance. 
 
Promoting evaluation utilization  
Evaluation use is an indispensable aspect of evaluation capacity. Without processing 
the fruits of evaluation in the development of laws and policies, each of the capacity 
aspects described above will lose meaning. Contact and consultation between 
commissioner and evaluator enhances the probability of evaluation use according to 
the literature as well as this study. Timeliness and timing in the policy cycle are also 
important factors. In general, departments publish evaluation reports and often the 
reports are sent to Parliament. In many cases, an official cabinet response is 
attached. This response often includes statements as to how a Minister intends to 
deal with the findings and (if available) recommendations. Besides this, evaluators 
themselves often valorize evaluation findings by means of presentations for policy 
officials or academics and articles in professional or academic journals.  
In the end, it is up to policy officials and the political context to what extent and 
how findings are used. There are no fixed or prescribed routes for this, apart from 
horizon clauses in financial regulations. These regulations expire after five years and 
can only be prolonged if sufficiently sustained by, among other things, ex post 
evaluation findings. 
To increase their learning value, users argue that ex post evaluations should not 
only look back, but be oriented towards future action to a larger degree. Evaluations 
are also deemed instructive if policy assumptions (policy theory) are given a reality 
check. Finally, more knowledge repositories could be established in which findings 
from previous evaluation studies are systematized. 
 
Conclusions and discussion (research question 4)  
 
Many of the aforementioned evaluation structures and programs are still quite 
young, making it hard to determine their impact on learning and accountability and 
ultimately, on the effectiveness of laws an policies. Hence, it would be interesting to 
closely follow future developments in the context of evaluation capacity. 
One key condition for evaluation capacity is that initiatives to enhance policy 
relevant knowledge and learning experiences are welcomed rather than punished, in 
the political context as well as in the central evaluation system. In relation to this, 
some respondents argued that to learn from evaluations requires formative (process 
and intermediate) evaluations next to summative work about end effects.  
  
 Pagina 123 van 146 
 

 
Literatuur  
 
Algemene Rekenkamer. (2011). Leren van subsidie-evaluaties. Den Haag: 
Algemene Rekenkamer.  
 
Algemene Rekenkamer. (2012). Effectiviteitsonderzoek bij de rijksoverheid. Den 
Haag: Algemene Rekenkamer.  
 
Algemene Rekenkamer. (2013). Effectiviteitsonderzoek bij de rijksoverheid: 
vervolgonderzoek. Den Haag: Algemene Rekenkamer.  
 
Algemene Rekenkamer, TK 1993-1994, 23710 nrs. 1-2. Wetgeving: organisatie, 
proces en produkt. Verkregen van 
http://resourcessgd.kb.nl/SGD/19931994/PDF/SGD_19931994_0006932.pdf .  
 
Astbury. B., & Leeuw, F.L. (2010). Unpacking black boxes: mechanisms and theory 
building in evaluation. American Journal of Evaluation, 31(3), 363-381.  
 
American Evaluation Association (AEA) (2004). Guiding principles for evaluators. 
Washington DC: AEA, www.eval.org.  
 
American Evaluation Association (AEA) (2013). Evaluation Roadmap for a more 
effective government (revised). Washington DC: AEA, www.eval.org.  
 
Backes, C.P., Hardy, E.M.J., Jansen, A.M.L., Polleunis, S., & Timmers, R. (2014). 
Evaluatie Bestuurlijke Lus Awb en internationale rechtsvergelijking, Maastricht: 
Universiteit Maastricht (i.o.v. WODC). 
 
Bakker, R. (2012). Evaluatie als fundering voor beleid. Den Haag: Ministerie van 
Binnenlandse Zaken. Verkregen van 
www.rijksoverheid.nl/documenten/rapporten/2012/06/01/evaluatie-als-fundering-
van-beleid . 
 
BEC-secretariaat (z.j.). Handreikingenboekje beleidskwaliteit en beleidsevaluatie. 
Intern document van het Ministerie van EZ.  
 
Bex, P.M.H.H., Bovens, F., & Goo, F.S. (2010). Evaluatie Wet extern klachtrecht: 
onderzoek naar de organisatie en effecten van het extern klachtrecht bij 
gemeenten, Eindrapportage (versie 1.0). Nieuwegein: SIRA Consulting.  
 
Bemelmans-Videc, M.L. (2002). Evaluation in the Netherlands 1990-2000: 
Consolidation and Expansion. In J.-E. Furubo, R.C. Rist & R. Sandahl (Red.). 
International Atlas of Evaluation. New Brunswick: Transaction Publishers.  
 
Bourgeois, I., Hart, R.E., Townsend, S.H., & Gagné, M. (2011). Using hybrid models 
to support the development of organizational evaluation capacity: a case narrative. 
Evaluation and program planning, 34, 228-235. Doi: 
10.1016/j.evalprogplan.2011.03.003.  
 
Bourgeois, I. en Cousins, J.B. (2013). Understanding Dimensions of Organizational 
Evaluation Capacity. Americal Journal of Evaluation, 34(3), 299-319. Doi: 
10.1177/1098214013477235.  
 
  
 Pagina 125 van 146 
 

 
 
EC (2015a). Monitoring and evaluation of cohesion policy (2014-2020). Evaluation 
plan Guidance Document on Evaluation Plans. Terms of Reference for Impact 
Evaluations. Guidance on Quality Management of External Evaluations. Verkregen 
van  
http://ec.europa.eu/regional_policy/sources/docoffic/2014/working/evaluation_plan
_guidance_en.pdf  
 
EC (2015b). Better Regulation Guidelines. Verkregen van 
http://ec.europa.eu/smart-regulation/guidelines/index_en.htm.  
 
EC (2015c). Better Regulation Toolbox. VI: Evaluations and fitness checks. 
Verkregen van 
http://ec.europa.eu/smart-regulation/guidelines/toc_tool_en.htm  
 
EC (2015d). Evaluatieplan Europees Sociaal Fonds (ESF) 2014-2020.  
 
Eijlander, Ph. (2003). De Awb-evaluatie geëvalueerd. Hoe is de tweede evalautie 
van de Awb aangepakt en wat heeft die opgeleverd? Nederlands Juristenblad, 9(9), 
265-269.  
 
Farrington, D.P., & Welsh, B.C. (2005). Het belang van experimentele evaluaties in 
de criminologie. Justitiële verkenningen, 31(8), 11-41.  
 
FEZ-EZ (z.j.) Handreiking evaluatieonderzoek EZ. Intern document van EZ.  
 
Forss, K., & Carlsson, J. (1997). The quest for quality – or can evaluation findings 
be trusted? Evaluation, 3(4), 481-501.   
 
Furubo J-E., Rist R.C., & Sandahl, R. (red.) (2002) International Atlas of Evaluation. 
New Brunswick, NJ and London: Transaction Publishers. 
 
GAO (2012) Designing evaluations, 2012 revision. Washington: US Government, 
Government Accountability Office 12-208G.  
  
GfK (2015). Het rijksinkoopstelsel; gebruik, kennisniveau en informatiebehoeften. 
Kwantitatief en kwalitatief onderzoek. Presentatie. Verkregen via 
www.rijksoverheid.nl/binaries/rijksoverheid/documenten/rapporten/2015/08/01/het
-rijksinkoopstelsel-gebruik-kennisniveau-en-informatiebehoeften/20150902-
rapportage-het-rijksinkoopstelsel-def.pdf.  
 
Gough D., Oliver, S., & Thomas, J. (2012). An introduction to systematic reviews. 
London: Sage. 
 
Van Gils, G., & Leeuw, F.L. (2010). Leren van Evalueren: Onderzoek naar het 
gebruik van evaluatieonderzoek bij het Ministerie van Buitenlandse Zaken. Den 
Haag: Ministerie van Buitenlandse Zaken, p. 47. 
 
HM Treasury (2011a). Green book. Appraisal and evaluation in central government. 
Treasury Guidance, London: TSO. Verkregen van: 
www.gov.uk/government/uploads/system/uploads/attachment_data/file/220541/gr
een_book_complete.pdf.  
 
HM Treasury (2011b). Magenta book. Guidance for evaluation. Verkregen van 
www.gov.uk/government/publications/the-magenta-book  
  
 Pagina 127 van 146 
 

 
Lösel, F. (1995). The efficacy of correctional treatment: A review and synthesis of 
meta-evaluations. In J. McGuire (red.), What works: Reducing reoffending (pp. 79-
114). Chichester, UK: Wiley. 
 
Mackay, K. (2006). Evaluation Capacity development. Institutionalization of 
Monitoring and Evaluation Systems to Improve Public Sector Management. ECD 
Working Paper Series, 15, Washington D.C.: The World Bank.  
 
Mark, M.M. (2012). Influences on evaluation quality: reflections and elaborations. 
American Journal of Evaluation, 33(1), 85-87. Doi:10.1177/1098214011426470. 
 
Mastenbroek, E., Van Voorst, S., & Meuwese, A. (2015). Closing the regulatory 
cycle? A meta-evaluation of ex-post legislative evaluations by the European 
Commission. Journal of European Public Policy 17(x). Doi: 
10.1080/13501763.2015.1076874. 
 
Mayne, J.L., Bemelmans-Videc, M.L., Hudson, L., & Conner, R. (1992). Advancing 
Public Policy Evaluation: Learning From International Experiences. Amsterdam: 
Elsevier. 
 
Mayne, J. (2008). Contribution analysis: an approach to exploring cause and effect. 
ILAC (Institutional learning and change) Brief. www.cgiar-ilac.org  
 
Michiels, F.C.M.A. (2010) Het evalueren van de Awb: een voortdurend proces. .. p. 
41-55. In 
 
Ministerie van Buitenlandse Zaken. (2009). Evaluatiebeleid en richtlijnen voor 
evaluaties. Den Haag: Inspectie Ontwikkelingssamenwerking en Beleidsevaluaties. 
 
Ministerie van BZK (2010). Strategische kennisagenda: investeren in legitimiteit. 
Verkregen van 
www.rijksoverheid.nl/documenten/rapporten/2010/03/02/strategische-
kennisagenda-investeren-in-legitimiteit 
 
Ministerie van Financiën. (2003). Handreiking Evaluatieonderzoek ex post. Den 
Haag: Directie Begrotingszaken. Geraadpleegd op 28 augustus 2015 op 
www.minfin.nl/vbtb. 
 
Ministerie van Financiën (2006). Regeling Periodiek Evaluatieonderzoek. 
Staatscourant Nr. 83.  
 
Ministerie van Financiën. (2014). Regeling Periodiek Evaluatieonderzoek. 
Staatscourant Nr. 27142.  
 
Ministerie van Financiën (2016). Handreiking beleidsdoorlichtingen 
http://www.rijksbegroting.nl/beleidsevaluaties/evaluaties-en-
beleidsdoorlichtingen/handreiking  
 
Ministerie van IenM (2013). Strategische kennis- en innovatieagenda (SKIA) 2012-
2016. Verkregen van 
www.rijksoverheid.nl/documenten/brochures/2012/06/29/ienm-maakt-ruimte-
strategische-kennis-en-innovatieagenda-skia-infrastructuur-en-milieu.  
 
  
 Pagina 129 van 146 
 

 
Panel van Advies IOB (2014), Derde advies van het panel van advies, Den Haag: 
IOB. 
 
Parapuf, A. (2016). Zicht op evaluatiecapaciteit. Een onderzoek naar het vermogen 
van Nederlandse ministeries om (wets)evaluaties te verrichten. Nijmegen: Radboud 
Universiteit (Masterscriptie Bestuurskunde, Organisatie & management).  
 
Pater, C., Sligte, H., & Van Eck, E. (2012). Verklarende evaluatie, een methodiek. 
Amsterdam: Kohnstamm Instituut.  
 
Patton, M.Q. (2008). Utilization-Focused Evaluation, 4th Ed. Thousand Oaks, CA: 
Sage.  
 
Patton, M.Q. (2010). Utilization-focused evaluation. Real-time and prospective 
aspects. In Wereldbank (red.), Exploring the potential of real-time and prospective 
evaluations, summary of a workshop (pp. 6-11). Washington DC: World Bank 
Independent Evaluation Group.  
 
Pattyn V., & Verweij S. (2014). Beleidsevaluaties tussen methode en praktijk: Naar 
een meer realistische evaluatiebenadering. Burger, Bestuur en Beleid, 4, 260-267. 
 
Pawson, R., & Tilley, N. (1997). Realistic Evaluation. London: Sage.  
 
Power, M. (1997). The audit society: rituals of verification. Oxford: Oxford 
University Press.  
 
Prins, A. (2012a), Gebruik en gebruikers van WODC publicaties: Een bibliografische 
analyse van het WODC 2008-2010, Groningen: Ad Prins 
 
Prins, A. (2012b), Citatieanalyse van overige WODC publicaties 2007-2011, 
Groningen: Ad Prins.  
 
Preskill, H. en Boyle, S. (2008). A Multidisciplinary Model of Evaluation Capacity 
Building. American Journal of Evaluation, 29(4), 443-459. DOI: 
10.1177/1098214008324182.  
 
Programma Versterking Juridische Functie Rijk (2010)  
 
Regering van Australië (2011). Identifying and evaluating regulation reforms. 
Productivity commission research report. Canberra: Productivity Commission. 
www.pc.gov.au.  
 
Regering van Zuid-Australië (2011). Better Regulation Handbook. How to design and 
review regulation, and prepare a Regulatory Impact Statement. Melbourne: 
Regering van Zuidaustralië.  
 
Regering van Australië (2014). Post implementation reviews. Guidance note. Office 
of Best Practice Regulation.  www.dpmc.gov.au/office-best-practice-
regulation/publication/post-implementation-reviews-guidance-note.  
 
Rijksacademie voor Financiën, economie en bedrijfsvoering (2015). Beleid in 
perspectief (cursus). Verkregen van www.rijksacademie.nl/opleiding/bip/.  
 
  
 Pagina 131 van 146 
 

 
UNEG (2010a). Quality Checklist for Evaluation Terms of Reference and Inception 
Reports. UNEG Guidance document 2010/1. Verkregen van 
www.uneval.org/document/library .  
 
UNEG (2010b). Quality Checklist for Evaluation Reports. UNEG Guidance document 
2010/1. Verkregen van www.uneval.org/document/library .  
 
Uylenburg, R., de Boer, J., Peeters, M.G.W.M., & Oosterhuis, F.H. (2011). Het 
STEM-geluid bij de evaluatie van milieuwetgeving. Arnhem: STEM Publicatie 2011/1.  
 
Van Aeken, K. (2007). From vision to reality: ex post evaluation of legislation. 
Legisprudence: international journal for the study of legislation 5(1), 41-68.  
 
Van Gils, G., & Leeuw, F.L. (2010). Leren van Evalueren: Onderzoek naar het 
gebruik van evaluatieonderzoek bij het Ministerie van Buitenlandse Zaken. Den 
Haag: Ministerie van Buitenlandse Zaken.  
 
Van Hoesel, P.H.M., Mevissen, J.W.M., & Dekker, L.H. (2015). Kennis voor Beleid - 
Beleidsonderzoek in Nederland. Assen: Van Gorcum.  
 
Van der Laan, A., Beijaard, F., Geurts, T., Kalidien, S., & Straalen, E.K. van (2013). 
Zelfevaluatie WODC, periode 2006-2012. Den Haag: WODC.  
 
Van Thiel, S., & Leeuw, F.L. (2002). The performance paradox in the public sector. 
Public Performance & Management Review, 25(3) 267-281.  
 
Vedung, E. (1997). Public Policy and Program Evalution. New Brunswick: 
Transaction Publishers.  
 
Veerman, G.J. m.m.v. Hendriks-de Lange, S.R. (2007). Over wetgeving. Principes, 
paradoxen en praktische beschouwingen. Den Haag: Sdu Uitgevers.  
 
Veerman, G.J., m.m.v. Mulder, R.J. en Meijsing, E.S.M. (2013). Een empathische 
wetgever. Meta-evaluatie van empirisch onderzoek naar de werking van wetten. 
Den Haag: Sdu Uitgevers.  
 
Veerman, G.J. (2014). Klaarheid over het Clearing House. RegelMaat, 29 (4), 199-
211.  
 
Vereniging voor Beleidsonderzoek (VBO) (2004). Twaalf tips over het offertetraject 
voor opdrachtgevers van beleidsonderzoek.  Nijmegen: VBO.  Verkregen van 
www.beleidsonderzoek.nl  
 
Von Meyenfeldt, L., Schrijvershof, C., & Wilms, P. (2008) Tussenevaluatie 
beleiddoorlichting. Den Haag: APE, rapport nr. 570.  
 
Wereldbank (1992). Governance and development. Washington, DC : The World 
Bank. Verkregen van 
http://documents.worldbank.org/curated/en/1992/04/440582/governance-
development  
 
Wereldbank (2002). Evaluation capacity development: a growing priority. 
Washington DC: World Bank Operations Evaluation Department. Précis, nr. 229.  
 
  
 Pagina 133 van 146 
 

 
Bijlage 1  Begeleidingscommissie  
 
Prof.mr.dr. E. Niemeijer 
Academie voor Wetgeving & Academie voor 
 
Overheidsjuristen 
Mr.dr. J. L.H.M. Stoop 
Ministerie van Infrastructuur en Milieu / Beleids- 
en Bestuursondersteuning / Hoofddir. 
Bestuurlijke & Juridische Zaken / Directie Milieu, 
Ruimte en Water / Afdeling Milieu 
Drs. D. de Groot 
Ministerie van Veiligheid en Justitie, Directie 
Wetgeving en Juridische Zaken, 
Wetgevingskwaliteitsbeleid  
Dr. E Mastenbroek 
Radboud Universiteit Nijmegen, Faculteit der 
Managementwetenschappen 
 
  
  
 Pagina 135 van 146 
 

 
Bijlage 3 Uitwerkingen van het evaluatiekader (internationaal)  
 
Europese Commissie: Richtlijnen voor Terms of Reference en andere richtlijnen voor 
aanvragers (EC, 2013a)  
& 
UNEG: Criteria voor de beoordeling van ToR’s (UNEG, 2010)  
 
- Wettelijk/ regulerend, institutioneel kader  
- Scope van de evaluatie: evaluatieobject, onderzoeksperiode, locatie in de 
beleidscyclus en (geografische) gebiedsafbakening 
- Belangrijkste stakeholders en gebruikers van de evaluatie 
- Evaluatieve en onderzoeksvragen 
- Beschikbare kennis over het programma en eventuele effecten (mede van 
invloed op volgende stap)  
- Belangrijkste te gebruiken methoden en technieken  
- Planning  
- Indicatief budget  
- Benodigde kwalificaties van het team dat de evaluatie uitvoert 
- Structuur van de offerte  
- Regels m.b.t. offreren en beoordelingscriteria  
 
 
European Evaluation Society (EES): Competenties van evaluatieonderzoekers 
(verkort)  
 
1. Evaluation knowledge 
1.1 Appreciates the distinctive role played by evaluation in society 
1.11 Exhibits familiarity with evaluation theories, schools and approaches 
1.12 Shows awareness of evaluation history and trends 
1.13 Appreciates the linkages between evaluation and the social sciences 
1.14 Understands program theory and its implications for evaluation 
1.15 Aims at evaluation independence and excellence in all contexts 
1.2 Masters the antecedents of evaluation quality 
1.21 Uses appropriate evaluation concepts and correct evaluation terms 
1.22 Displays a capacity to identify relevant evaluation questions 
1.23 Knows how to engage constructively with all stakeholders 
1.24 Comprehends the value of diverse evaluation approaches 
1.25 Adapts evaluation designs and methods to specifi c contexts 
1.3 Understands the potential and limits of evaluation instruments and 
tools 
1.31 Data collection and analysis 
1.32 Experimental and quasi experimental methods 
1.33 Qualitative, participatory and mixed methods 
1.34 Case studies, surveys, interviews, expert panels 
1.35 Indicators, rating and monitoring systems 
2. Professional practice 
2.1 Demonstrates capacity to manage and deliver evaluations 
2.11 Responds to legitimate stakeholders’ needs and concerns 
2.12 Assesses the evaluation context and identifi es the program logic 
2.13 Manages resources and skills prudently so as to achieve results 
2.14 Gathers, uses and interprets evidence with care and judgment 
2.15 Reports fairly and encourages effective use of evaluation results 
2.2 Displays interpersonal skills 
2.21 Writes fluently and communicates clearly 
2.22 Values team work and leads by example 
2.23 Uses sound negotiating and confl ict resolution skills 
2.24 Demonstrates gender awareness and cultural sensitivity 
2.25 Nurtures professional relationships 
  
 Pagina 137 van 146 
 

 
UKES Guidelines for Commissioners  
 
To ensure good practice in evaluation, it would be helpful if Commissioners: 
- acknowledge the benefits of external, independent evaluation. 
- operate fair tendering situations in which competitors ideas are not 
exploited or intellectual property misused as a result of commissioning. 
- hold preliminary consultations with all parties to the evaluation to support a 
relevant, realistic and viable specification. 
- specify the purpose and audience(s) for the evaluation with appropriate 
background material to encourage relevant tenders. 
- operate a tendering procedure that is open and fair ensuring that 
appropriately qualified assessors are involved and making explicit criteria 
upon which a tender decision will be made. 
- clarify the constraints that commissioners operate under, e.g. timescales, 
budgets, deadlines, and accountability. 
- adhere to the terms agreed in the contract and consult with evaluators and 
other interest groups if significant changes are required to the design or 
delivery of the evaluation. 
- specify the legal terms and responsibilities of the evaluation in the contract. 
- match the aims and potential outcome of the evaluation to the knowledge 
and expertise of the potential evaluator(s). 
- provide access to documentation and data required for evaluation purposes. 
- establish clear principles for the reporting and dissemination of evaluation 
reports funded by public monies, consistent with acknowledged procedures 
which ensure quality evaluation and reporting. 
- have realistic expectations on what an evaluation might provide including 
sufficient time for evaluators to respond to an initial invitation to tender and 
produce a proposal. 
- include experienced evaluators (who are not potential applicants for 
funding) in initial drafts of evaluation specifications, including feasible 
budget and realistic timescales. 
- have trust in and mutual respect for all stakeholders, participants, 
commissioners and evaluator(s). 
- take advice of evaluators on research methodologies for collecting and 
analysing data. 
- communicate openly and keep the evaluation team informed of changes in 
circumstances affecting the evaluation. 
- recognise where evaluators need to keep their sources of information 
anonymous. 
- preserve the integrity of the findings, e.g. by not quoting or publicising 
findings out of context. 
  
  
 Pagina 139 van 146 
 

 
wordt van kracht, met evaluatieverplichtingen voor de departementen, ex post en ex ante. De 
departementen dienen een duidelijk evaluatiebeleid te ontwikkelen en te voeren. De kwaliteit 
van evaluaties dient op peil te zijn, de organisatie van het evaluatieproces dient duidelijk 
belegd te zijn, en er dienen voorzieningen te zijn om het gebruik van de resultaten te 
bevorderen. In de aangepaste RPE van 2006 wordt als nieuw doelmatigheidsinstrument de 
beleidsdoorlichting geïntroduceerd teneinde beleid ‘vooraf beter te onderbouwen en achteraf 
beter te evalueren’ (IOFEZ, 2004). Elk departement wordt geacht om bevindingen uit 
verschillende doeltreffendheids- en andere evaluaties hierin samen te brengen (synthetiseren) 
163
op het niveau van de beleidsdoelen (artikelen) in de begroting.  
Sindsdien kent het centrale verantwoordingsstelsel twee hoofdinstrumenten: (1) de IBO’s, in 
164
aard en aanpak vooral ex ante en gericht op alternatieven van beleid cq. heroverweging en 
(2) de beleidsdoorlichtingen. 
2008. In de Regeringsnota Vertrouwen in wetgeving waarin de nadruk ligt op 
terughoudendheid met wetgeving en ruimte wordt door de minister van Justitie de oprichting 
aangekondigd van een ‘Clearing House voor systematische wetsevaluatie’, met het oog op 
‘ervaringskennis op het terrein van voorbereiding van wetgeving, effecten van wetgeving en de 
uitvoering daarvan’.  
2010 Het kabinet biedt de rapporten van 20 werkgroepen Brede Heroverwegingen aan de 
Tweede Kamer aan. De Brede Heroverwegingen, ter voorbereiding van politieke keuzes over 
toekomstige bezuinigingen, maken deel uit van het kabinetsbeleid om de financiële en 
economische crisis het hoofd te bieden 
(www.rijksoverheid.nl/documenten/kamerstukken/2010/04/01/aanbieding-
heroverwegingsrapporten).  
2012-2013 De Rekenkamer publiceert twee opeenvolgende overzichten van evaluatiestudies 
op rijksniveau die gaan over doeltreffendheid en doelmatigheid (OER). Iets minder dan de helft 
van het geld dat ministers jaarlijks uitgeven aan beleid (€94 miljard in 2011) is sinds 2006 
42% (€ 36 miljard) daadwerkelijk op doeltreffendheid onderzocht – zoals verplicht op grond 
van artikel 20 Cw. Deels heeft dit te maken met gebrekkige aansluiting tussen 
evaluatieonderzoek en de formulering van begrotingsdoelstellingen. Voor een deel bestaan hier 
redenen voor bij beleidsmakers die de AR vervolgens heeft nagetrokken en in haar rapport van 
een algemene reactie voorziet. (1) Het is niet duidelijk hoe het beleid op doeltreffendheid kan 
worden geëvalueerd (2) Het is nog te vroeg om de doeltreffendheid te kunnen vaststellen (3) 
Het beleid wordt al door anderen op doeltreffendheid geëvalueerd (4) 
Doeltreffendheidsonderzoek voegt weinig toe (5) Het beleid is inmiddels stopgezet (6) De 
kosten van doeltreffendheid zijn te hoog (Algemene Rekenkamer, 2013:4). 
Op 1 januari 2013 treedt een herziene ‘Regeling Periodiek Evaluatieonderzoek’ (RPE) in 
werking die verbetering van overzichten van evaluatie- en overig onderzoek vereist en beter 
inzicht in de programmering van beleidsdoorlichtingen in begrotingen en jaarverslagen. 
Daarnaast moet met Verantwoord Begroten (VB) met ingang van de begroting 2013 de 
165
informatievoorziening over evaluaties worden verbeterd. In 2015 is de RPE opnieuw herzien. 
Met VB is het belang van beleidsdoorlichtingen verder toegenomen doordat de begrotingen en 
                                                
163
 De rapporten zijn openbaar op de website van de Rijksbegroting: 
www.rijksbegroting.nl/beleidsevaluaties/evaluaties-en-beleidsdoorlichtingen/archief.  
164
 Bijv. de IBO wetenschappelijk onderzoek (2014) had als centrale vraag: “Is de huidige inzet 
van middelen voor wetenschappelijk onderzoek optimaal voor het bereiken van een maximale 
maatschappelijke output?” voor de beantwoording waarvan internationale studies en ander 
materiaal is verzameld en voorts vragenlijsten zijn uitgezet en interviews zijn gehouden. 
Daarna zijn 5 beleidsvarianten beschreven met het oog op de toekomst – waaronder 
handhaving van de status quo. 
165
 Voor elke evaluatie worden de titel, het betreffende artikel en start- en einddatum vermeld. 
Van afgeronde evaluaties wordt de vindplaats aangegeven, waar mogelijk met een hyperlink 
naar de evaluatie zelf. Ook wordt gestimuleerd om evaluatieonderzoek te gebruiken bij de 
motivering van beleidswijzigingen 
(http://www.rijksbegroting.nl/algemeen/overzichten,Verantwoord-begroten.html link naar 
flyer).  
  
 Pagina 141 van 146 
 

 
van de invoering van een leerlingvolgsysteem. De voor- en nameting zouden dan nog kunnen 
worden uitgebreid met een controlegroep van scholen die al wel een leerlingvolgsysteem 
gebruiken, waardoor een difference-in-differencesopzet ontstaat.’ 
 
Verklarende evaluatie 
Voor een verklarende evaluatie van het onderdeel centrale toetsing is het van belang na te 
gaan:  
a. welke evidentie er is op basis van onderzoek en ervaring van leerkrachten, schoolleiders, 
besturen, inspectie en deskundigen voor de afzonderlijke veronderstellingen in deze sets; 
b. hoe scholen en leerkrachten op centrale toetsing verwachten te reageren (en waarom) in de 
wijze van onderwijs geven, de accenten in het lesprogramma, de mate van differentiëren 
tussen leerlingen, de wijze van toetsen, enz. (ex ante);  
c. hoe zij feitelijk reageren en waarom (pilots, ex post);  
d. of en hoe leerkrachten, schoolleiders, besturen, inspectie en deskundigen na enkele jaren 
toe- of afgenomen leerwinst verklaren (in hoeverre is de centrale toetsing en in hoeverre zijn 
andere veranderingen daar debet aan). Visies van leerkrachten en deskundigen op deze 
punten kunnen deels op basis van beschikbare data worden getoetst. (...)  
 
Stappenplan verklarende evaluatie (OCW)  
 
Pater et al. (2012) dragen onderzoekers een aantal stappen aan om tot verklarende 
evaluatie te komen, en passen deze vervolgens toe op een tweetal cases rond 
voortijdig schoolverlaten (VM2 en De Wijkschool). Deze laten zich als volgt 
samenvatten:   
  
1. Vooraf: Desk research  
- Verzamel/ activeer kennis over het betreffende beleidsthema (wetenschappelijke / 
evaluaties; inzichten van experts)  
- Reconstrueer globaal de beleidscontext waar de beleidsinterventie uit voortkomt 
(documenten, wetsvoorstellen, nota’s, adviezen etc.) 
- Ga na wat het veronderstelde probleem is en of het plausibel is dat het probleem 
overwonnen wordt door de veronderstelde veranderingsmechanismen 
- Indien beschikbaar, analyseer de al bestaande effectevaluaties met het doel te 
bepalen of de effecten gerelateerd zijn of kunnen worden aan verschillen in de context 
waarin interventies zijn gepleegd.  
2. Reconstructie van de beleidstheorie: hoe wordt dit beleid geacht te werken? 
- Reconstrueer de beleidsinterventie en de achterliggende beleidstheorie aan de hand 
van beleidsdocumenten. Construeer schema’s van causale ketens aan de hand van 
veronderstellingen in de beleidstheorie en evt. empirisch onderzoek. 
- Expliciteer de beleidstheorie in dialoog met beleidsmakers 
- Analyseer de verkregen informatie uit interviews op impliciete en expliciete 
causaliteiten, mechanismen, differentiatie in contexten, doelgroepen, 
belanghebbenden, bedoelde en onbedoelde effecten. Pas de reconstructie van de 
beleidstheorie en de schema’s van causale ketens aan. 
3. Confrontatie van de beleidstheorie met de literatuur (in ieder geval bij ex ante, 
optioneel bij proces- en ex post evaluatie) 
4. Veldonderzoek (alleen bij proces- en ex post evaluatie). In het veldonderzoek wordt 
de gereconstrueerde beleidstheorie getoetst aan de doelgroep en andere 
betrokkenen. Dit kan uiteraard alleen als de interventie al in gang gezet is. Formuleer 
voor elke informant relevante en te beantwoorden vragen en denk na over de meest 
geschikte onderzoeksmethode voor elk type betrokkenen.  
 
De auteurs betogen dat het een iteratief proces betreft: ‘De stopregel wordt 
gevormd door het niet meer verkrijgen van volledig nieuwe informatie.’ (2012:90).  
 
  
 Pagina 143 van 146 
 

 
• De probleemstelling of centrale onderzoeksvraag sluit aan bij de aanleiding 
van de evaluatie en de probleemanalyse 
• Bij een beleidsdoorlichting: alle elementen/vragen uit de RPE zijn benoemd 
• Bij een individuele evaluatie: de benoemde onderzoeksvragen geven 
antwoord op alle relevante elementen van een beleidsdoorlichting om 
zodoende als basis gebruikt te worden voor de beleidsdoorlichting  
4. Onderzoeksaanpak 
• De causale relatie tussen beleidsinstrument(en), de activiteiten en het 
vermeende effect worden onderzocht  
• Er is aangegeven of een nulmeting is gedaan 
• Er is aangegeven of er mogelijk een controlegroep bestaat 
• De beperkingen en onmogelijkheden ten aanzien van het meten van effecten 
worden benoemd 
• Duidelijk is op welke alternatieve wijze de doeltreffendheid wordt 
onderzocht, indien er geen nulmeting en controlegroep is (bepalen 
plausibiliteit).. 
• Verkennen van en advies over verbeteringen in de mogelijkheden tot 
effectmeting. 
• De wijze waarop data worden verzameld staat beschreven (databases, 
interviews, enquête) 
• De beperkingen ten aanzien van uitspraken over de doeltreffendheid bij een 
alternatieve onderzoeksaanpak (geen nulmeting/controlegroep) zijn 
benoemd 
5. Uitbesteden of zelf doen 
• Er is aangegeven of en waarom het onderzoek wordt uitbesteed of zelf wordt 
gedaan 
6. Tijdpad 
• Het tijdpad van het onderzoek (start- en einddatum en tussenproducten) is 
helder 
7. Organisatie en kosten  
• Aanspreekpunt voor het onderzoek (projectleider) is benoemd 
• Er is aangegeven hoe het onderzoek wordt begeleid 
• Er is aangegeven welk budget beschikbaar is voor het onderzoek 
8. Communicatie 
• Er is aangegeven of de evaluatie wel/niet wordt aangeboden aan de Tweede 
Kamer  
 
Checklist evaluatierapport FEZ (EZ)  
 
1. Volledigheid 
• Het rapport geeft antwoord op de centrale onderzoeksvraag 
• Het rapport geeft antwoord op alle (deel)vragen uit de evaluatieopzet/offerte 
2. Duidelijkheid 
• De conclusie over de doeltreffendheid van het beleid is helder (het beleid is 
wel/niet effectief of kan niet worden aangetoond, maar wel indicaties dat 
……………) 
• Het rapport maakt duidelijk onderscheid tussen de bevindingen, de analyse 
en de conclusies die daaruit getrokken worden 
  
 Pagina 145 van 146 
 

